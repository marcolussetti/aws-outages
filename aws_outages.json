{
  "archive": [
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Amazon Connect login performance",
      "date": "1617678631",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:10 PM PDT</span>&nbsp;We are investigating reports of increased error rates in the US-EAST-1 region for users to log onto Amazon Connect.</div><div><span class=\"yellowfg\"> 9:21 PM PDT</span>&nbsp;We are continuing to investigate reports of increased error rates in the us-east-1 region for users to log onto Amazon Connect</div><div><span class=\"yellowfg\"> 9:33 PM PDT</span>&nbsp;Between 6:57 PM and 9:24 PM PDT Amazon Connect users experienced increased login error rates in the us-east-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon WorkMail (N. Virginia)",
      "summary": "[RESOLVED] Degraded WorkMail WebMail performance",
      "date": "1617678855",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:14 PM PDT</span>&nbsp;We are investigating a degraded customer experience with the WorkMail Web client in the US-EAST-1 Region impacting logins.</div><div><span class=\"yellowfg\"> 9:12 PM PDT</span>&nbsp;We are continuing to investigate a degraded customer experience with the WorkMail Web client in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 9:33 PM PDT</span>&nbsp;Between 6:57 PM and 9:25 PM PDT we experienced a degraded customer experience logging in to the WorkMail Web client in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workmail-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Degraded instance registration",
      "date": "1617683231",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:27 PM PDT</span>&nbsp;Between 7:05 PM and 8:50 PM PDT, we experienced increased load balancer provisioning and back-end target registration times in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Lambda (Oregon)",
      "summary": "[RESOLVED] Lambda Management Console Errors",
      "date": "1617754061",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:07 PM PDT</span>&nbsp;We are investigating increased error rates for the Lambda Management Console in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 5:46 PM PDT</span>&nbsp;Between 3:04 PM and 5:18 PM PDT, we experienced increased error rates for the Lambda Management Console in the US-WEST-2 Region. The event was limited to customers accessing the AWS Lambda console. The AWS Lambda data plane and API were not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-west-2"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased AWS Billing Console errors",
      "date": "1618316888",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;We are investigating increased error rates for customers in India when performing Credit Card and Net Banking payments in the AWS Billing Console.</div><div><span class=\"yellowfg\"> 6:08 AM PDT</span>&nbsp;We continue to investigate increased error rates for customers in India when performing Credit Card and Net Banking payments in the AWS Billing Console.</div><div><span class=\"yellowfg\"> 8:17 AM PDT</span>&nbsp;Between April 12 10:25 AM PDT and April 13 6:55 AM PDT, we experienced increased error rates for customers in India when performing Credit Card and Net Banking payments in the AWS Billing Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1618339468",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:17 PM PDT</span>&nbsp;We are working to resolve the issue resulting in increased error rates for the following EC2 APIs in the US-EAST-1 Region: RunInstances, *SecurityGroups, *NetworkInterfaces, *RouteTables, *AccountAttributes, and *NetworkAcls. These APIs will affect the ability to launch new EC2 instances and make mutating changes to Virtual Private Cloud (VPC) network configuration(s). Existing instances and networks continue to work normally. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\">12:56 PM PDT</span>&nbsp;We continue to work toward recovery for the issue resulting in increased API error rates for the EC2 APIs in the US-EAST-1 Region. We have identified the root cause and applied mitigations to reduce the impact, while we continue to work towards full mitigation. Some APIs may experience errors or “request limit exceeded” when calling an affected API or using the EC2 Management Console. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact via the Personal Health and/or Service Health Dashboards. We will provide an update in 30 minutes.</div><div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;We continue to work towards full resolution for the issue resulting in increased error rates for the EC2 APIs in the US-EAST-1 Region. We have applied some request throttling for the affected APIs, which has reduced error rates, allowing several APIs to see early recovery. We are adjusting these throttling for some of the affected APIs, which are causing some additional API errors and elevated errors in the EC2 Management Console. We would expect API error rates to continue to recover with the mitigation steps we have taken as we work towards full recovery.</div><div><span class=\"yellowfg\"> 2:13 PM PDT</span>&nbsp;We continue to work towards full recovery for the issue resulting in increased error rates for the EC2 APIs in the US-EAST-1 Region. We have adjusted request throttles to reduce error rates for the affected APIs. While this has worked for some of the affected APIs, such as RunInstances, some of the affected APIs are now returning “request limit exceeded”. If this does occur, attempt to reduce your request rate for the affected API and retry. With the request throttling, some of the affected services are also beginning to see recovery. We continue to work on resolving the underlying root cause and expect to be fully recovered within the next hour.</div><div><span class=\"yellowfg\"> 3:01 PM PDT</span>&nbsp;We have further adjusted request throttles to reduce error rates for the affected APIs, so “request limit exceeded” errors should now be significantly reduced. We are now in the final stages of resolving the issue with the underlying data store. Once resolved, we will remove all API throttles and expect all API operations to return to normal levels.</div><div><span class=\"yellowfg\"> 3:55 PM PDT</span>&nbsp;We have resolved the issue resulting in the increased error rates for the EC2 APIs, and removed all API request throttling, in the US-EAST-1 Region. Beginning at 11:11 AM PDT, we experienced an increase in API error rates for RunInstances and networking related EC2 APIs. At 12:57 PM PDT, request throttling was applied to several of the affected APIs, which helped to improve error rates for the RunInstances API. We continued to work towards full resolution, while removing request throttles, until 3:30 PM PDT, at which time all affected APIs returned to normal levels of operation. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased Provisioning and Scaling Latencies",
      "date": "1618343445",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:50 PM PDT</span>&nbsp;We can confirm increased provisioning and scaling latencies for Elastic Load Balancers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:38 PM PDT</span>&nbsp;We have determined the root cause of increased scaling latencies for Elastic Load Balancers in the US-EAST-1 Region and are working towards mitigation.</div><div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;The team is closely monitoring the issue with EC2 APIs, and continues to see reductions in scaling and provisioning latencies. Other load balancing operations, including target registrations and traffic processing are unaffected. Once the issue with the EC2 APIs is resolved, we will process the backlog of any pending operations before we observe full resolution.</div><div><span class=\"yellowfg\"> 3:44 PM PDT</span>&nbsp;Beginning at 11:13 AM PDT we experienced increased provisioning and scaling latencies for Elastic Load Balancers in the US-EAST-1 Region. Recovery for scaling latency occurred at 1:15 PM PDT, and recovery for provisioning latency occurred at 3:35 PM PDT. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Batch (Oregon)",
      "summary": "[RESOLVED] Compute Environments going INVALID",
      "date": "1618364745",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:45 PM PDT</span>&nbsp;We are investigating increased transitions to INVALID of some AWS Batch Compute Environments in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 7:12 PM PDT</span>&nbsp;We can confirm increased transitions to INVALID of some AWS Batch Compute Environments in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:00 PM PDT</span>&nbsp;Between 5:25 PM and 7:52 PM PDT, some AWS Batch Compute Environments transitioned to INVALID in the US-WEST-2 Region. The issue has been resolved and the service is working normally.</div>",
      "service": "batch-us-west-2"
    },
    {
      "service_name": "AWS AppSync (N. Virginia)",
      "summary": "[RESOLVED] Increased API Latencies",
      "date": "1619193400",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:56 AM PDT</span>&nbsp;We are investigating increased API latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:36 AM PDT</span>&nbsp;We are starting to see recovery for increased API latencies and timeouts in the US-EAST-1 Region and continue to work towards resolution. </div><div><span class=\"yellowfg\"> 9:51 AM PDT</span>&nbsp;Between 4:41 AM and 9:00 AM PDT, we experienced increased API latencies and timeouts in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "appsync-us-east-1"
    },
    {
      "service_name": "AWS Lambda (N. Virginia)",
      "summary": "[RESOLVED] Increased Latencies and Error Rates",
      "date": "1619482717",
      "status": "0",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:22 PM PDT</span>&nbsp;Between 4:14 PM and 4:22 PM PDT the Lambda invoke API experienced increased latencies and error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation Delays",
      "date": "1620324983",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:16 AM PDT</span>&nbsp;We are investigating delays in propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We can confirm that actual invalidations are being propagated as usual, but the invalidation status confirmation through the console and API is delayed. This issue is not impacting propagation times for changes to CloudFront configurations as previously stated. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">12:10 PM PDT</span>&nbsp;We have identified the root cause of delays in reporting status changes of CloudFront invalidations. We continue to work toward resolution. All CloudFront edge locations are consuming configuration changes and invalidations normally. Also, End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">12:43 PM PDT</span>&nbsp;Between 10:04 AM PDT and 12:20 PM PDT, customers might have experienced delays in reporting status change of CloudFront invalidations. During this time, all CloudFront edge locations were consuming configuration changes and invalidations normally but were not updating status changes in console or via CloudFront APIs. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sao Paulo)",
      "summary": "[RESOLVED] Network Connectivity Issue",
      "date": "1620331000",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:56 PM PDT</span>&nbsp;We are investigating connectivity issues for some instances in a single Availability Zone (sae1-az2) in the SA-EAST-1 Region. </div><div><span class=\"yellowfg\"> 1:19 PM PDT</span>&nbsp;Starting at 12:20 PM PDT, we experienced low levels of packet loss for Internet Connectivity for some instances in a single Availability Zone (sae1-az2) in the SA-EAST-1 Region. Between 12:48 PM and 12:59 PM PDT, DNS resolution within the affected Availability Zone (sae1-az2) and connectivity between the affected Availability Zone (sae1-az2) and other Availability Zones using public IP addressing also experienced low levels of packet loss. At 1:12 PM PDT, all packet loss issues were resolved. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-sa-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] API Errors, Increased Provisioning Times / Registration Latencies",
      "date": "1620993901",
      "status": "0",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:47 AM PDT</span>&nbsp;We are investigating increased API error rates and increased provisioning/registration latencies for ELBs in the US-EAST-1 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;Starting at 5:05 AM PDT, we experienced periods of increased error rates and provisioning/registration latencies for ELB APIs in the US-EAST-1 Region. The periods of elevated API error rates were resolved at 7:55 AM PDT. The periods of increased provisioning/latencies were resolved for the vast majority of customers by 8:31 AM PDT, with full recovery at 9:11 AM PDT. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (N. Virginia)",
      "summary": "[RESOLVED] Increased Errors Connecting to WorkSpaces",
      "date": "1621023810",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:23 PM PDT</span>&nbsp;We are investigating connectivity issues and WorkSpaces automatically rebooting in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:05 PM PDT</span>&nbsp;We can confirm an issue causing a loss of connectivity and reboots for a subset of Windows WorkSpaces in the US-EAST-1 Region. We have identified the root cause and are rolling out a mitigation.</div><div><span class=\"yellowfg\"> 3:26 PM PDT</span>&nbsp;We are currently deploying a mitigation for the issue that is causing loss of connectivity and reboots for a subset of Windows WorkSpaces in the US-EAST-1 Region. Workspaces will recover as this mitigation is deployed.</div><div><span class=\"yellowfg\"> 5:01 PM PDT</span>&nbsp;We continue to see recovery for some Windows WorkSpaces as our mitigation is deployed in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 6:10 PM PDT</span>&nbsp;Recently, we experienced an issue that caused loss of connectivity and reboots for a subset of Windows WorkSpaces in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workspaces-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1621884419",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:26 PM PDT</span>&nbsp;We are experiencing increased API error rates for the EC2 APIs in the US-WEST-2 Region. We have identified root cause and are actively working to mitigate impact. Some APIs may experience errors or “request limit exceeded” when calling an affected API or using the EC2 Management Console. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact via the Personal Health and/or Service Health Dashboards. We'll continue to update this post as we have more information to share.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;Between 11:02 AM and 12:40 PM PDT, we experienced increased API error rates in the US-WEST-2 Region. During this time, customers may have experienced 500 Errors or \"Request Limit Exceeded\" when calling an affected API, or using the EC2 Management Console. In many cases, retries of failed API requests were successful. Other AWS services that utilize the affected APIs for their own workflows also experienced impact. Those AWS Services have communicated with affected customers via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-west-2"
    },
    {
      "service_name": "AWS Sign-In (N. Virginia)",
      "summary": "[RESOLVED] Increased Latencies for SAML Sign-In",
      "date": "1623264111",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We are investigating periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. </div><div><span class=\"yellowfg\">12:31 PM PDT</span>&nbsp;We can confirm periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\">12:58 PM PDT</span>&nbsp;We are seeing signs of recovery for the timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;Between 7:46 AM and 11:37 AM PDT, we experienced periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to investigate likely root causes and continue to drive to full resolution. The service is operating normally.</div>",
      "service": "signin-us-east-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates and Latencies for SAML Based Federation",
      "date": "1623266658",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:24 PM PDT</span>&nbsp;We can confirm periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\">12:58 PM PDT</span>&nbsp;We are seeing signs of recovery for the timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:21 PM PDT</span>&nbsp;Between 7:46 AM and 11:37 AM PDT, we experienced periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to investigate likely root causes and continue to drive to full resolution. The service is operating normally.</div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Frankfurt)",
      "summary": "[RESOLVED] Connectivity Issues &amp; API Errors",
      "date": "1623356647",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:24 PM PDT</span>&nbsp;We are investigating connectivity issues for some EC2 instances in a single Availability Zone (euc1-az1) in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 1:55 PM PDT</span>&nbsp;We can confirm increased API error rates and latencies for the EC2 APIs and connectivity issues for instances within a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region, caused by an increase in ambient temperature within a subsection of the affected Availability Zone. Other Availability Zones within the EU-CENTRAL-1 Region are not affected by the issue and we continue to work towards resolving the issue.</div><div><span class=\"yellowfg\"> 2:36 PM PDT</span>&nbsp;We continue to work on resolving the connectivity issues affecting EC2 instances in a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region. Ambient temperatures within the affected subsection of the Availability Zone have begun to return to normal levels and we are working to recover the affected EC2 instances and networking devices within the affected Availability Zone. For the vast majority of affected EC2 instances, once network connectivity is restored, the instances will recover. A small number of EC2 instances may have power cycled as a result of the increased temperatures. While we continue to make progress in resolving the issue, we continue to recommend failing away to other Availability Zones in the region if you are able to do so.</div><div><span class=\"yellowfg\"> 3:26 PM PDT</span>&nbsp;We continue to work on resolving the connectivity issues affecting EC2 instances in a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region. While temperatures continue to return to normal levels, engineers are still not able to enter the affected part of the Availability Zone. We believe that the environment will be safe for re-entry within the next 30 minutes, but are working on recovery remotely at this stage. Once we have access to the affected subsection of the Availability Zone, we will be working towards restoring network connectivity and any EC2 instances that were impaired by the high ambient temperatures. We continue to recommend failing away to other Availability Zones in the region if you are able to do so.</div><div><span class=\"yellowfg\"> 4:12 PM PDT</span>&nbsp;We continue to work on resolving the connectivity issues affecting EC2 instances in a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region. Unfortunately, we continue to wait for environmental conditions to improve within the subsection of the affected Availability Zone where it is safe to send in engineers. Some EBS volumes are also experiencing degraded performance within the affected Availability Zone, but are expected to recover once network connectivity is restored. We are working to resolve the issue and will keep you updated on our progress.</div><div><span class=\"yellowfg\"> 4:33 PM PDT</span>&nbsp;We have restored network connectivity within the affected Availability Zone and continue to work on full recovery of EC2 instances and EBS volumes. Customers should begin to see recovery at this stage.</div><div><span class=\"yellowfg\"> 5:19 PM PDT</span>&nbsp;We have restored network connectivity within the affected Availability Zone in the EU-CENTRAL-1 Region. The vast majority of affected EC2 instances have now fully recovered but we’re continuing to work through some EBS volumes that continue to experience degraded performance. The environmental conditions within the affected Availability Zone have now returned to normal levels. We will provide further details on the root cause in a subsequent posts, but can confirm that there was no fire within the facility.</div><div><span class=\"yellowfg\"> 6:54 PM PDT</span>&nbsp;Starting at 1:18 PM PDT we experienced connectivity issues to some EC2 instances, increased API errors rates, and degraded performance for some EBS volumes within a single Availability Zone in the EU-CENTRAL-1 Region. At 4:26 PM PDT, network connectivity was restored and the majority of affected instances and EBS volumes began to recover. At 4:33 PM PDT, increased API error rates and latencies had also returned to normal levels. The issue has been resolved and the service is operating normally.\n\nThe root cause of this issue was a failure of a control system which disabled multiple air handlers in the affected Availability Zone. These air handlers move cool air to the servers and equipment, and when they were disabled, ambient temperatures began to rise. Servers and networking equipment in the affected Availability Zone began to power-off when unsafe temperatures were reached. Unfortunately, because this issue impacted several redundant network switches, a larger number of EC2 instances in this single Availability Zone lost network connectivity.\n\nWhile our operators would normally had been able to restore cooling before impact, a fire suppression system activated inside a section of the affected Availability Zone. When this system activates, the data center is evacuated and sealed, and a chemical is dispersed to remove oxygen from the air to extinguish any fire. In order to recover the impacted instances and network equipment, we needed to wait until the fire department was able to inspect the facility. After the fire department determined that there was no fire in the data center and it was safe to return, the building needed to be re-oxygenated before it was safe for engineers to enter the facility and restore the affected networking gear and servers. The fire suppression system that activated remains disabled. This system is designed to require smoke to activate and should not have discharged. This system will remain inactive until we are able to determine what triggered it improperly. In the meantime, alternate fire suppression measures are being used to protect the data center. \n\nOnce cooling was restored and the servers and network equipment was re-powered, affected instances recovered quickly. A very small number of remaining instances and volumes that were adversely affected by the increased ambient temperatures and loss of power remain unresolved. We continue to work to recover those last affected instances and volumes, and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery of those resources, we recommend replacing any remaining affected instances or volumes if possible.</div>",
      "service": "ec2-eu-central-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Frankfurt)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1623359538",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:12 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the eu-central-1 Region.</div><div><span class=\"yellowfg\"> 3:05 PM PDT</span>&nbsp;We have identified the root cause of the issue causing increased elevated error rates and latencies in the EU-CENTRAL-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:46 PM PDT</span>&nbsp;We are starting to see some recovery in API errors and latencies in the EU-CENTRAL-1 Region and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;Between 1:10 PM and 5:17 PM PDT we experienced increased error rates and latencies for EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-eu-central-1"
    },
    {
      "service_name": "Amazon Kinesis Firehose (Frankfurt)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1623360846",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:34 PM PDT</span>&nbsp;We are investigating increased API error rates in the eu-central-1 Region.</div><div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;We have identified the root cause of the issue causing increased elevated error rates in the eu-central-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:47 PM PDT</span>&nbsp;We are starting to see some recovery in API errors in the EU-CENTRAL-1 Regions and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;Between 1:10 PM PDT and 5:26 PM PDT we experienced increased error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "firehose-eu-central-1"
    },
    {
      "service_name": "Amazon Relational Database Service (Frankfurt)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1623363601",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:20 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some database instances in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 4:28 PM PDT</span>&nbsp;We are continuing to investigate connectivity affecting some instances in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 4:59 PM PDT</span>&nbsp;Network connectivity has been restored within the affected Availability Zone in the EU-CENTRAL-1 Region and we are starting to see some recovery of affected RDS instances. We are continuing to work toward full recovery of the remaining RDS instances.</div><div><span class=\"yellowfg\"> 5:50 PM PDT</span>&nbsp;Network connectivity has been restored within the affected Availability Zone (euc1-az1) in the EU-CENTRAL-1 Region and we have recovered the vast majority of affected RDS instances. We are continuing to work toward full recovery of the remaining RDS instances.\n</div><div><span class=\"yellowfg\"> 6:57 PM PDT</span>&nbsp;Between 1:10 PM and 5:45 PM PDT a small number of instances were unavailable in the EU-CENTRAL-1 Region. The issue has been resolved for nearly all instances. A small number of remaining instances are hosted on hardware which was adversely affected by increased ambient temperatures and loss of power. We continue to work to recover all affected instances and have opened notifications for the remaining impacted customers via the Personal Health Dashboard.</div>",
      "service": "rds-eu-central-1"
    },
    {
      "service_name": "AWS CloudFormation (Frankfurt)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1623370690",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:18 PM PDT</span>&nbsp;We are investigating elevated error rates for the AWS CloudFormation APIs in the EU-CENTRAL-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:38 PM PDT</span>&nbsp;We are starting to see some recovery in API errors and latencies in the EU-CENTRAL-1 Regions and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 5:46 PM PDT</span>&nbsp;Between 4:45 PM and 5:29 PM PDT, AWS CloudFormation experienced elevated error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-eu-central-1"
    },
    {
      "service_name": "AWS CodeBuild (N. Virginia)",
      "summary": "[RESOLVED] Increased Latency for Build Operations",
      "date": "1623692973",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:49 AM PDT</span>&nbsp;Beginning at 5:20 AM PDT, we experienced increased queue latency for Build operations in the US-EAST-1 Region. We have identified the subsystem responsible for the increase in latency, and have begun mitigation efforts. At this time, we are beginning to see signs of recovery, and continue to work toward full resolution.</div><div><span class=\"yellowfg\">11:24 AM PDT</span>&nbsp;Beginning at 5:15 AM we began experiencing increased latencies for Build operations. At 7:15 AM, we began notifying impacted customers via the Personal Health Dashboard. By 9:48 AM, we had processed the backlog of latent build operations, and at 10:20 AM, we verified that all new build operations were progressing normally. The issue has been resolved and the service is operating normally.</div>",
      "service": "codebuild-us-east-1"
    },
    {
      "service_name": "AWS CodeBuild (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rate when creating a build",
      "date": "1623763587",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:26 AM PDT</span>&nbsp;We are investigating an issue where customers may run into errors preventing them from creating new builds in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:02 AM PDT</span>&nbsp;We have mitigated the issue and are monitoring for recovery. </div><div><span class=\"yellowfg\"> 8:11 AM PDT</span>&nbsp;Changes have been reverted and we continue to monitor recovery. </div><div><span class=\"yellowfg\"> 8:31 AM PDT</span>&nbsp;Recently, AWS CodeBuild experienced elevated AccountLimitExceededException error rates when starting a build.  The issue was resolved at 8:10 AM PDT, and the service is operating normally.</div>",
      "service": "codebuild-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1623887035",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:43 PM PDT</span>&nbsp;We are investigating increased error rates for ELB APIs in the US-EAST-2 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\"> 5:30 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for ELB APIs in the US-EAST-2 Region and continue to work towards resolution. Connectivity to existing load balancers remains unaffected.</div><div><span class=\"yellowfg\"> 6:42 PM PDT</span>&nbsp;The increased error rates and latencies for ELB APIs in the US-EAST-2 Region are related to a Amazon Relational Database Service issue. Connectivity to existing load balancers remains unaffected. For further updates, please use the RDS Service Health Dashboard post.</div><div><span class=\"yellowfg\"> 9:40 PM PDT</span>&nbsp;Between 3:30 PM and 8:14 PM PDT, we experienced increased API error rates and latencies in the US-EAST-2 Region which were related to the Amazon Relational Database Service issue. The root cause has been addressed, and the service is operating normally.</div>",
      "service": "elb-us-east-2"
    },
    {
      "service_name": "Amazon Relational Database Service (Ohio)",
      "summary": "[RESOLVED] Instance Unavailablility",
      "date": "1623887565",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:52 PM PDT</span>&nbsp;We are investigating connectivity issues to some RDS instances in the US-EAST-2 region.</div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;We can confirm connectivity issues to some RDS instances in the US-EAST-2 Region, and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 6:04 PM PDT</span>&nbsp;We are actively working to mitigate connectivity issues for some RDS instances in the US-EAST-2 Region and continue to work towards resolution. The issue with Aurora PostgreSQL is also impacting Elasticache, ELB, API Gateway and Elastic Beanstalk APIs. Existing Elasticache, ELB and API Gateway instances continue to operate normally.</div><div><span class=\"yellowfg\"> 6:33 PM PDT</span>&nbsp;We have narrowed the cause of the connectivity issues impacting some RDS instances in the US-EAST-2 Region and we are continuing to work towards mitigation. The issue with Aurora PostgreSQL is also impacting Elasticache, ELB, API Gateway and Elastic Beanstalk APIs. Existing Elasticache, ELB and API Gateway instances continue to operate normally.</div><div><span class=\"yellowfg\"> 7:09 PM PDT</span>&nbsp;We have begun to see some recovery for the connectivity issues impacting some RDS instances in the US-EAST-2 Region, and we are continuing to work towards mitigation.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We are making progress towards restoring connectivity for the set of impacted Aurora PostgreSQL instances. Some database instances may see intermittent periods of recovery, but may not be fully recovered and we continue to work towards full recovery of these instances.</div><div><span class=\"yellowfg\"> 9:45 PM PDT</span>&nbsp;Between 3:30 PM and 9:30 PM PDT, some Amazon Relational Database Service (RDS) Aurora PostgreSQL instances in the US-EAST-2 Region experienced increased latency and fault rates. This issue also caused issues with Elasticache, ELB, and API Gateway APIs. Existing Elasticache, ELB, and API Gateway instances were not impacted by this event and were operating normally during this entire impact period. We have recovered the vast majority of affected instances and will contact any customers who are still impacted through the Personal Health Dashboard (PHD).</div>",
      "service": "rds-us-east-2"
    },
    {
      "service_name": "Amazon Simple Storage Service (Stockholm)",
      "summary": "[RESOLVED] Elevated IAM Error Rate",
      "date": "1623976331",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:32 PM PDT</span>&nbsp;We are investigating an elevated failure rate for AWS IAM authentication API requests in the EU-NORTH-1 Region.  This issue will also cause API requests for other AWS services in the EU-NORTH-1 Region to experience elevated failures. </div><div><span class=\"yellowfg\"> 5:45 PM PDT</span>&nbsp;We are seeing signs of recovery of the IAM authentication API error rates, and this is also showing improvement for other services.  We are actively working towards full mitigation.</div><div><span class=\"yellowfg\"> 5:52 PM PDT</span>&nbsp;The IAM authentication API error rates continue to improve, and many services have fully recovered.  We are actively working towards full mitigation.</div><div><span class=\"yellowfg\"> 5:58 PM PDT</span>&nbsp;Between 4:59 PM and 5:32 PM PDT we experienced elevated failure rate for AWS IAM authentication API requests in the EU-NORTH-1 Region. While some other services continue to move toward recovery, requests made to S3 are no longer experiencing elevated error rates. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-eu-north-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Stockholm)",
      "summary": "[RESOLVED] Elevated IAM Error Rate",
      "date": "1623977653",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:54 PM PDT</span>&nbsp;The IAM authentication API error rates continue to improve, and many services have fully recovered.  We are actively working towards full mitigation.</div><div><span class=\"yellowfg\"> 6:14 PM PDT</span>&nbsp;Starting at 4:59 PM PDT, the AWS IAM authentication API experienced elevated error rates in the EU-NORTH-1 Region. This issue caused AWS services to experience increased error rates, as they rely on the IAM service to authenticate their API requests. Services began to see recovery at 5:24 PM, with full recovery at 6:02 PM. All services are operating normally in the EU-NORTH-1 Region at this time.</div>",
      "service": "ec2-eu-north-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Stockholm)",
      "summary": "[RESOLVED] ELB API and Connectivity Issues",
      "date": "1623980954",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:49 PM PDT</span>&nbsp;Starting at 4:59 PM PDT, the AWS IAM authentication API experienced elevated error rates in the EU-NORTH-1 Region. This issue caused ELB to experience increased API error rates and latencies, as the APIs rely on the IAM service to authenticate requests. Recovery for the APIs began recovery at 5:24 PM, with full recovery at 6:11 PM. Between 6:03 PM and 6:28 PM PDT, we experienced an increase in connectivity issues and WAF errors for some load balancers. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\"> 6:51 PM PDT</span>&nbsp;Starting at 4:59 PM PDT, the AWS IAM authentication API experienced elevated error rates in the EU-NORTH-1 Region. This issue caused ELB to experience increased API error rates and latencies, as the APIs rely on the IAM service to authenticate requests. Recovery for the APIs began recovery at 5:24 PM, with full recovery at 6:11 PM. Between 6:03 PM and 6:28 PM PDT, we experienced an increase in connectivity issues and WAF errors for some load balancers. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-eu-north-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Hong Kong)",
      "summary": "[已解決] API 錯誤率增加 | [已解决] API 错误率增加 | [RESOLVED] Increased API Error Rates",
      "date": "1623995010",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:43 PM PDT</span>&nbsp;我们正在调查AP-EAST-1区域中EC2 API错误率上升的原因 | 我們正在調查AP-EAST-1區域中EC2 API錯誤率上升的原因 | We are investigating increased API error rates for the EC2 APIs in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">10:51 PM PDT</span>&nbsp;我们确认在 AP-EAST-1 区域内的单个可用区中 EC2 API 的错误率上升和 某些EBS卷的性能下降 | 我們確認在 AP-EAST-1 區域內的單個可用區中 EC2 API 的錯誤率上升和 某些EBS卷的性能下降 | We can confirm increased error rates for the EC2 APIs and degraded performance for some EBS volumes within a single Availability Zone within the AP-EAST-1 Region</div><div><span class=\"yellowfg\">11:08 PM PDT</span>&nbsp;我们持续调查 EC2 API 的错误率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 区域中单个可用区 (ape1-az2) 内的 EC2 实例连接问题。 AP-EAST-1 区域内的其他可用区不受此事件的影响。|我們持續調查 EC2 API 的錯誤率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 區域中單個可用區 (ape1-az2) 內的 EC2 實例連接問題。 AP-EAST-1 區域內的其他可用區不受此事件的影響。| We continue to investigate increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. Other Availability Zones within the AP-EAST-1 Region are not affected by this event. </div><div><span class=\"yellowfg\">11:27 PM PDT</span>&nbsp;我们持续调查 EC2 API 的错误率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 区域中单个可用区 (ape1-az2) 内的 EC2 实例连接问题。 我们正在确认此事件的根本原因，但现阶段还无法确定。由于 AP-EAST-1 区域内的其他可用区不受此事件的影响，我们建议您暂时不要使用受影响的可用区。| 我們持續調查 EC2 API 的錯誤率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 區域中單個可用區 (ape1-az2) 內的 EC2 實例連接問題。 我們正在確認此事件的根本原因，但現階段還無法確定。由於 AP-EAST-1 區域內的其他可用區不受此事件的影響，我們建議您暫時不要使用受影響的可用區。| We continue to investigate increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. We are making progress towards determining root cause for this event, but have not been able to determine it at this stage. Since other Availability Zones within the AP-EAST-1 Region are not affected by this event, we recommend that you fail away from the affected Availability Zone.</div><div><span class=\"yellowfg\">Jun 18, 12:03 AM PDT</span>&nbsp;我们持续调查 EC2 API 的错误率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 区域中单个可用区 (ape1-az2) 内的 EC2 实例连接问题。 我们正在确认此事件的根本原因，我们认为这与 EC2 实例和附加 EBS 卷之间的通信有关。这会导致可用区内受影响 EBS 卷的性能下降，进而发生操作系统内的 IO 卡住而导致 EC2 实例受损。在我们继续努力解决问题的同时，我们建议您从受影响的可用区进行故障转移。 | 我們持續調查 EC2 API 的錯誤率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 區域中單個可用區 (ape1-az2) 內的 EC2 實例連接問題。 我們正在確認此事件的根本原因，我們認為這與 EC2 實例和附加 EBS 卷之間的通信有關。這會導致可用區內受影響 EBS 卷的性能下降，進而發生操作系統內的 IO 卡住而導致 EC2 實例受損。在我們繼續努力解決問題的同時，我們建議您從受影響的可用區進行故障轉移。 | We continue to investigate increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. We are making progress in determining the root cause and believe it is related to communication between EC2 instances and attached EBS volumes. This leads to degraded performance for affected EBS volumes within the Availability Zone and can also lead to impaired EC2 instances due to stuck IO within the operating system. While we continue to work towards resolving the issue, we recommend that you fail away from the affected Availability Zone if you have not already done so.</div><div><span class=\"yellowfg\">Jun 18, 12:47 AM PDT</span>&nbsp;我們已確定導致 EC2 API 的錯誤率增加、AP-EAST-1 區域中某些 EBS 磁碟區和 EC2 實例連線問題的效能降低問題的根本原因。在受影響的可用區內的某些 EBS 儲存伺服器受損，導致受影響的 EBS 磁碟區效能降低。我們正在採取措施解決 EBS 儲存伺服器損害，這應該能開始解決受影響的 EC2 實例和 EBS 磁碟區的問題。在我們繼續努力解決問題的同時，我們建議您暫時不要使用受影響的可用區。| We have determined the root cause of the issue causing increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. Some EBS storage servers within the affected Availability Zone are impaired, which is causing the degraded performance for the affected EBS volumes. We are taking steps to address the EBS storage server impairments, which should begin to resolve the issue for affected EC2 instances and EBS volumes. We continue to recommend that you fail away from the affected Availability Zone if you have not already done so.</div><div><span class=\"yellowfg\">Jun 18, 12:57 AM PDT</span>&nbsp;我們持續看到受影響的可用性區域內 EBS 磁碟區的效能降低。絕大多數受影響的 EBS 磁碟區現在已經復原，我們正在處理剩餘的 EBS 磁碟區。雖然大部分的服務現在都在受影響的可用性區域內看到復原，但我們還是不建議在完成完整復原之前，還是不建議回復至可用區域。我們將繼續為您提供更新。\n我們已經開始採取措施來解決 EBS 儲存伺服器問題，並且正在看到某些受影響的 EBS 磁碟區的復原。我們將繼續處理剩餘的受損 EBS 儲存伺服器，以完全解決問題. | We have begun taking steps to address the EBS storage server impairments, and are seeing recovery for some of the affected EBS volumes. We will continue to work on remaining impaired EBS storage servers to fully resolve the issue.</div><div><span class=\"yellowfg\">Jun 18,  1:27 AM PDT</span>&nbsp;我们继续看到受影响的可用区内 EBS 卷的性能下降。大多数受影响的 EBS 卷现已恢复，我们将继续努力恢复其余受影响的卷。我们继续努力争取全面解决这个问题 | 我們持續看到受影響的可用性區域內 EBS 磁碟區的效能降低。大部分受影響的 EBS 磁碟區現在都已經復原，我們會繼續修復其餘受影響的磁碟區。我们继续努力全面解决这一问题 | We continue to see an improvement in degraded performance for EBS volumes within the affected Availability Zone. The majority of the affected EBS volumes have now been recovered and we continue to work on recovering the remaining affected volumes. We continue to work toward full resolution of the issue.</div><div><span class=\"yellowfg\">Jun 18,  1:53 AM PDT</span>&nbsp;我们继续看到受影响的可用区内 EBS 卷的性能下降。绝大多数受影响的 EBS 卷现已恢复，我们正在处理剩余的 EBS 卷。虽然大多数服务现在都在受影响的可用区内看到恢复，但我们建议在完成完全恢复之前，不要回到可用区。我们将继续为您提供最新信息. 我们已开始采取措施来解决 EBS 存储服务器损坏问题，并且看到一些受影响的 EBS 卷正在恢复。我们将继续处理剩余受损的 EBS 存储服务器，以充分解决此问题。| We continue to see an improvement in degraded performance for EBS volumes within the affected Availability Zone. The vast majority of affected EBS volumes have now been recovered and we are working on the remaining EBS volumes. While most services are now seeing recovery within the affected Availability Zone, we do not yet recommend failing back to the Availability Zone until we have completed full recovery. We will continue to provide you with updates.</div><div><span class=\"yellowfg\">Jun 18,  2:36 AM PDT</span>&nbsp;从 12:58 PM UTC+8 开始，在 AP-EAST-1 地区的一个可用区（ape1-az2）内，我们遇到了 EC2 APIs 的错误率增加、一些 EBS 卷的性能下降和 EC2 实例连接问题。该问题的根本原因是受影响的可用性区域内底层 EBS 存储服务器的性能下降。工程师采取了行动，缓解和解决 EBS 存储服务器性能下降的问题，从而解决了这个问题。在 03:45 PM UTC+8 性能下降的 EBS 卷开始恢复，到 03:58 PM UTC+8，绝大部分受影响的 EBS 卷已经恢复。我们继续在少数仍在经历性能下降的 EBS 卷上工作，并将通过个人健康仪表板或这些卷提供进一步的更新。所有的服务现在都在受影响的可用性区域内正常运行。这个问题已经得到解决，服务运行正常。| 從 12:58 PM UTC+8 開始，在 AP-EAST-1 地區的一個可用區（ape1-az2）內，我們遇到了 EC2 APIs 的錯誤率增加、一些 EBS 卷的性能下降和 EC2 實例連接問題。該問題的根本原因是受影響的可用性區域內底層 EBS 存儲服務器的性能下降。工程師採取了行動，緩解和解決 EBS 存儲服務器性能下降的問題，從而解決了這個問題。在 03:45 PM UTC+8 性能下降的 EBS 捲開始恢復，到 03:58 PM UTC+8，絕大部分受影響的 EBS 卷已經恢復。我們繼續在少數仍在經歷性能下降的 EBS 捲上工作，並將通過個人健康儀表板或這些卷提供進一步的更新。所有的服務現在都在受影響的可用性區域內正常運行。這個問題已經得到解決，服務運行正常。| Starting at 9:58 PM PDT we experienced increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. The root cause of the issue was degraded performance for underlying EBS storage servers within the affected Availability Zone. Engineers took action to mitigate and resolve the degraded EBS storage server performance, which resolved the issue. At 12:41 AM PDT, EBS volumes with degraded performance began to recover and by 12:58 AM PDT, the vast majority of affected EBS volumes had recovered. We continue to work on a small number of EBS volumes that are still experiencing degraded performance, and will provide further updates via the Personal Health Dashboard for those volumes. All services are now operating normally within the affected Availability Zone. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-east-1"
    },
    {
      "service_name": "Amazon Relational Database Service (Hong Kong)",
      "summary": "[已解决] 实例不可用 ｜[已解決] 實例不可用 ｜[RESOLVED] Instance Unavailability",
      "date": "1623996605",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:10 PM PDT</span>&nbsp;我們正在調查 AP-EAST-1 區域中影響某些實例的連接問題 |We are investigating connectivity issues affecting some instances in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">11:55 PM PDT</span>&nbsp;我們持續調查 AP-EAST-1 區域中，影響到單一可用區(ape1-az2)實例的連接問題，這問題與許多 EC2 API 的錯誤有關係。| We continue to investigate connectivity issues affecting instances in a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. The issues are related to the increased EC2 API error rates.</div><div><span class=\"yellowfg\">Jun 18,  2:51 AM PDT</span>&nbsp; 從 12:58 PM UTC+8 開始，我們遇到了影響 AP-EAST-1 區域的單一可用區域 (ape1-az2) 執行個體的連線能力和 EBS 儲存問題。問題的根本原因與 EC2 API 錯誤率的增加有關。恢復後的 EC2，並且在 5:45PM UTC+8之前，絕大多數受影響的執行個體已經復原。我們會繼續處理少數仍遇到問題的執行個體，並透過個人健康儀表板提供進一步的更新。問題已解決，服務正常運作。| Starting at 9:58 PM PDT, we experienced connectivity and EBS storage issues affecting instances in a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. The root cause of the issue was related to increased EC2 API error rates. Recovery followed EC2's, and by 2:45 AM PDT, the vast majority of affected instances had recovered. We continue to work on a small number of instances that are still experiencing issues, and will provide further updates via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-ap-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Ohio)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1624564393",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:53 PM PDT</span>&nbsp;Between 12:15 PM and 12:21 PM PDT we observed connectivity issues between some customer networks and the US-EAST-2 Region. Connectivity within the Region was not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-east-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Network Connectivity Issues",
      "date": "1624992057",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:40 AM PDT</span>&nbsp;Between 11:07 AM and 11:15 AM PDT we experienced a network connectivity issue that impacted Amazon VPC Inter-Region Peering in the US-EAST-2 Region. Connectivity to instances and services within the Region was not impacted by the event. The issue has been resolved and connectivity has been restored. </div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS Certificate Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latency",
      "date": "1624993969",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:12 PM PDT</span>&nbsp;We are investigating increased API latency and error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:36 PM PDT</span>&nbsp;We can confirm increased API latency and increased API error rates for the ACM APIs in the US-EAST-1 Region. During this time, you may be unable to Request new certificates, and may also observe errors when attempting to List and/or nodify existing certificates. This issue impacts both the AWS Management Console, and the ACM APIs. Additionally, you may also receive API errors when attempting to associate new resources. Existing associated resources are unaffected, and continue to operate as normal. We have identified the root cause of the issue and are working toward mitigation and resolution. We will provide further updates as we have more information to share.</div><div><span class=\"yellowfg\"> 1:14 PM PDT</span>&nbsp;We continue to work toward mitigating the affected subsystem responsible for the increase in API Errors and Latencies for the ACM APIs. Other AWS Services (such as ClientVPN) who attempt to create or associate new certificates may also be impacted by this issue. Existing resources remain unaffected by this issue and continue to operate normally. </div><div><span class=\"yellowfg\"> 2:13 PM PDT</span>&nbsp;We are continuing to drive to root cause and work toward mitigating the affected subsystem responsible for the increase in API Errors and Latencies for the ACM APIs. Other AWS Services (such as ClientVPN) who attempt to create or associate new certificates may also be impacted by this issue. Existing resources remain unaffected by this issue and continue to operate normally.</div><div><span class=\"yellowfg\"> 2:56 PM PDT</span>&nbsp;We have identified some workloads on the affected subsystem of the ACM API that may be causing the increase in API errors and latency, and we are reviewing and testing procedures to mitigate their impact.  We do not have an ETA at this time.  This issue does affect services like CloudFront and ElasticSearch that rely on ACM for their certificate needs. It would also impact CloudFormation workflows that either directly or indirectly need to manipulate ACM certificates.\n\nAll workflows that depend on ACM certificates that are already created are not impacted by this event, and continue to operate normally.</div><div><span class=\"yellowfg\"> 3:33 PM PDT</span>&nbsp;We continue to work toward mitigating the increased latencies and error rates affecting the ACM APIs. Until this point, some requests and retries have been succeeding. At this time, we are temporarily not accepting additional API requests, in order to help accelerate mitigation and recovery. Once we begin accepting new API requests, requests will be throttled. We will continue to provide updates as we progress.</div><div><span class=\"yellowfg\"> 4:49 PM PDT</span>&nbsp;We are starting to see some ACM API calls succeed for CloudFront and ELB and we are starting to propagate changes for CloudFront distributions to our edge locations.  Customer facing APIs are still throttled.  ACM is continuing to make progress towards recovery.  </div><div><span class=\"yellowfg\"> 5:47 PM PDT</span>&nbsp;We are seeing recovery for customers and throttling has been removed from most APIs.  We are working through the final changes to unblock the following APIs: RequestCertificate, ListCertificates, and ImportCertificate and expect to have those final changes in-place shortly. We will update as we make progress towards full recovery. </div><div><span class=\"yellowfg\"> 7:05 PM PDT</span>&nbsp;We are seeing recovery for customers and throttling has been removed from most APIs.  We have unblocked RequestCertificate for most use cases and are working to have the final changes in-place shortly. We will update as we make progress towards full recovery.</div><div><span class=\"yellowfg\"> 7:46 PM PDT</span>&nbsp;Between 11:45 AM and 7:42 PM PDT, customers experienced increased ACM API errors and latency in the US-EAST-1 Region that impacted the ability to issue new certificates, import certificates and retrieve information about certificates from ACM. Existing certificates that were already vended to services such as CloudFront and ELB continued to operate and were unaffected. This issue also impacted provisioning and scaling workflows for services that depend on ACM for certificate management needs, such as CloudFront and ELB, as well as CloudFormation operations that involve mutating ACM certificates. This issue was caused by a previously unknown limit in an ACM storage subsystem. We have identified the limit issue and have mitigated it.  The issue has been fully resolved and all ACM API requests are being answered normally.  During this time, all existing resources that had a configured ACM certificate (such as ELB load balancers and CloudFront distributions) continued to operate normally, and were not impaired by this issue.</div>",
      "service": "certificatemanager-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Propagation Delays",
      "date": "1625000294",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:58 PM PDT</span>&nbsp;We are investigating delays in propagating changes to CloudFront distributions to our edge locations. This is related to the ACM issue in the US-EAST-1 Region that we have posted to the Service Health Dashboard. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 4:08 PM PDT</span>&nbsp;CloudFront distribution changes continue to be affected by the AWS Certificate Manager issue in US-EAST-1. ACM is continuing to make progress towards recovery, but change propagation for CloudFront distribution changes will continue to be affected until the ACM issue is fully mitigated. At that point, we will begin to process the backlog of distribution changes. The backlog of changes may take additional time to complete. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 5:06 PM PDT</span>&nbsp;We are starting to see some ACM API calls succeed for CloudFront and we are starting to propagate changes for CloudFront distributions to our edge locations. ACM is continuing to make progress towards recovery. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 6:11 PM PDT</span>&nbsp;We continue to process the backlog of distribution changes and propagate the updates to our edge locations. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 7:38 PM PDT</span>&nbsp;Between 11:45 AM and 7:25 PM PDT, we experienced delays in propagating changes to CloudFront distributions to our edge locations due to the ACM issue in US-EAST-1. This issue has been resolved and the service is operating normally. During this time, previous configured distributions continued to operate without any issues and there were no issues with serving content from our CloudFront edge locations.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Frankfurt)",
      "summary": "[RESOLVED] Instance connectivity",
      "date": "1626179354",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:29 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs and connectivity issues for some instances in a single Availability Zone in the EU-CENTRAL-1 Region</div><div><span class=\"yellowfg\"> 6:05 AM PDT</span>&nbsp;We are seeing increased error rates and latencies for the RunInstances and CreateSnapshot APIs, and increased connectivity issues for some instances in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. We have resolved the networking issues that affected the majority of instances within the affected Availability Zone, but continue to work on some instances that are experiencing degraded performance for some EBS volumes. Other Availability Zones are not affected by this issue. We would recommend failing away from the affected Availability Zone until this issue has been resolved.</div><div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We continue to make progress in resolving the connectivity issues affecting some instances in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. The increased error rates and latencies for the RunInstance and CreateSnapshot APIs have been resolved, as well as the degraded performance for some EC2 volumes within the affected Availability Zone. We continue to work on the remaining EC2 instances that are still impaired as a result of this event, some of which may have experienced a power cycle. While we do not expect any further impact at this stage, we would recommend continuing to utilize other Availability Zones in the EU-CENTRAL-1 region until this issue has been resolved.</div><div><span class=\"yellowfg\"> 7:24 AM PDT</span>&nbsp;Starting at 5:07 AM PDT we experienced increase connectivity issues for some instances, degraded performance for some EBS volumes and increased error rates and latencies for the EC2 APIs in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. By 6:03 AM PDT, API error rates had returned to normal levels, but some Auto Scaling workflows continued to see delays until 6:35 AM PDT. By 6:10 AM PDT, the vast majority of EBS volumes with degraded performance had been resolved as well, and by 7:05 AM PDT, the vast majority of affected instances had been recovered, some of which may have experienced a power cycle. A small number of remaining instances are hosted on hardware which was adversely affected by this event and require additional attention. We continue to work to recover all affected instances and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances if possible.</div>",
      "service": "ec2-eu-central-1"
    },
    {
      "service_name": "Amazon CloudWatch (Tokyo)",
      "summary": "[RESOLVED] エラー率およびレイテンシーの上昇 | Increased Error rates and Latencies",
      "date": "1626280770",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:39 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの増加を調査しています。現在この問題の解決に取り組んでいます。 | We are investigating increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:04 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける API エラー率の上昇およびログイベントの遅延を確認しています。現在この問題の解決に取り組んでいます。| We can confirm elevated API error rates and some delayed log events in AP-NORTHEAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける API エラー率の上昇およびログイベントの遅延の原因を特定しました。復旧の兆候を確認しており、解決に向け引き続き対応を行っています。 | We have identified the root cause of the elevated API error rates and some delayed log events in the AP-NORTHEAST-1 Region. We are beginning to see signs of recovery and continue working towards resolution.</div><div><span class=\"yellowfg\">12:07 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの増加について引き続き調査および解決に取り組んでいます。| We continue to work on investigating and resolving increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 2:28 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの上昇を解消すると予測される緩和策の実施を完了しました。エラー率の改善が見られる一方で引き続き高い水準のレイテンシーを確認しており、完全な解決に向け引き続き対応を行っています。 | We have completed a mitigation strategy that we anticipated would resolve the increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. While we have seen some recovery in error rates, we continue to see elevated levels of latency and continue working towards full resolution.</div><div><span class=\"yellowfg\"> 3:42 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のレイテンシー上昇と残存しているエラーの解消のため、第二の緩和策の展開を行なっております。解決に向け引き続き対応を行っております。 | We are deploying a second mitigation strategy to resolve elevated latency and the remaining level of errors for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. We continue working towards recovery.</div><div><span class=\"yellowfg\"> 5:05 PM PDT</span>&nbsp;日本時間 2021/07/14 23:01 から 2021/07/15 08:20 の間、AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの上昇が発生しておりました。この問題は解決し、現在サービスは正常に動作しています。 | Between 7:01 AM and 4:20 PM PDT we experienced increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudwatch-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Milan)",
      "summary": "[RESOLVED] Increased Error rates",
      "date": "1626910891",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;We are investigating increased API error rates for the RunInstances API in the EU-SOUTH-1 Region. </div><div><span class=\"yellowfg\"> 5:04 PM PDT</span>&nbsp;We can confirm increased API error rates for the RunInstances API in the EU-SOUTH-1 Region. This is also affecting services that depend on EC2 such as Auto Scaling, and launches of service instances that are built on EC2, such as RDS and ElastiCache. Instances that are already launched are operating normally. We have identified the root cause and are actively testing a mitigation plan. We expect to have an update on the success of this mitigation effort in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:19 PM PDT</span>&nbsp;Between 3:59 PM and 5:07 PM PDT customers experienced increased error rates for the EC2 RunInstances API in the EU-SOUTH-1 Region. This is also affected services that depend on EC2 such as Auto Scaling, and launches of service instances that are built on EC2, such as RDS and ElastiCache. The issue has been resolved and the RunInstances API is now operating normally.  This issue only affected new instance launches, instances that were already running were not affected. </div>",
      "service": "ec2-eu-south-1"
    },
    {
      "service_name": "AWS Lambda (Milan)",
      "summary": "[RESOLVED] Increased Invoke Error Rate",
      "date": "1628028846",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:14 PM PDT</span>&nbsp;We are investigating increased invoke error rates in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 3:45 PM PDT</span>&nbsp;Between 2:39 PM and 3:16 PM PDT, we experienced increased invoke error rates in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "lambda-eu-south-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Management Console Error Rates",
      "date": "1628029136",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:18 PM PDT</span>&nbsp;We are investigating increased errors for AWS Management Console in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 3:46 PM PDT</span>&nbsp;Between 2:38 PM and 3:16 PM PDT, we experienced increased errors for AWS Management Console in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon ElastiCache (Ireland)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1628216388",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:19 PM PDT</span>&nbsp;We are experiencing increased latencies while provisioning new ElastiCache nodes and elevated API error rates in the EU-WEST-1 Region. Existing ElastiCache clusters are not impacted and are continuing to serve traffic. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:33 PM PDT</span>&nbsp;Between 6:22 PM and 7:31 PM PDT, Amazon ElastiCache experienced increased latencies while provisioning new ElastiCache nodes and elevated API error rates in the EU-WEST-1 Region. Existing ElastiCache clusters were not impacted and continued to serve traffic. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticache-eu-west-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1628298493",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:08 PM PDT</span>&nbsp;We are investigating increased error rate and latencies for DynamoDB in US-EAST-1 region.</div><div><span class=\"yellowfg\"> 6:32 PM PDT</span>&nbsp;We can confirm increased error rates for DynamoDB in the US-EAST-1 Region. We have identified the root cause of the issue and are working towards resolution.</div><div><span class=\"yellowfg\"> 7:03 PM PDT</span>&nbsp;We have seen some improvement to the error rates for DynamoDB in the US-EAST-1 Region and continue to work towards full resolution. For customers that are experiencing 503 errors, retries may resolve the issue in some cases. In other cases, recreating the connection to DynamoDB may address the error rates. We continue to take steps towards full resolution for all affected tables.</div><div><span class=\"yellowfg\"> 8:11 PM PDT</span>&nbsp;We continue to make progress in addressing the increased error rates for DynamoDB in the US-EAST-1 Region. The root cause of the issue is a problem with the metadata subsystem used by DynamoDB, where several nodes are in an unhealthy state. We continue to work towards restoring the health of these nodes. The issue affected a subset of DynamoDB tables that are associated with the unhealthy nodes in the metadata subsystem. For these tables, customers will experience increased error rates until we have resolved the issue. DynamoDB tables that are not associated with the affected metadata nodes, are not affected by this issue. We continue to work towards full resolution.</div><div><span class=\"yellowfg\"> 9:00 PM PDT</span>&nbsp;We continue to see an improvement in the error rates for affected DynamoDB tables in the US-EAST-1 Region. Since the start of the event, we have seen a 75% reduction in error rates and are now working on resolving the errors for the remaining DynamoDB tables. </div><div><span class=\"yellowfg\">10:20 PM PDT</span>&nbsp;We have resolved the error rates for the majority of the affected DynamoDB tables and now have a small number of DynamoDB tables that are still experiencing error rates and a small number of global secondary indexes that are experiencing propagation delays. While all the nodes in the metadata store are now healthy, some are not yet able to process incoming requests, which we are working to resolve.</div><div><span class=\"yellowfg\">11:11 PM PDT</span>&nbsp;We have now resolved the error rates affecting DynamoDB tables in the US-EAST-1 Region. A small number of DynamoDB tables continue to experience delayed propagation for global secondary indexes, but these are moving towards full recovery as well. We’re continuing to monitor the service, but customers should be seeing recovery for their DynamoDB tables at this stage.</div><div><span class=\"yellowfg\">Aug 7,  1:50 AM PDT</span>&nbsp;We have now resolved the Global Secondary index propagation delays for most customers in the US-EAST-1 region. Our mitigation efforts are working as expected and we continue to work towards full recovery.</div><div><span class=\"yellowfg\">Aug 7,  2:53 AM PDT</span>&nbsp;Between August 6 5:23 PM and August 7 2:48 AM PDT, DynamoDB customers experienced API errors and delayed propagation for global secondary indexes in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "dynamodb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Kubernetes Service (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1628306280",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:18 PM PDT</span>&nbsp;We are investigating increased error rates for Cluster create and update operations in the US-EAST-1 Region. Some existing clusters are also impacted.</div><div><span class=\"yellowfg\"> 9:30 PM PDT</span>&nbsp;We are continuing to investigate increased error rates for Cluster create and update operations in the US-EAST-1 Region. We are seeing recovery for some clusters, and continue to work on recovery for all clusters.</div><div><span class=\"yellowfg\">10:34 PM PDT</span>&nbsp;We can confirm errors creating and updating Clusters in the US-EAST-1 Region, and are continuing to work towards resolution. We are starting to see recovery for existing clusters, and continue to work on recovery for all clusters.</div><div><span class=\"yellowfg\">11:33 PM PDT</span>&nbsp;We continue to see improvement in the error rates for creating and upgrading clusters in the US-EAST-1 Region. We are starting to see recovery in availability of existing clusters. We are continuing to work towards a full resolution.</div><div><span class=\"yellowfg\">Aug 7, 12:27 AM PDT</span>&nbsp;Between August 6 5:50 PM PDT and 11:27 PM PDT, we experienced errors for creating and upgrading clusters in the US-EAST-1 Region. Some existing clusters were also impacted during this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "eks-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1628306461",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:21 PM PDT</span>&nbsp;We are investigating increased provisioning latencies for new load balancers, delayed propagation of load balancer CloudWatch metrics, and inconsistent health check statuses from the DescribeTargetHealth API for Network Load Balancers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:08 PM PDT</span>&nbsp;We can confirm delayed propagation of load balancer CloudWatch metrics and provisioning latencies for Network Load Balancers in the US-EAST-1 Region, and are continuing to work towards resolution. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\">11:22 PM PDT</span>&nbsp;Between 6:08 PM and 11:12 PM PDT, we experienced increased provisioning latencies for Network Load Balancers in the US-EAST-1 Region, which has now fully recovered. We are continuing to experience delays in the propagation of CloudWatch metrics for load balancers. The system used to generate these metrics has begun to recover, and we anticipate full recovery in the next few hours after we process the backlog of metrics.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] DescribeImages API",
      "date": "1628699197",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:26 AM PDT</span>&nbsp;We are investigating an issue where the ‘virtualization type’ field is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. The virtualization type for some images is being returned as “paravirt” instead of “hvm”. This can impact the instance selection in the console and also cause some EMR jobs to fail to start as EMR does not support “paravirt” images. We have identified the root cause of the issue and are working to resolve it. Note that the underlying images (AMI) are not affected by this issue, which has only affected the returning of the metadata for the ‘virtualization type’ field. Once resolved, images will continue to operate as they did before. </div><div><span class=\"yellowfg\">10:25 AM PDT</span>&nbsp;We continue to make progress in resolving the issue where the virtualization type is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. The issue will prevent some images from launching on the current generation (e.g. c5, m5, r5, t3, etc.) instance types and some previous generation instances (e.g t2) as they do not support “paravirt” virtualization. Customers could choose to launch on a previous generation instance type that does support ‘paravirt’ virtualization, or attempt launching via the EC2 command line tools, which in many cases when allow for the instance to be launched. We continue to work on resolving the issue.</div><div><span class=\"yellowfg\">11:01 AM PDT</span>&nbsp;We have begun to see recovery for some of the affected images (AMI), which are now returning the correct virtualization type. We expect to see full recovery within the next hour, at which time all operations affected by this issue should be operating normally. We will provide an update once we have fully recovered.</div><div><span class=\"yellowfg\">11:24 AM PDT</span>&nbsp;We have resolved the issue where the virtualization type is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. Starting at 2:43 AM PDT, some images (AMI) returned the incorrect virtualization type (‘paravirt’ instead of ‘hvm’) in a DescribeImages API call. Since some instance types do not support ‘paravirt’ images, this prevented the EC2 Management Console and some EMR jobs from being able to launch new EC2 instances. As of 11:17 AM PDT, the issue has been fully resolved. Instances launched during this time do not need to be relaunched. The issue has been resolved and the service is operating normally.\n</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1629201355",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:55 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:24 AM PDT</span>&nbsp;We can confirm increased API error rates for ECS in the US-EAST-1 Region. We have identified the issue and are working towards mitigation. In most cases, a retry of the API operation will succeed.</div><div><span class=\"yellowfg\"> 6:15 AM PDT</span>&nbsp;We can confirm increased API error rates for ECS in the US-EAST-1 Region. We have identified the root cause and continue to work towards recovery. In most cases, a retry of the API operation will succeed.\n</div><div><span class=\"yellowfg\"> 7:12 AM PDT</span>&nbsp;Between 3:40 AM and 6:45 AM PDT we experienced increased API error rates for ECS in the US-EAST-1 Region. In most cases, API calls succeeded when retried.  We continue to work to recover residual impact and have opened notifications for the remaining impacted customers via the Personal Health Dashboard.</div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "Amazon EventBridge (N. Virginia)",
      "summary": "[RESOLVED] Elevated Event Delivery Latency",
      "date": "1629229180",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:39 PM PDT</span>&nbsp;We can confirm elevated event delivery latency for EventBridge events in the US-EAST-1 Region. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency. </div><div><span class=\"yellowfg\"> 1:53 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We have taken initial mitigation actions, which have reduced the delivery latency for some requests, and continue to work toward full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 3:12 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We are seeing further reduction of the delivery latency for some requests. We are exploring further mitigation steps as we continue to work toward full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 4:39 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We are seeing further reduction of the delivery latency for more requests. We have identified the component that is contributing to the event delivery latency and are mitigating impact within this component as we work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 5:36 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We continue to observe further reduction of the delivery latency for more requests. We have completed additional mitigation steps that have led to the reduction in delivery latency and continue to work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 6:17 PM PDT</span>&nbsp;We continue to take mitigation actions to improve elevated event delivery latency for EventBridge events in the US-EAST-1 Region. While we observe improvement in event delivery latency, we are still experiencing elevated latency in the component responsible for matching events and continue to work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 7:07 PM PDT</span>&nbsp;We previously took steps towards mitigating event delivery latency based upon the suspected root cause and observed improvements in event delivery latency in the US-EAST-1 Region. We have now identified that we are continuing to see event delivery latency due to the component responsible for matching events with EventBridge rules and are taking further steps to mitigate the issue as we work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 8:06 PM PDT</span>&nbsp;We have completed our actions towards mitigating event delivery latency for EventBridge events in the US-EAST-1 Region. Event delivery latency for new events has fully recovered and we are processing the backlog of previously published events. Other AWS Services that make use of EventBridge are observing improvements in elevated event delivery latency as well.\n</div><div><span class=\"yellowfg\"> 9:05 PM PDT</span>&nbsp;Between 8:31 AM and 9:00 PM PDT, we experienced elevated event delivery latency for EventBridge events in the US-EAST-1 Region. Other AWS services that make use of EventBridge are no longer experiencing elevated event delivery latency. The issue has been resolved and the service is operating normally. </div>",
      "service": "events-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] IAM errors and propagation latency",
      "date": "1629996723",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:52 AM PDT</span>&nbsp;We are investigating elevated latencies and errors on the IAM APIs. In addition, we are investigating propagation delays for recently created or recently updated IAM users, credentials, roles, policies. Authentication and authorization of existing users, credentials, roles, policies are not impacted. Other AWS services like AWS CloudFormation that use IAM roles were also impacted.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;Between 6:44 AM and 9:12 AM PDT, customers experienced elevated latency and error rates in response to IAM API requests, as well as delays in describing recently created or modified IAM resources. In addition, between 6:44 AM and 10:02 AM, propagation of IAM API updates was delayed in the ME-SOUTH-1, EU-SOUTH-1, AP-EAST-1, and AF-SOUTH-1 regions, and newly created or recently updated IAM users, credentials, roles, and policies may not have been available for authentication and authorization in those regions. Other AWS Services that rely on IAM changes to provision resources, such as CloudFormation, were also impacted. Authentication and authorization for existing users, credentials, roles, policies were not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "AWS Internet Connectivity (Oregon)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1630434320",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:25 AM PDT</span>&nbsp;We are investigating an issue which is affecting network traffic for some customers using AWS services in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:13 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. While we continue to work towards root cause, we believe that the issue is affecting connectivity to Network Load Balancers from EC2 instances, connectivity from Lambda to EC2 instances and other AWS services, as well as connectivity between EC2 and some AWS services using PrivateLink. In an effort to further mitigate the impact, we are shifting some services and network flows away from the affected Availability Zone to mitigate the impact.</div><div><span class=\"yellowfg\"> 1:00 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. We have narrowed down the issue to an increase in packet loss within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services. The issue continues to only affect the single Availability Zone (usw2-az2) within the US-WEST-2 region, so shifting traffic away from Networking Load Balancer and NAT Gateway within the affected Availability Zone can mitigate the impact. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, are seeing impact as a result of this issue. </div><div><span class=\"yellowfg\"> 2:26 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region and are actively working on mitigation. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in usw2-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in usw2-az2.</div><div><span class=\"yellowfg\"> 3:23 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:02 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. Beginning at 10:58 AM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the US-WEST-2 Region. At 2:45 PM, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:35 PM, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Oregon)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1630435612",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:46 AM PDT</span>&nbsp;We are investigating connectivity, provisioning, and target registration issues for load balancers in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:10 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. While we continue to work towards root cause, we believe that the issue is affecting connectivity to Network Load Balancers from EC2 instances, connectivity from Lambda to EC2 instances and other AWS services, as well as connectivity between EC2 and some AWS services using PrivateLink. In an effort to further mitigate the impact, we are shifting some services and network flows away from the affected Availability Zone to mitigate the impact.</div><div><span class=\"yellowfg\"> 1:00 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. We have narrowed down the issue to an increase in packet loss within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services. The issue continues to only affect the single Availability Zone (usw2-az2) within the US-WEST-2 region, so shifting traffic away from Networking Load Balancer and NAT Gateway within the affected Availability Zone can mitigate the impact. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, are seeing impact as a result of this issue. </div><div><span class=\"yellowfg\"> 2:24 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region and are actively working on mitigation. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in usw2-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in usw2-az2. </div><div><span class=\"yellowfg\"> 3:22 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:02 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. Beginning at 10:58 AM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the US-WEST-2 Region. At 2:45 PM, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:35 PM, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Ireland)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1630445941",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;We are investigating connectivity issues for load balancers in a single Availability Zone (euw1-az2) in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;We can confirm network connectivity issues affecting a single Availability Zone (euw1-az2) in the EU-WEST-1 Region and are actively working on mitigation. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, may also see impact as a result of this issue. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in euw1-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in euw1-az2.</div><div><span class=\"yellowfg\"> 3:35 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:04 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (euw1-az2) in the EU-WEST-1 Region. Beginning at 2:19 PM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the EU-WEST-1 Region. At 3:25 PM PDT, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:40 PM PDT, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-eu-west-1"
    },
    {
      "service_name": "AWS Direct Connect (Tokyo)",
      "summary": "[RESOLVED] ネットワーク接続性 | Network Connectivity",
      "date": "1630543140",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;日本時間 2021/09/02 07:30 から一部の AWS Direct Connect 接続と AP-NORTHEAST-1 リージョン間にネットワーク接続性の問題が発生していることを確認しております。この問題について調査を行っております。| Starting at 3:30 PM PDT, we began to experience network connectivity issues, impacting AWS Direct Connect connectivity between some AWS Direct Connections and the AP-NORTHEAST-1 Region. We are actively investigating the issue. </div><div><span class=\"yellowfg\"> 6:02 PM PDT</span>&nbsp;一部の AWS Direct Connect 接続と AP-NORTHEAST-1 リージョン間にネットワーク接続性の問題について追加の情報をご案内いたします。日本時間 2021/09/02 07:30 からコアネットワークデバイスに複数の問題が発生していることを確認しております。現在、問題が発生したデバイスについて復旧を進めており、デバイスがオンラインの状態に戻ることで接続性の問題が解消することが期待されます。現状では復旧の目途に関する情報はございません。進展がございましたら、随時更新致します。| We wanted to provide some more information for the event affecting some Direct Connect network connectivity in the AP-NORTHEAST-1 Region. Starting at 3:30 PM PDT, we began to experience network connectivity issues due to some failures in core networking devices. We are currently working on restoring these devices and we expect some restoration of connectivity as these devices come back online. We currently do not have an ETA on full recovery and will update further as information comes to hand. </div><div><span class=\"yellowfg\"> 6:43 PM PDT</span>&nbsp;現在引き続き故障したデバイスの復旧を試みており、完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。| We are still trying to recover the failed devices and do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover are recommended to do so to achieve recovery.</div><div><span class=\"yellowfg\"> 7:33 PM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョン内の故障したデバイスの復旧に取り組んでおりますが、現時点において完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。| We are continuing to work on recovering a number of failed devices within the AP-NORTHEAST-1 Region, but do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover to VPN are recommended to do so to achieve recovery.</div><div><span class=\"yellowfg\"> 8:20 PM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョン内の故障したデバイスの復旧に取り組んでおりますが、現時点において完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。Direct Connect Gateway と Transit Gateway をご利用のお客様に関しては、AWS Site-to-Site VPN をご作成いただき Transit Gateway にアタッチしてご利用いただくことをお勧めいたします。こちらの VPN へのフェイルオーバーの設定手順に関しては次の記事をご参照ください: <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/\">https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/</a> | We are continuing to work on recovering a number of failed devices within the AP-NORTHEAST-1 Region, but do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover to VPN are recommended to do so to achieve recovery. For customers using Direct Connect gateway and Transit Gateway, we recommend creating an AWS Site-to-Site VPN and attach it to your Transit Gateway. Instructions for how to do this failover can be found here: <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/\">https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/</a></div><div><span class=\"yellowfg\"> 9:06 PM PDT</span>&nbsp;復旧の兆しが確認できておりますが、引き続き事象の完全な解消に取り組んでおります。VPN を使用するワークアラウンドを実施いただいているお客様につきましては、完全な復旧のご連絡まではワークアラウンドを継続してご利用いただくことをお勧めいたします。| We are beginning to see signs of recovery, and continue to work toward full resolution. We suggest that customers that may have implemented the suggested workaround via VPN continue to use this workaround until we advise of full recovery.</div><div><span class=\"yellowfg\"> 9:51 PM PDT</span>&nbsp;日本時間 2021/09/02 07:30 から 13:42 の間、Direct Connect 接続を利用した AP-NORTHEAST-1 リージョン内の AWS サービスへの通信においてパケットロスの増加が発生しました。今回の事象は、 Direct Connect を利用したネットワークトラフィックを AP-NORTHEAST-1 リージョン内の全てのアベイラビリティーゾーンに接続するのに使用される複数のコアネットワークデバイスの問題に起因しておりました。現在問題は解消し、サービスは正常に稼働しています。\n| Between 3:30 PM and 9:42 PM PDT we experienced elevated packet loss for customers connecting to AWS services within AP-NORTHEAST-1 Region through their Direct Connect connections. This was caused by the loss of serveral core networking devices that are used to connect Direct Connect network traffic to all Availability Zones in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">Sep 6,  8:25 PM PDT</span>&nbsp;日本時間 9月2日 の Direct Connect のイベントに関する詳細情報を提供いたします。詳細については、以下をご参照ください。ご質問がある場合は、AWS サポートにお問い合わせください。<a href=\"https://aws.amazon.com/message/17908\">https://aws.amazon.com/message/17908</a> | We'd like to share more information about the Direct Connect event on Wednesday September 2nd. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/17908\">https://aws.amazon.com/message/17908</a></div>",
      "service": "directconnect-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Hong Kong)",
      "summary": "[RESOLVED] API错误率的上升 | API的錯誤率上升 | Increased API error rates",
      "date": "1631281173",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:39 AM PDT</span>&nbsp;我们正在调查AP-EAST-1地区EC2 API错误率的上升，若有新的进展时会立即更新。| 我們正在調查AP-EAST-1地區EC2 API的錯誤率上升，若有新的進展時會立即更新。| We are investigating increased EC2 API error rates in the AP-EAST-1 Region. We will provide more information as it becomes available.</div><div><span class=\"yellowfg\"> 7:30 AM PDT</span>&nbsp;在 20:23 至 21:56 (UTC+8) 之间，我们在 AP-EAST-1 区域遇到了 API 和新实例启动错误率增加的情况。发生原因是子系统中的少数主机无法处理 API 请求。 在多数情况下，重试调用 API 之后可成功执行。 目前问题已解决，服务已恢复正常运行。| 在 20:23 至 21:56 (UTC+8) 之間，我們在 AP-EAST-1 區域遇到了 API 和新實例啟動錯誤率增加的情況。發生原因是子系統中的少數主機無法處理 API 請求。 在多數情況下，重試調用 API 之後可成功執行。 目前問題已解決，服務已恢復正常運行。| Between 5:23 AM and 6:56 AM PDT we experienced increased error rates for APIs and new instance launches in the AP-EAST-1 Region. The root cause was a small number of hosts in a subsystem that failed to process API requests. Retries of API calls would have succeeded in many cases. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Degraded EBS Volume Performance",
      "date": "1632712292",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:11 PM PDT</span>&nbsp;We are investigating degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. Some new EC2 instance launches, within the affected Availability Zone, are also impacted by this issue. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 8:41 PM PDT</span>&nbsp;We can confirm degraded performance for some EBS volumes within a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. Existing EC2 instances within the affected Availability Zone that use EBS volumes may also experience impairment due to stuck IO to the attached EBS volume(s). Newly launched EC2 instances within the affected Availability Zone may fail to launch due to the degraded volume performance. We continue to work toward determining root cause and mitigating impact but recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 9:17 PM PDT</span>&nbsp;We are making progress in determining the root cause and have isolated it to a subsystem within the EBS service. We are working through multiple steps to mitigate the issue and will continue to provide updates as we make progress. Other Availability Zones remain unaffected by this issue and affected EBS volumes and EC2 instances within the affected Availability Zone have plateaued at this stage. We continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\"> 9:47 PM PDT</span>&nbsp;We continue to make progress in determining the root cause of the issue causing degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. A subsystem within the larger EBS service that is responsible for coordinating storage hosts is currently degraded due to increased resource contention. We continue to work to understand the root cause of the elevated resource contention, but are actively working to mitigate the issue. Once mitigated, we expect performance for the affected EBS volumes to return to normal levels. We will continue to provide you with updates on our progress. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">10:23 PM PDT</span>&nbsp;We continue to make progress in determining the root cause of the issue causing degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. We have made several changes to address the increased resource contention within the subsystem responsible for coordinating storage hosts with the EBS service. While these changes have led to some improvement, we have not yet seen full recovery for the affected EBS volumes. We continue to expect full recovery of the affected EBS volumes once the subsystem issue has been addressed. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">11:19 PM PDT</span>&nbsp;We continue to work to address the increased resource contention within the EBS service subsystem responsible for coordinating EBS storage hosts. We have applied several mitigations, and while we have seen some improvements, we have not yet seen performance for affected volumes return to normal levels. We are currently rolling out another mitigation that we believe addresses the root cause and has shown promising signs in testing. We will know within the next 30-45 minutes as to whether this restores normal operations for current affected volumes. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">11:43 PM PDT</span>&nbsp;We can confirm that the deployed mitigation has worked and we have started to see recovery for some affected EBS volumes within the affected Availability Zone (USE1-AZ2). We are still finishing the deployment of the mitigation, but expect performance of affected EBS volumes in this single Availability Zone to return to normal levels over the next 60 minutes.</div><div><span class=\"yellowfg\">Sep 27, 12:30 AM PDT</span>&nbsp;We can confirm that the performance has returned to normal levels for the majority of the affected EBS volumes within the affected Availability Zone (USE1-AZ2). We continue to work towards full recovery for the remaining EBS volumes.</div><div><span class=\"yellowfg\">Sep 27,  1:15 AM PDT</span>&nbsp;We can confirm that the performance has returned to normal levels for the majority of the affected EBS volumes within the affected Availability Zone (USE1-AZ2). Starting at 12:12 AM PDT, we saw recovery slow down some affected EBS volumes as well as some degraded performance for a small number of additional volumes in the affected Availability Zone. We have investigated the root cause and mitigations are underway to complete the performance recovery of the affected EBS volumes.  We continue to work towards full resolution for all affected EBS volumes. In some cases, customers may be experiencing volume state transition delays, which we expect to clear up once volumes have fully recovered.</div><div><span class=\"yellowfg\">Sep 27,  2:26 AM PDT</span>&nbsp;We continue to apply mitigations to address the degraded performance for the smaller set of remaining EBS volumes affected by this issue. While the vast majority of affected EBS volumes were operating normally by 12:05 AM PDT, we have been working to recover a smaller set of affected volumes since 12:12 AM PDT. We continue to make progress on restoring performance for affected volumes, and expect full recovery to take another 2 hours. In some cases, customers may also be experiencing volume state transition delays, which will resolve when the underlying volume has fully recovered.</div><div><span class=\"yellowfg\">Sep 27,  3:36 AM PDT</span>&nbsp;We had restored performance for the vast majority of affected EBS volumes within the affected Availability Zone in the US-EAST-1 Region at 12:05 AM PDT and have been working to restore a remaining smaller set of EBS volumes. EC2 instances affected by this issue have now also recovered and new EC2 instance launches with attached EBS volumes have been succeeding since 1:30 AM PDT. Other services - including Redshift, OpenSearch, and Elasticache - are seeing recovery. Some RDS databases are still experiencing connectivity issues, but we’re working towards full recovery. We are in the process of restoring performance for the remaining small number of EBS volumes and EC2 instances that are still affected by this issue. </div><div><span class=\"yellowfg\">Sep 27,  4:21 AM PDT</span>&nbsp;Starting at 6:41 PM PDT on September 26th, we experienced degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. The issue was caused by increased resource contention within the EBS subsystem responsible for coordinating EBS storage hosts. Engineering worked to identify the root cause and resolve the issue within the affected subsystem. At 11:20 PM PDT, after deploying an update to the affected subsystem, IO performance for the affected EBS volumes began to return to normal levels. By 12:05 AM on September 27th, IO performance for the vast majority of affected EBS volumes in the USE1-AZ2 Availability Zone were operating normally. However, starting at 12:12 AM PDT, we saw recovery slow down for a smaller set of affected EBS volumes as well as seeing degraded performance for a small number of additional volumes in the USE1-AZ2 Availability Zone. Engineering investigated the root cause and put in place mitigations to restore performance for the smaller set of remaining affected EBS volumes. These mitigations slowly improved the performance for the remaining smaller set of affected EBS volumes, with full operations restored by 3:45 AM PDT. While almost all of EBS volumes have fully recovered, we continue to work on recovering a remaining small set of EBS volumes. We will communicate the recovery status of these volumes via the Personal Health Dashboard. While the majority of affected services have fully recovered, we continue to recover some services, including RDS databases and Elasticache clusters. We will also communicate the recovery status of these services via the Personal Health Dashboard. The issue has been fully resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Chime",
      "summary": "[RESOLVED] Increased contact search failures",
      "date": "1632832788",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:39 AM PDT</span>&nbsp;We are investigating increased latency in being able to search for contacts and start chime messages. We will provide more information as it becomes available</div><div><span class=\"yellowfg\"> 6:31 AM PDT</span>&nbsp;Between 2:55 AM and 6:10 AM PDT, we experienced increased error rates when searching for contacts and starting Chime messages. The issue is resolved and the service is operating normally.  </div>",
      "service": "chime"
    },
    {
      "service_name": "AWS Lambda (Ireland)",
      "summary": "[RESOLVED] Increased invoke timeouts",
      "date": "1633447955",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:32 AM PDT</span>&nbsp;We are investigating increased invoke timeout rates for AWS Lambda functions in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 9:15 AM PDT</span>&nbsp;We are still investigating increased invoke timeout rates for AWS Lambda functions in the EU-WEST-1 Region. A subset of functions making API calls to other AWS services within the region are affected.</div><div><span class=\"yellowfg\">10:04 AM PDT</span>&nbsp;We are still investigating increased invoke timeout rates and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. We have isolated the timeouts to network connectivity issues in a section of the AWS Lambda compute subsystem in the region, and are working through resolution and root cause. The issue affects a subset of functions making API calls to other AWS services within the region.</div><div><span class=\"yellowfg\">10:43 AM PDT</span>&nbsp;We can confirm increased invoke timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. The issue affects a subset of functions making API calls to other AWS services within the region. There are no identified workarounds for affected Lambda functions at this time. We continue to investigate a network connectivity issue in a section of the AWS Lambda compute subsystem. We will formulate a mitigation plan based on these findings as we work towards narrowing root cause and work towards resolution.</div><div><span class=\"yellowfg\">11:49 AM PDT</span>&nbsp;We continue to experience increased invoke timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. The issue affects a subset of functions making API calls to other AWS services within the region. At this point we have not identified any workarounds for affected Lambda functions, and we continue to investigate possible networking issues in a section of the AWS Lambda compute subsystem. We will make more information about our mitigation plan available as soon as we have more information to share.</div><div><span class=\"yellowfg\"> 2:03 PM PDT</span>&nbsp;We continue to investigate our subsystems to identify the root cause of timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. Impairment is limited to a small proportion of non-VPC Lambda functions that make outbound network connections. We will update information about workarounds and mitigation plans once we have more information to share.</div><div><span class=\"yellowfg\"> 3:12 PM PDT</span>&nbsp;Between October 4 9:46 AM and October 5 2:47 PM PDT, we experienced timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. Impairments were limited to a small proportion of non-VPC Lambda functions that made outbound network connections during this time. The issue has been resolved and the service is operating normally.   </div>",
      "service": "lambda-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Network Connectivity Issue",
      "date": "1633484779",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:46 PM PDT</span>&nbsp;We are investigating network connectivity for some instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have identified the root cause and are working to restore connectivity for the affected instances. Some EBS volumes are also experiencing degraded performance due to this issue.</div><div><span class=\"yellowfg\"> 7:13 PM PDT</span>&nbsp;We continue to work to mitigate the issue affecting connectivity to some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is a loss of power to a small number of EC2 instances and some networking devices within the affected Availability Zone. A small number of EC2 instances are also experiencing network connectivity issue as a result of the loss of power to the affected networking devices. We are working to restore power to the affected instances. If you are able to launch new EC2 instances, all APIs are operating normally with in the affected Availability Zone. We do not expect the issue to affect any other power line-ups within the affected Availability Zone or other Availability Zones within the Region.</div><div><span class=\"yellowfg\"> 7:34 PM PDT</span>&nbsp;We have restored power to the affected line-up and are seeing recovery for the majority of the affected EC2 instances and EBS volumes. We continue to work on the remaining EC2 instances and EBS volumes.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;Starting at 6:05 PM PDT some EC2 instances within a single Availability Zone experienced a loss of power. Other instances within the affected Availability Zone experienced connectivity issues and some EBS volumes experienced degraded performance. The root cause of the event was a loss of power to a single line-up within the affected Availability Zone. Power was restored to the affected line-up at 7:20 PM PDT and at 7:35 PM PDT, the affected EC2 instances and EBS volumes had recovered. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1633637388",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:09 PM PDT</span>&nbsp;PDT 시간으로 12:24 PM과 12:34 PM 사이에, AP-NORTHEAST-2 리전에서 API 에러율 증가가 발생했습니다. 이 이슈는 해결되었고 서비스는 현재 정상 동작 중입니다. 이슈가 영향을 미치는 중에는 API 실패가 발생했을 수 있고, 고객들이 API 호출들을 재시도 할 것을 권고합니다. 고객들이 서비스의 가용성을 복구하기 위한 추가적인 조치를 취할 필요는 없습니다. 이 기간 동안 다른 AWS 서비스들에서도 API 실패가 발생할 수 있었습니다. 만일 궁금한 점이나 서비스와 관련된 운영상의 이슈를 겪으신다면, AWS Support Center를 통해 AWS 기술지원팀에 문의해주십시오. https://console.aws.amazon.com/support  |  Between 12:24 PM and 12:34 PM PDT we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.  During the period of impact, API's may have failed and we recommend that customers retry these API calls.  Customers do not need to take additional measures to restore the availability of this service.  Additional AWS services may have seen API failures during this period.  If you have any questions or are experiencing any operational issues with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Elevated Error Rates for AWS Management Console ",
      "date": "1634052623",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:30 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:38 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:47 AM PDT</span>&nbsp;Between 8:10 AM and 8:39 AM PDT we experienced increased error rates and latency for the AWS Management Console. The issue has been resolved and the console is operating normally.  This issue did not affect any AWS service APIs, they were operating correctly during this time.</div>",
      "service": "management-console"
    },
    {
      "service_name": "AWS Directory Service (Sydney)",
      "summary": "[RESOLVED] Increased Authentication Error Rates ",
      "date": "1634086680",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:58 PM PDT</span>&nbsp;We are investigating authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Other AWS services, including Workspaces and Client VPN, are also affected by this event if they are configured to use AD Connector directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 7:04 PM PDT</span>&nbsp;We have identified the root cause of the authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. We are currently working on a fix to mitigate the issue. Other AWS services, including Workspaces, Client VPN and SSO, are also affected by this event if they are configured to use AD Connector directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 7:22 PM PDT</span>&nbsp;We have identified the root cause of the authentication errors on AD Connector Directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Our mitigation efforts are working as expected and customers should see resolution over the next 3 hours. We are currently working on steps to speed this up and will update resolution timings as we progress. Other AWS services, including Workspaces, Client VPN and SSO, are also affected by this event if they are configured to use AD Connector Directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;Our actions to expedite recovery have been successful and we now expect the fix to be completely deployed within the hour.</div><div><span class=\"yellowfg\"> 9:19 PM PDT</span>&nbsp;Between 1:00 PM and 9:15 PM PDT we experienced authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Other AWS services, including Workspaces, Client VPN and SSO were also affected by this event if they were configured to use AD Connector directories with MFA authentication enabled. The issue has been resolved and the service is operating normally.</div>",
      "service": "directoryservice-ap-southeast-2"
    },
    {
      "service_name": "AWS Internet Connectivity (US-West)",
      "summary": "[RESOLVED] Network Connectivity in US-GOV-WEST-1",
      "date": "1635874351",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:32 AM PDT</span>&nbsp;We are investigating connectivity issues in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">11:45 AM PDT</span>&nbsp;The issue is affecting network connectivity from the Internet to EC2 instances in a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region, between instances within this Availability Zone and between instances within this Availability Zone and other Availability Zones.</div><div><span class=\"yellowfg\">12:54 PM PDT</span>&nbsp;We have resolved the issue affecting Internet connectivity to a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-gov-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (US-West)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1635875642",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:54 AM PDT</span>&nbsp;Network Connectivity\nWe're investigating network connectivity issues for instances within a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region. We are also seeing increased error rates and latencies for the EC2 APIs within the region and are working to solve the issue.</div><div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue is affecting network connectivity from the Internet to instances in the affected Availability Zone, between instances within the affected Availability Zone and between instances within the affected Availability Zone and other Availability Zones. The EC2 APIs are also experiencing increased error rates and latencies within the US-GOV-WEST-1 Region, which is also affecting the AWS Management Console. We have made some progress in mitigating the impact for other AWS services, such as connectivity to Amazon S3, but continue to work on resolving the issue.</div><div><span class=\"yellowfg\">12:09 PM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. We have identified the root cause of the issue and are now focused on mitigating the issue. We are seeing some recovery in the error rates and latencies for the EC2 APIs and launches of new instances are once again working within the region. For recovery at this stage, we recommend focusing on shifting workloads and traffic away from the affected Availability Zone (USGW-AZ3).</div><div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;We are seeing recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. Once networking was restored to the affected Availability Zone, affected AWS services are also recovering. While the majority are seeing full recovery, we continue to work on the networking-related EC2 APIs, which are still seeing errors at this stage.</div><div><span class=\"yellowfg\"> 1:10 PM PDT</span>&nbsp;We have seen recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The increased error rates and latencies for the EC2 APIs have also recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-gov-west-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (US-West)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1635879208",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:53 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;Between 10:15 AM and 12:53 PM PDT we experienced increased API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-us-gov-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Sao Paulo)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1636421771",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:36 PM PST</span>&nbsp;We are investigating internet connectivity issues within the SA-EAST-1 Region. This is affecting connectivity into the SA-EAST-1 Region but also causing increased error rates and latencies for AWS service APIs within the region.</div><div><span class=\"yellowfg\"> 5:51 PM PST</span>&nbsp;Between 5:15 PM and 5:41 PM PST we experienced Internet connectivity issues for a single Availability Zone (sae1-az1) within the SA-EAST-1 Region. Some AWS services also experienced increased error rates and latencies during this time. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-sa-east-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1636495106",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:58 PM PST</span>&nbsp;We are investigating increased 5xx error rates and latencies for requests to the Amazon S3 APIs in the US-EAST-1 Region. Where possible, we recommend that requests that fail with a 5xx error be retried.</div><div><span class=\"yellowfg\"> 2:48 PM PST</span>&nbsp;We have identified the S3 subsystem responsible for increased 5xx error rates for the S3 PUT APIs, and are working to isolate the root cause within this subsystem. Customers may also be experiencing increased latency when performing PUT operations. During this time, we recommend customers retry any failed requests.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;We are continuing to see increased 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. We have narrowed down the root cause to a specific sub-system within S3 and continue to make progress in mitigating the impact to this service but have not yet seen significant improvement. S3 API error rates and latencies have stayed a consistent low level with the vast majority of request retries succeeding. While the vast majority of requests are being processed within the normal latency levels, request tail latencies are exceeding 1 second in some cases. In some applications, increasing client timeouts may also help to mitigate the issue. </div><div><span class=\"yellowfg\"> 4:52 PM PST</span>&nbsp;We are starting to see some improvement in the 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. The issue affected a subsystem that stores routing metadata used by Amazon S3 to map API requests to the storage nodes. A recent update caused increased load within this subsystem, which led to increased error rates and latencies for the S3 APIs. We have now successfully mitigated this increased load within this subsystem and are seeing early signs of recovery. As the sub-system processes the backlog of requests, S3 API error rates and latencies will continue to improve.</div><div><span class=\"yellowfg\"> 5:53 PM PST</span>&nbsp;We continue to see a gradual improvement in error rates as we process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We are currently working on mitigations to speed up the processing of the backlog during this event. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally. </div><div><span class=\"yellowfg\"> 6:58 PM PST</span>&nbsp;We continue to process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We have implemented two parallel mitigations to improve the speed of processing. Both mitigations are in process of deployment. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally.</div><div><span class=\"yellowfg\"> 7:51 PM PST</span>&nbsp;We have completed the mitigation to accelerate processing of the mappings between S3 API request metadata and storage. The backlog has been fully processed and S3 API errors and latencies have returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-standard"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift cluster reboot and degraded performance",
      "date": "1636496591",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:23 PM PST</span>&nbsp;We are investigating cluster reboots and degraded performance for Redshift Clusters in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:47 PM PST</span>&nbsp;We continue to investigate cluster reboots and degraded performance for Redshift RA3 Clusters in the US-EAST-1 Region. The cluster reboots are being triggered by writes that are being impacted by the elevated S3 put latencies. Redshift attempts to retry these writes automatically but if they are unsuccessful after extended attempts, the cluster may restart. If you are able to pause write workloads while we work towards resolving the S3 put latency issue, your clusters will no longer restart and can serve read queries normally.</div><div><span class=\"yellowfg\"> 4:53 PM PST</span>&nbsp;We have identified and continue to work on mitigating the root cause of the Redshift RA3 Clusters reboots and degraded performance in the US-EAST-1 Region. The cluster reboots are triggered by Redshift writes to S3 that have been impacted by the elevated S3 PUT API latencies. If you are impacted and able to pause Redshift write workloads on your RA3 clusters while we work towards recovery, your clusters will no longer restart and can serve read queries normally. As S3 API error rates and latencies continue to improve, we expect Redshift RA3 cluster restarts to decline as well.</div><div><span class=\"yellowfg\"> 7:19 PM PST</span>&nbsp;While the S3 API error rates and latencies continue to hold steady, we continue to see a low rate of Redshift RA3 Cluster restarts. While we continue to take steps to mitigate and reduce the risk of a restart for affected Redshift clusters, we expect to see full recovery when the S3 error rates and latencies have fully recovered. Please refer to the S3 Service Health Dashboard updates for progress towards recovery.</div><div><span class=\"yellowfg\"> 7:47 PM PST</span>&nbsp;As S3 has completed their mitigation and is operating normally, Redshift RA3 clusters are seeing writes succeed and clusters are no longer rebooting. For the vast majority of customers the issue has been resolved and the service is operating normally. We continue to work with a small number of impacted customers individually to recover their clusters and will reach out via the AWS Personal Health Dashboard and AWS Support.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Stockholm)",
      "summary": "[RESOLVED] Increased EC2 Console Error Rates",
      "date": "1637596278",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:51 AM PST</span>&nbsp;We are investigating increased error rates within the EC2 Management Console in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are starting to see recovery for the issue causing increased error rates in the EC2 Console in the EU-NORTH-1 Region. We expect to see full recovery shortly. In the meantime, the EC2 APIs and EC2 CLI remain unaffected by the issue.</div><div><span class=\"yellowfg\"> 8:38 AM PST</span>&nbsp;We have resolved the issue causing increased error rates for the EC2 Management Console in the EU-NORTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-north-1"
    },
    {
      "service_name": "Amazon EventBridge (Cape Town)",
      "summary": "[RESOLVED] Event Delivery Delays",
      "date": "1637641154",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating increased event delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:13 PM PST</span>&nbsp;We continue to investigate increased event delivery delays in AF-SOUTH-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:26 PM PST</span>&nbsp;We can confirm elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken initial mitigation actions based on our investigations and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23, 12:04 AM PST</span>&nbsp;We continue working to identify the root cause of elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken further mitigation actions which have reduced the delivery latency and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23,  1:59 AM PST</span>&nbsp;Between November 22 7:30 PM and November 23 1:32 AM PST, we experienced elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "events-af-south-1"
    },
    {
      "service_name": "Amazon CloudWatch (Cape Town)",
      "summary": "[RESOLVED] Metric Stream Delivery Delays ",
      "date": "1637641175",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating CloudWatch Metric Streams delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:09 PM PST</span>&nbsp;We can confirm elevated API error rates for CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. We are actively working to resolve the issue. Customers may experience issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams is not affected.</div><div><span class=\"yellowfg\"> 9:42 PM PST</span>&nbsp;Between 7:30 PM and 9:25 PM PST, we experienced elevated error rates when calling CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. Customers may have experienced issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudwatch-af-south-1"
    },
    {
      "service_name": "AWS Lambda (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1637779569",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:46 AM PST</span>&nbsp;We are investigating increased invoke error rates in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:10 AM PST</span>&nbsp;We continue to investigate increased function invoke error rates within the US-EAST-2 Region. We are working to determine the root cause of the issue and will continue to provide updates as we make progress in resolving the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">11:34 AM PST</span>&nbsp;We have identified the root cause within Lambda that is causing the increased error rates for function invocations within the US-EAST-2 Region. The subsystem responsible for executing functions is currently impaired and we are working to resolve it. We have seen some improvement in Lambda function invocation error rates and believe that this will continue as we take steps to resolve the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:27 AM and 12:03 PM PST we experienced increased Lambda invoke error rates in the US-EAST-2 Region. It may take some additional time to process backlogged async invoke traffic that accumulated during this period. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-2"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1637780142",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:55 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console is the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:46 AM PST</span>&nbsp;We are seeing some recovery in error rates and latencies for the AWS Management Console in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:13 PM PST</span>&nbsp;Between 10:26 AM and 12:05 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon API Gateway (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1637780385",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We are investigating increased API errors for API Gateway in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:24 AM PST</span>&nbsp;We have confirmed that the elevated API error rate is restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs).</div><div><span class=\"yellowfg\">12:20 PM PST</span>&nbsp;Between 10:27 AM and 12:06 PM PST, we experienced elevated API error rate in the US-EAST-2 Region, restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs). The issue has been resolved, and the service is operating normally. </div>",
      "service": "apigateway-us-east-2"
    },
    {
      "service_name": "AWS Batch (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates and Scaling Delays",
      "date": "1637781298",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:14 AM PST</span>&nbsp;We are investigating increased API error rates and delays in scaling of some AWS Batch Compute Environments in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:25 AM and 12:00 PM PST we experienced increased error rates for all AWS Batch APIs, as well as some Compute Environment scaling delays, in the US-EAST-2 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "batch-us-east-2"
    },
    {
      "service_name": "Amazon Simple Notification Service (Oregon)",
      "summary": "[RESOLVED] Increased Latency and Error Rates",
      "date": "1638021360",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:56 AM PST</span>&nbsp;We are investigating increased latency and error rates for API calls in the US-WEST-2 Region. We will provide more information as we continue to investigate.</div><div><span class=\"yellowfg\"> 6:12 AM PST</span>&nbsp;We are starting to see improved SNS API success rates and latency in the US-WEST-2 Region, and are working towards full recovery.</div><div><span class=\"yellowfg\"> 6:42 AM PST</span>&nbsp;Between 5:05 AM and 5:55 AM PST, we experienced increased SNS API latency and error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-us-west-2"
    },
    {
      "service_name": "Amazon CloudWatch (Ireland)",
      "summary": "[RESOLVED] Delayed CloudWatch Metrics",
      "date": "1638042899",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:55 AM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters.  We are are working towards resolution.</div><div><span class=\"yellowfg\">12:49 PM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters. We have isolated the likely root cause to a subsystem that saw an unexpected jump in resource consumption.  We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 1:52 PM PST</span>&nbsp;We have implemented a fix to address the CloudWatch log event processing delays in the EU-WEST-1 Region and are starting to see signs of recovery. We will provide an update once full recovery has been observed.</div><div><span class=\"yellowfg\"> 2:56 PM PST</span>&nbsp;Between 10:26 AM and 02:40 PM PST, we experienced increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. This was due to a subsystem that saw an unexpected jump in resource consumption. The issue has been resolved and the service is operating normally. New events are processing as normal, while we work through the message backlog. We expect to completely drain the backlog over the next 1 hour.</div>",
      "service": "cloudwatch-eu-west-1"
    },
    {
      "service_name": "AWS CloudShell (N. Virginia)",
      "summary": "[RESOLVED] Increased Latencies and Failure Rates",
      "date": "1638507226",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:53 PM PST</span>&nbsp;We are experiencing increased latencies and failures in launching CloudShell environments in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:26 PM PST</span>&nbsp;Between 7:05 PM and 9:15 PM PST we experienced increased latencies and failures launching CloudShell environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudshell-us-east-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1638894176",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:26 AM PST</span>&nbsp;We are experiencing API and console issues in the US-EAST-1 Region.  We have identified root cause and we are actively working towards recovery.  This issue is affecting the global console landing page, which is also hosted in US-EAST-1.  Customers may be able to access region-specific consoles going to https://console.aws.amazon.com/.  So, to access the US-WEST-2 console, try https://us-west-2.console.aws.amazon.com/</div><div><span class=\"yellowfg\"> 4:25 PM PST</span>&nbsp;We are seeing improvements in the error rates and latencies in the AWS Management Console in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:14 PM PST</span>&nbsp;Between 7:32 AM to 4:56 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1638895771",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:49 AM PST</span>&nbsp;We are experiencing elevated error rates for EC2 APIs in the US-EAST-1 region.  We have identified root cause and we are actively working towards recovery.</div><div><span class=\"yellowfg\"> 3:31 PM PST</span>&nbsp;Between 7:32 AM and 3:10 PM PST we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Contact Handling",
      "date": "1638896019",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:53 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:08 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 9:18 AM PST</span>&nbsp;We can confirm degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 4:47 PM PST</span>&nbsp;We are seeing improvements to contact handling in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:10 PM PST</span>&nbsp;Between 7:25 AM PST and 4:47 PM PST we experienced degraded Contact handling, increased user login errors, and increased API error rates in the US-EAST-1 Region. During this time, end-customers may have experienced delays or errors when placing a call or starting a chat, and agents may have experienced issues logging in or being connected with end-customers. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1638896271",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:57 AM PST</span>&nbsp;We are currently investigating increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:40 PM PST</span>&nbsp;Between 7:40 AM and 2:25 PM PST, we experienced increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region. Data plane operations were not impacted. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "dynamodb-us-east-1"
    },
    {
      "service_name": "AWS Support Center",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1638896490",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:01 AM PST</span>&nbsp;We are investigating increased error rates for the Support Center console and Support API in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:55 PM PST</span>&nbsp;We continue to see increased error rates for the Support Center console and Support API in the US-EAST-1 Region. Support Cases successfully created via the console or the API may not be successfully routed to Support Engineers. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Between 7:33 AM and 2:25 PM PST, we experienced increased error rates for the Support Center console and Support API in the US-EAST-1 Region. This resulted in errors in creating support cases and delays in routing cases to Support Engineers. The issue has been resolved and our Support Engineering team is responding to cases. The service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "supportcenter"
    },
    {
      "service_name": "Amazon EventBridge (N. Virginia)",
      "summary": "[RESOLVED] Event Delivery Delays",
      "date": "1638917670",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:54 PM PST</span>&nbsp;We have temporarily disabled event deliveries in the US-EAST-1 Region. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will not trigger targets. These events will still be received by EventBridge and will deliver once we recover.</div><div><span class=\"yellowfg\"> 3:00 PM PST</span>&nbsp;We have re-enabled event deliveries in the US-EAST-1 Region, but are experiencing event delivery latencies. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will be delayed.</div><div><span class=\"yellowfg\"> 4:31 PM PST</span>&nbsp;We continue to see event delivery latencies in the US-EAST-1 region. We have identified the root cause and are working toward recovery.</div><div><span class=\"yellowfg\"> 6:00 PM PST</span>&nbsp;Event delivery latency for new events in the US-EAST-1 Region have returned to normal levels. We continue to process a backlog of events.</div><div><span class=\"yellowfg\"> 9:21 PM PST</span>&nbsp;Between 7:30 AM and 8:40 PM PST we experienced elevated event delivery latency in the US-EAST-1 Region. Event delivery latencies have returned to normal levels. Some CloudTrail events for API calls between 7:35 AM and 6:05 PM PST may be delayed but will be delivered in the coming hours.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "events-us-east-1"
    },
    {
      "service_name": "Amazon API Gateway (N. Virginia)",
      "summary": "[RESOLVED] Elevated Errors and Latencies",
      "date": "1638919396",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:23 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 4:05 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are continuing to work towards resolution.</div><div><span class=\"yellowfg\"> 4:41 PM PST</span>&nbsp;We have seen improvement in error rates and latencies for invokes in the US-EAST-1 region. We continue to drive towards full recovery.</div><div><span class=\"yellowfg\"> 5:23 PM PST</span>&nbsp;Between 9:02 AM and 5:01 PM PST we experienced increased error rates and latencies for invokes in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "apigateway-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated Fargate task launch failures",
      "date": "1638919976",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:32 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day, but we are still investigating task launch failures using the Fargate launch type. Task launches using the EC2 launch type are not impacted.</div><div><span class=\"yellowfg\"> 4:44 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are working towards recovery.</div><div><span class=\"yellowfg\"> 5:31 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are starting to see recovery. As we work towards full recovery, customers may experience insufficient capacity errors and these are being addressed as well.</div><div><span class=\"yellowfg\"> 7:30 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">11:01 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this and have recently seen a decrease in these errors while continuing to work towards full recovery. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">Dec 8,  2:29 AM PST</span>&nbsp;Between 7:31 AM PST on December 7 and 2:20 AM PST on December 8, ECS experienced increased API error rates, latencies, and task launch failures. API error rates and latencies recovered by 6:10 PM PST on December 7. After this point, ECS customers using the EC2 launch type were fully recovered. ECS customers using the Fargate launch type along with EKS customers using Fargate continued to see decreasing impact in the form of insufficient capacity errors between 4:40 PM PST on December 7 and 2:20 AM on December 8. The service is now operating normally. A small set of customers may still experience low levels of insufficient capacity errors and will be notified using the Personal Health Dashboard in that case. There was no impact to running tasks during the event although any ECS task that failed health checks would have been stopped because of that failing health check.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "AWS Batch (N. Virginia)",
      "summary": "[RESOLVED] Increased Job Processing Delays",
      "date": "1638922058",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:07 PM PST</span>&nbsp;We have identified the root cause of increased delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 5:20 PM PST</span>&nbsp;We have seen improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 8:02 PM PST</span>&nbsp;Improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region is accelerating, we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:29 PM PST</span>&nbsp;Between 7:35 AM and 8:13 PM PST, we experienced increase job state transition delays of AWS Batch Jobs in the US-EAST-1 Region.  The issue has been resolved and the service is now operating normally for new job submissions.  Jobs that were delayed from earlier in the event will be processed in order until we clear the queue.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "batch-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639055699",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:15 AM PST</span>&nbsp;We are investigating elevated error rates for the Redshift Management Console in the US-EAST-1 region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:34 AM PST</span>&nbsp;We continue to investigate elevated error rates for the Redshift Management Console in the US-EAST-1 region. We have identified the issue causing elevated error rates and are actively working to resolve the issue. Customer clusters remain available and are operating normally. Customers can use API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries. </div><div><span class=\"yellowfg\"> 6:30 AM PST</span>&nbsp;We have identified the root cause of the issue causing elevated error rates and are in the process of deploying a fix that will resolve the issue. We do not have a precise ETA for the deployment to complete that we can share at this time. Customer clusters remain available and are operating normally. Customers can use the API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries.</div><div><span class=\"yellowfg\"> 6:58 AM PST</span>&nbsp;Between 12:00 AM and 6:25 AM PST we saw elevated error rates for the Amazon Redshift Management Console in the US-EAST-1 region. This initial error rate increase was observed at 12:00 AM PST and at 4:05 AM PST this error rate increased further. Customer clusters were operating normally throughout and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. We have completed the deployment of a fix. The issue has been resolved and the Management Console and the Amazon Redshift service are operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639067113",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:25 AM PST</span>&nbsp;Earlier today we reported elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. At 6:58 AM PST we reported the issue was resolved but we have since detected that some errors persist despite much lower rates. Redshift clusters remain available and customers can manage their clusters using the API and CLI. Customers can also execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:05 AM PST</span>&nbsp;We continue to investigate intermittent elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. Customers will be able to get the console to load if they refresh their browser tab several times. Once the console has loaded, the console will work as expected and customers will be able to execute queries normally. Redshift clusters remain available and customers can manage their clusters using the API and CLI and can execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;Between 7:25 AM and 10:05 AM PST we experienced increased error rates for the Amazon Redshift Management Console in the US-EAST-1 Region. During this time, clusters were operating normally  and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. The issue is resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Oregon)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639582979",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:43 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-2 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:14 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-2 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.\n</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-2"
    },
    {
      "service_name": "AWS Internet Connectivity (N. California)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583545",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:52 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (US-West)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583722",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:55 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:00 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-GOV-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-GOV-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:16 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-gov-west-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "[RESOLVED] Console Application Upload Errors",
      "date": "1640158400",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:33 PM PST</span>&nbsp;We are investigating an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. Customers who need to update or deploy a new application version should do so using the AWS CLI. Existing applications are not impacted by this issue</div><div><span class=\"yellowfg\">Dec 22, 12:34 AM PST</span>&nbsp;We continue to investigate an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. We are determining the root causes and working through steps to mitigate the issue. Customers who need to update or deploy a new application version should do so using the AWS CLI while we work towards resolving the issue. Existing applications are not impacted by this issue.</div><div><span class=\"yellowfg\">Dec 22,  1:20 AM PST</span>&nbsp;We have identified the root cause and prepared a fix to address the issue that prevents customers from uploading new application versions through the Elastic Beanstalk console in multiple Regions. The service team is testing this fix and preparing for deployment to the Regions that are affected by this issue. We expect to see full recovery by 3:00 AM PST and will continue to keep you updated if this ETA changes. Customers who need to update or deploy a new application version should do so using the AWS CLI until the issue is fully resolved.</div><div><span class=\"yellowfg\">Dec 22,  3:21 AM PST</span>&nbsp;Between December 21, 2021 at 6:37 PM  and December 22, 2021 at 03:17 AM PST, customers were unable to upload their code through the Elastic Beanstalk console due to a Content Security Policy (CSP) error. Customers were impacted when they attempted to upload a new application version for existing environments or upload their code when creating a new environment in multiple regions. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] API Error Rates",
      "date": "1640176551",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:35 AM PST</span>&nbsp;We are investigating increased EC2 launch failures and networking connectivity issues for some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 5:01 AM PST</span>&nbsp;We can confirm a loss of power within a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. This is affecting availability and connectivity to EC2 instances that are part of the affected data center within the affected Availability Zone. We are also experiencing elevated RunInstance API error rates for launches within the affected Availability Zone. Connectivity and power to other data centers within the affected Availability Zone, or other Availability Zones within the US-EAST-1 Region are not affected by this issue, but we would recommend failing away from the affected Availability Zone (USE1-AZ4) if you are able to do so. We continue to work to address the issue and restore power within the affected data center.</div><div><span class=\"yellowfg\"> 5:18 AM PST</span>&nbsp;We continue to make progress in restoring power to the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have now restored power to the majority of instances and networking devices within the affected data center and are starting to see some early signs of recovery. Customers experiencing connectivity or instance availability issues within the affected Availability Zone, should start to see some recovery as power is restored to the affected data center. RunInstances API error rates are returning to normal levels and we are working to recover affected EC2 instances and EBS volumes. While we would expect continued improvement over the coming hour, we would still recommend failing away from the Availability Zone if you are able to do so to mitigate this issue.</div><div><span class=\"yellowfg\"> 5:39 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. Network connectivity within the affected Availability Zone has also returned to normal levels. While all services are starting to see meaningful recovery, services which were hosting endpoints within the affected data center - such as single-AZ RDS databases, ElastiCache, etc. - would have seen impact during the event, but are starting to see recovery now. Given the level of recovery, if you have not yet failed away from the affected Availability Zone, you should be starting to see recovery at this stage. </div><div><span class=\"yellowfg\"> 6:13 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. We continue to make progress in recovering the remaining EC2 instances and EBS volumes within the affected Availability Zone. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 6:51 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. For the remaining EC2 instances, we are experiencing some network connectivity issues, which is slowing down full recovery. We believe we understand why this is the case and are working on a resolution. Once resolved, we expect to see faster recovery for the remaining EC2 instances and EBS volumes. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 8:02 AM PST</span>&nbsp;Power continues to be stable within the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have been working to resolve the connectivity issues that the remaining EC2 instances and EBS volumes are experiencing in the affected data center, which is part of a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have addressed the connectivity issue for the affected EBS volumes, which are now starting to see further recovery. We continue to work on mitigating the networking impact for EC2 instances within the affected data center, and expect to see further recovery there starting in the next 30 minutes. Since the EC2 APIs have been healthy for some time within the affected Availability Zone, the fastest path to recovery now would be to relaunch affected EC2 instances within the affected Availability Zone or other Availability Zones within the region.</div><div><span class=\"yellowfg\"> 9:28 AM PST</span>&nbsp;We continue to make progress in restoring connectivity to the remaining EC2 instances and EBS volumes. In the last hour, we have restored underlying connectivity to the majority of the remaining EC2 instance and EBS volumes, but are now working through full recovery at the host level. The majority of affected AWS services remain in recovery and we have seen recovery for the majority of single-AZ RDS databases that were affected by the event. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">11:08 AM PST</span>&nbsp;We continue to make progress in restoring power and connectivity to the remaining EC2 instances and EBS volumes, although recovery of the remaining instances and volumes is taking longer than expected. We believe this is related to the way in which the data center lost power, which has led to failures in the underlying hardware that we are working to recover. While EC2 instances and EBS volumes that have recovered continue to operate normally within the affected data center, we are working to replace hardware components for the recovery of the remaining EC2 instances and EBS volumes. We have multiple engineers working on the underlying hardware failures and expect to see recovery over the next few hours. As is often the case with a loss of power, there may be some hardware that is not recoverable, and so we continue to recommend that you relaunch your EC2 instance, or recreate you EBS volume from a snapshot, if you are able to do so.</div><div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;Over the last hour, after addressing many of the underlying hardware failures, we have seen an accelerated rate of recovery for the affected EC2 instances and EBS volumes. We continue to work on addressing the underlying hardware failures that are preventing the remaining EC2 instances and EBS volumes. For customers that continue to have EC2 instance or EBS volume impairments, relaunching affected EC2 instances or recreating affecting EBS volumes within the affected Availability Zone, continues to be a faster path to full recovery. </div><div><span class=\"yellowfg\"> 1:39 PM PST</span>&nbsp;We continue to make progress in addressing the hardware failures that are delaying recovery of the remaining EC2 instances and EBS volumes. At this stage, if you are still waiting for an EC2 instance or EBS volume to fully recover, we would strongly recommend that you consider relaunching the EC2 instance or recreating the EBS volume from a snapshot. As is often the case with a loss of power, there may be some hardware that is not recoverable, which will prevent us from fully recovering the affected EC2 instances and EBS volumes. We are not quite at that point yet in terms of recovery, but it is unlikely that we will recover all of the small number of remaining EC2 instances and EBS volumes. If you need help in launching new EC2 instances or recreating EBS volumes, please reach out to AWS Support.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Since the last update, we have more than halved the number of affected EC2 instances and EBS volumes and continue to work on the remaining EC2 instances and EBS volumes. The remaining EC2 instances and EBS volumes have all experienced underlying hardware failures due to the nature of the initial power event, which we are working to resolve. We expect to make further progress on this list within the next hour, but some of the remaining EC2 instances and EBS volumes may not be recoverable due to hardware failures. If you have the ability to relaunch an affected EC2 instance or recreate an affected EBS volume from snapshot, we continue to strongly recommend that you take that path.</div><div><span class=\"yellowfg\"> 4:22 PM PST</span>&nbsp;Starting at 4:11 AM PST some EC2 instances and EBS volumes experienced a loss of power in a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Instances in other data centers within the affected Availability Zone, and other Availability Zones within the US-EAST-1 Region were not affected by this event. At 4:55 AM PST, power was restored to EC2 instances and EBS volumes in the affected data center, which allowed the majority of EC2 instances and EBS volumes to recover. However, due to the nature of the power event, some of the underlying hardware experienced failures, which needed to be resolved by engineers within the facility. Engineers worked to recover the remaining EC2 instances and EBS volumes affected by the issue. By 2:30 PM PST, we recovered the vast majority of EC2 instances and EBS volumes. However, some of the affected EC2 instances and EBS volumes were running on hardware that has been affected by the loss of power and is not recoverable. For customers still waiting for recovery of a specific EC2 instance or EBS volume, we recommend that you relaunch the instance or recreate the volume from a snapshot for full recovery. If you need further assistance, please contact AWS Support.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640193970",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:26 AM PST</span>&nbsp;We are investigating increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">10:49 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Some customers may begin to see signs of recovery.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">11:56 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">12:10 PM PST</span>&nbsp;As the root cause of this impact is related to Directory Services, we will continue to provide updates on the new post we have just created for Directory Service in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:56 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. The issue has been resolved and the service is operating normally.  If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Directory Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640203590",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:06 PM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 2:29 PM PST</span>&nbsp;We continue to resolve increased error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication.  We are prioritizing the most impacted directories to expedite resolution.  Additional customers will see recovery as resolution takes place.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;Our mitigation efforts are working as expected and we are making steady progress toward recovery of error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication. We continue to prioritize the most impacted directories to expedite resolution. Additional customers will see recovery as resolution takes place. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 5:57 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Directory Services in US-EAST-1 Region. This also impacted some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. The issue has been resolved and the service is operating normally. Customers using other Active Directory functionality were not impacted by this issue. If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "directoryservice-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Mumbai)",
      "summary": "[RESOLVED] Internet connectivity",
      "date": "1640371277",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:41 AM PST</span>&nbsp;Between 8:59 AM and 9:32 AM PST and between 9:40 AM and 10:16 AM PST we observed Internet connectivity issues with a network provider outside of our network in the AP-SOUTH-1 Region. This impacted Internet connectivity from some customer networks to the AP-SOUTH-1 Region. Connectivity between EC2 instances and other AWS services within the Region was not impacted by this event. The issue has been resolved and we continue to work with the external provider to ensure it does not reoccur.\n</div>",
      "service": "internetconnectivity-ap-south-1"
    },
    {
      "service_name": "Amazon Pinpoint (N. Virginia)",
      "summary": "[RESOLVED] Pinpoint Sending/Receiving Delays",
      "date": "1642185596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:39 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of Pinpoint customers sending and receiving SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">10:53 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:42 AM PST</span>&nbsp;We can confirm that the sending and receiving of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:43 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while sending and receiving SMS messages using US toll-free numbers. Starting at 5:14 AM SMS message delivery receipts were delayed. These delays will continue while we work with our downstream partners through the backlog of delayed delivery receipts.  The issues have been resolved and the service is operating normally.</div>",
      "service": "pinpoint-us-east-1"
    },
    {
      "service_name": "Amazon Simple Notification Service (N. Virginia)",
      "summary": "[RESOLVED] SMS Delivery Delays",
      "date": "1642186756",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of SNS and Pinpoint customers delivering SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">11:00 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We can confirm that the delivery of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:44 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while delivering SMS messages using US toll-free numbers. Also starting at 5:14 AM, SMS message delivery receipts were delayed, which created a backlog of undelivered delivery receipts. We are continuing to work with our downstream partners to clear this backlog. Receipts for new SMS deliveries will also be delayed until this backlog clears. The issues have been resolved and the service is operating normally.</div>",
      "service": "sns-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Increased API Error Rates ",
      "date": "1642432540",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:15 AM PST</span>&nbsp;메시지: AP-NORTHEAST-2 리전에서 API 오류율 증가에 대해 조사하고 있습니다. | We are investigating increased API error rates in the ap-northeast-2 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PST</span>&nbsp;태평양 표준시(PST) 오전 6시 55분부터 오전 7시 40분 사이에 AP-NORTHEAST-2 리전에서 API 오류율이 증가했습니다. 현재 문제가 해결되었으며 서비스가 정상적으로 작동하고 있습니다. 궁금한 점이 있거나 서비스와 관련하여 운영상의 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 AWS 지원 부서에 문의 부탁 드립니다. | Between 6:55 AM and 7:40 AM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS Internet Connectivity (Seoul)",
      "summary": "[RESOLVED] 네트워크 연결 | Network Connectivity",
      "date": "1644150596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:29 AM PST</span>&nbsp;English follows Korean | 한국어버전 뒤에 영어버전이 있습니다\n\n태평양 표준시 (PST) 기준 오전 3시 32분에서 오전 3시 54분사이에 AP-NORTHEAST-2 리전의 단일 가용 영역(apne2-az3)에 있는 일부 EC2 인스턴스의 인터넷 연결에 영향을 미치는 연결 문제가 발생했습니다. 리전 내의 인스턴스 및 서비스에 대한 연결은 영향을 받지 않았습니다. 현재 문제가 해결되었고 연결이 복원되었습니다. 현재 이 문제를 해결하기 위해 필요한 조치는 없습니다. 궁금하신 점이 있으시거나 서비스 운영에 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 문의하시기 바랍니다.\n\nBetween 3:32 AM and 3.54 AM PST we experienced connectivity issues affecting Internet connectivity for some EC2 instances in a single Availability Zone (apne2-az3) in AP-NORTHEAST-2 Region. Connectivity to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored. No action is currently required to address this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "internetconnectivity-ap-northeast-2"
    },
    {
      "service_name": "Amazon Virtual Private Cloud (Frankfurt)",
      "summary": "[RESOLVED] Elevated Error Rates for VPC Console",
      "date": "1646295847",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:24 AM PST</span>&nbsp;We are investigating elevated error rates for the VPC Management Console in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">12:51 AM PST</span>&nbsp;We have identified the root cause and are starting to see recovery of the VPC Management Console in the EU-CENTRAL-1 Region. We recommend signing out and signing back into your account to refresh your session. We continue working towards full recovery and will continue to keep you updated.</div><div><span class=\"yellowfg\"> 1:06 AM PST</span>&nbsp;Between March 2 10:03 PM and March 3 12:30 AM PST, the VPC Management Console experienced elevated error rates in the EU-CENTRAL-1 Region. API and CLI access were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "vpc-eu-central-1"
    },
    {
      "service_name": "AWS Lambda (US-West)",
      "summary": "[RESOLVED] Increased API and Invoke Error Rates",
      "date": "1646856563",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:09 PM PST</span>&nbsp;We are investigating increased invoke and API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">12:37 PM PST</span>&nbsp;We can confirm increased invoke and API error rates in the US-GOV-WEST-1 Region. We have deployed a mitigation strategy and continue to work through full resolution.</div><div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;Between 11:07 AM and 12:50 PM PST, we experienced increased invoke and API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-gov-west-1"
    },
    {
      "service_name": "Multiple services (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1646859683",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.\n</div><div><span class=\"yellowfg\"> 1:16 PM PST</span>&nbsp;We are seeing recovery in API error rates for all services.</div><div><span class=\"yellowfg\"> 1:29 PM PST</span>&nbsp;Between 12:43 PM and 12:59 PM PST, we experienced increased error rates and latencies for some AWS services within the US-EAST-1 Region. All services are now operating normally, but S3 Event Notifications continues to process a backlog of events that developed during the event. \n\nThe root cause of this issue was an update to the SQS and Lambda endpoints that inadvertently prevented some traffic from reaching these endpoints.</div><div><span class=\"yellowfg\"> 1:45 PM PST</span>&nbsp;S3 Event Notifications have delivered the backlog of events. This issue is resolved and all services are now operating normally.</div>",
      "service": "multipleservices-us-east-1"
    },
    {
      "service_name": "AWS DataSync (N. Virginia)",
      "summary": "[RESOLVED] Elevated error rates",
      "date": "1647437391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We are investigating elevated error rates for DataSync tasks with EFS source or destination locations resulting in \"The DataSync destination location is not mounted correctly\".</div><div><span class=\"yellowfg\"> 7:00 AM PDT</span>&nbsp;We continue to investigate elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We'll provide an update at 8:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We have identified the cause of the elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We continue to work towards resolution. We'll provide another update at 9:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 9:11 AM PDT</span>&nbsp;We have started to deploy an update to mitigate the elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The deployment will take approximately 1 hour and 45 minutes to reach all affected regions. We will provide another update once the deployment is complete.</div><div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;Between March 15 10:57 PM and March 16 9:57 AM PDT we experienced elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The issue has been resolved and the service is operating normally.</div>",
      "service": "datasync-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Delayed ENI attachment times",
      "date": "1648779488",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:18 PM PDT</span>&nbsp;We are investigating delayed ENI attachment times for EC2 instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Newly launched EC2 instances or new ENI attachments, may experience delay in establishing network connectivity within the affected Availability Zone. This issue may also affect resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We have identified the root cause and are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;While ENI attachment times have improved, they are still taking longer than normal in the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone. We have identified the root cause for this resource contention and are working to fully resolve the issue. For customers launching instances in the affected Availability Zone or attaching new ENIs to existing instances, full network connectivity on the ENIs may take several minutes to be established, instead of seconds. While we expect attachment times to continue to improve, full recovery here may take up to 2 hours. This issue also affects resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We continue to see an improvement in ENI attachment times, and while they are getting much closer to normal levels, we're still seeing some ENI attachments take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. We are making progress in resolving the resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, and remain on track for full recovery within 1.5 hours. This issue also affects resource provisioning for other services, such as ELB, EMR, ECS and Glue. Some of these services, such as EMR, have mitigated impact by shifting traffic away from the affected Availability Zone, and others like ELB, are now seeing recovery as ENI attachment latencies have improved. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:58 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We remain on track for full recovery within the next hour. At these ENI attachments times, many of the affected services are seeing recovery, or limited impact, as a result of the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 9:31 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We expect to see full recovery within the next 30 to 60 minutes. At these ENI attachments times, ELB and Glue are seeing recovery, while other services - including EMR, EKS, ECS, and RDS - are seeing limited impact within the affected Availability Zone. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:04 PM PDT</span>&nbsp;Over the last 15 minutes, we have seen a further improvement in ENI attachment times within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. The resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone has been resolved, and network state is now beginning to propagate through the affected Availability Zone. As this happens, we would expect to see ENI attachment times return to normal levels over the next 15 - 30 minutes. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:39 PM PDT</span>&nbsp;While we have continued to make some progress over the last 30 minutes, progress has been slower than expected and ENI attachment times have not yet returned to normal levels within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. ENI network state continues to propagate through the affected Availability Zone, but is expected to take another 15 - 30 minutes before we reach normal ENI attachment times. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">11:27 PM PDT</span>&nbsp;ENI network state continues to propagate through the affected Availability Zone (USE1-AZ4) further reducing ENI attachment times. Several affected services - including ELB, Glue and RDS - are now seeing full recovery, while other services - including EMR, EKS, and ECS - are experiencing limited impact at this stage. While we have not yet seen ENI attachment times return to normal levels just yet, we continue to make progress in resolving the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">Apr 1,  1:45 AM PDT</span>&nbsp;Starting at 5:03 PM PDT, we experienced increased ENI attachment times for newly launched EC2 instances and newly attached ENIs within a single Availability Zone  (USE1-AZ4) in the US-EAST-1 Region. The issue was caused by increased resource contention in the subsystem responsible for the propagation of ENI network mappings within the affected Availability Zone. Engineers worked to identify the root cause of the resource contention and took steps to resolve it. By 7:45 PM PDT, ENI attachment times had returned to low single digit minute levels, which allowed most workflows to proceed and limited the impact to other AWS services. Some internal services, such as EMR, weighted away from the affected Availability Zone, which mitigated the impact. ENI attachment times continued to improve as the resource contention issue was addressed, and by 12:50 AM PDT, ENI attachment times had returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Power event impacting some instances",
      "date": "1649238391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:46 AM PDT</span>&nbsp;Starting at 1:23 AM PDT, some EC2 instances experienced a loss of power and some EBS volumes experienced degraded performance within a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. Power was quickly restored to the affected instances and EBS volumes and by 1:40 AM PDT, the majority of EC2 instances and EBS volumes had fully recovered. By 2:05 AM PDT, the vast majority of affected EC2 instances and EBS volumes had fully recovered. Some RDS databases were also affected by the event, and recovered after power was restored. Customers with affected EC2 instances and EBS volumes were notified via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-1"
    }
  ],
  "current": []
}
