{
  "archive": [
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639055699",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:15 AM PST</span>&nbsp;We are investigating elevated error rates for the Redshift Management Console in the US-EAST-1 region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:34 AM PST</span>&nbsp;We continue to investigate elevated error rates for the Redshift Management Console in the US-EAST-1 region. We have identified the issue causing elevated error rates and are actively working to resolve the issue. Customer clusters remain available and are operating normally. Customers can use API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries. </div><div><span class=\"yellowfg\"> 6:30 AM PST</span>&nbsp;We have identified the root cause of the issue causing elevated error rates and are in the process of deploying a fix that will resolve the issue. We do not have a precise ETA for the deployment to complete that we can share at this time. Customer clusters remain available and are operating normally. Customers can use the API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries.</div><div><span class=\"yellowfg\"> 6:58 AM PST</span>&nbsp;Between 12:00 AM and 6:25 AM PST we saw elevated error rates for the Amazon Redshift Management Console in the US-EAST-1 region. This initial error rate increase was observed at 12:00 AM PST and at 4:05 AM PST this error rate increased further. Customer clusters were operating normally throughout and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. We have completed the deployment of a fix. The issue has been resolved and the Management Console and the Amazon Redshift service are operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639067113",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:25 AM PST</span>&nbsp;Earlier today we reported elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. At 6:58 AM PST we reported the issue was resolved but we have since detected that some errors persist despite much lower rates. Redshift clusters remain available and customers can manage their clusters using the API and CLI. Customers can also execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:05 AM PST</span>&nbsp;We continue to investigate intermittent elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. Customers will be able to get the console to load if they refresh their browser tab several times. Once the console has loaded, the console will work as expected and customers will be able to execute queries normally. Redshift clusters remain available and customers can manage their clusters using the API and CLI and can execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;Between 7:25 AM and 10:05 AM PST we experienced increased error rates for the Amazon Redshift Management Console in the US-EAST-1 Region. During this time, clusters were operating normally  and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. The issue is resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Oregon)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639582979",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:43 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-2 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:14 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-2 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.\n</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-2"
    },
    {
      "service_name": "AWS Internet Connectivity (N. California)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583545",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:52 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (US-West)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583722",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:55 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:00 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-GOV-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-GOV-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:16 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-gov-west-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "[RESOLVED] Console Application Upload Errors",
      "date": "1640158400",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:33 PM PST</span>&nbsp;We are investigating an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. Customers who need to update or deploy a new application version should do so using the AWS CLI. Existing applications are not impacted by this issue</div><div><span class=\"yellowfg\">Dec 22, 12:34 AM PST</span>&nbsp;We continue to investigate an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. We are determining the root causes and working through steps to mitigate the issue. Customers who need to update or deploy a new application version should do so using the AWS CLI while we work towards resolving the issue. Existing applications are not impacted by this issue.</div><div><span class=\"yellowfg\">Dec 22,  1:20 AM PST</span>&nbsp;We have identified the root cause and prepared a fix to address the issue that prevents customers from uploading new application versions through the Elastic Beanstalk console in multiple Regions. The service team is testing this fix and preparing for deployment to the Regions that are affected by this issue. We expect to see full recovery by 3:00 AM PST and will continue to keep you updated if this ETA changes. Customers who need to update or deploy a new application version should do so using the AWS CLI until the issue is fully resolved.</div><div><span class=\"yellowfg\">Dec 22,  3:21 AM PST</span>&nbsp;Between December 21, 2021 at 6:37 PM  and December 22, 2021 at 03:17 AM PST, customers were unable to upload their code through the Elastic Beanstalk console due to a Content Security Policy (CSP) error. Customers were impacted when they attempted to upload a new application version for existing environments or upload their code when creating a new environment in multiple regions. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] API Error Rates",
      "date": "1640176551",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:35 AM PST</span>&nbsp;We are investigating increased EC2 launch failures and networking connectivity issues for some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 5:01 AM PST</span>&nbsp;We can confirm a loss of power within a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. This is affecting availability and connectivity to EC2 instances that are part of the affected data center within the affected Availability Zone. We are also experiencing elevated RunInstance API error rates for launches within the affected Availability Zone. Connectivity and power to other data centers within the affected Availability Zone, or other Availability Zones within the US-EAST-1 Region are not affected by this issue, but we would recommend failing away from the affected Availability Zone (USE1-AZ4) if you are able to do so. We continue to work to address the issue and restore power within the affected data center.</div><div><span class=\"yellowfg\"> 5:18 AM PST</span>&nbsp;We continue to make progress in restoring power to the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have now restored power to the majority of instances and networking devices within the affected data center and are starting to see some early signs of recovery. Customers experiencing connectivity or instance availability issues within the affected Availability Zone, should start to see some recovery as power is restored to the affected data center. RunInstances API error rates are returning to normal levels and we are working to recover affected EC2 instances and EBS volumes. While we would expect continued improvement over the coming hour, we would still recommend failing away from the Availability Zone if you are able to do so to mitigate this issue.</div><div><span class=\"yellowfg\"> 5:39 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. Network connectivity within the affected Availability Zone has also returned to normal levels. While all services are starting to see meaningful recovery, services which were hosting endpoints within the affected data center - such as single-AZ RDS databases, ElastiCache, etc. - would have seen impact during the event, but are starting to see recovery now. Given the level of recovery, if you have not yet failed away from the affected Availability Zone, you should be starting to see recovery at this stage. </div><div><span class=\"yellowfg\"> 6:13 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. We continue to make progress in recovering the remaining EC2 instances and EBS volumes within the affected Availability Zone. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 6:51 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. For the remaining EC2 instances, we are experiencing some network connectivity issues, which is slowing down full recovery. We believe we understand why this is the case and are working on a resolution. Once resolved, we expect to see faster recovery for the remaining EC2 instances and EBS volumes. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 8:02 AM PST</span>&nbsp;Power continues to be stable within the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have been working to resolve the connectivity issues that the remaining EC2 instances and EBS volumes are experiencing in the affected data center, which is part of a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have addressed the connectivity issue for the affected EBS volumes, which are now starting to see further recovery. We continue to work on mitigating the networking impact for EC2 instances within the affected data center, and expect to see further recovery there starting in the next 30 minutes. Since the EC2 APIs have been healthy for some time within the affected Availability Zone, the fastest path to recovery now would be to relaunch affected EC2 instances within the affected Availability Zone or other Availability Zones within the region.</div><div><span class=\"yellowfg\"> 9:28 AM PST</span>&nbsp;We continue to make progress in restoring connectivity to the remaining EC2 instances and EBS volumes. In the last hour, we have restored underlying connectivity to the majority of the remaining EC2 instance and EBS volumes, but are now working through full recovery at the host level. The majority of affected AWS services remain in recovery and we have seen recovery for the majority of single-AZ RDS databases that were affected by the event. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">11:08 AM PST</span>&nbsp;We continue to make progress in restoring power and connectivity to the remaining EC2 instances and EBS volumes, although recovery of the remaining instances and volumes is taking longer than expected. We believe this is related to the way in which the data center lost power, which has led to failures in the underlying hardware that we are working to recover. While EC2 instances and EBS volumes that have recovered continue to operate normally within the affected data center, we are working to replace hardware components for the recovery of the remaining EC2 instances and EBS volumes. We have multiple engineers working on the underlying hardware failures and expect to see recovery over the next few hours. As is often the case with a loss of power, there may be some hardware that is not recoverable, and so we continue to recommend that you relaunch your EC2 instance, or recreate you EBS volume from a snapshot, if you are able to do so.</div><div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;Over the last hour, after addressing many of the underlying hardware failures, we have seen an accelerated rate of recovery for the affected EC2 instances and EBS volumes. We continue to work on addressing the underlying hardware failures that are preventing the remaining EC2 instances and EBS volumes. For customers that continue to have EC2 instance or EBS volume impairments, relaunching affected EC2 instances or recreating affecting EBS volumes within the affected Availability Zone, continues to be a faster path to full recovery. </div><div><span class=\"yellowfg\"> 1:39 PM PST</span>&nbsp;We continue to make progress in addressing the hardware failures that are delaying recovery of the remaining EC2 instances and EBS volumes. At this stage, if you are still waiting for an EC2 instance or EBS volume to fully recover, we would strongly recommend that you consider relaunching the EC2 instance or recreating the EBS volume from a snapshot. As is often the case with a loss of power, there may be some hardware that is not recoverable, which will prevent us from fully recovering the affected EC2 instances and EBS volumes. We are not quite at that point yet in terms of recovery, but it is unlikely that we will recover all of the small number of remaining EC2 instances and EBS volumes. If you need help in launching new EC2 instances or recreating EBS volumes, please reach out to AWS Support.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Since the last update, we have more than halved the number of affected EC2 instances and EBS volumes and continue to work on the remaining EC2 instances and EBS volumes. The remaining EC2 instances and EBS volumes have all experienced underlying hardware failures due to the nature of the initial power event, which we are working to resolve. We expect to make further progress on this list within the next hour, but some of the remaining EC2 instances and EBS volumes may not be recoverable due to hardware failures. If you have the ability to relaunch an affected EC2 instance or recreate an affected EBS volume from snapshot, we continue to strongly recommend that you take that path.</div><div><span class=\"yellowfg\"> 4:22 PM PST</span>&nbsp;Starting at 4:11 AM PST some EC2 instances and EBS volumes experienced a loss of power in a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Instances in other data centers within the affected Availability Zone, and other Availability Zones within the US-EAST-1 Region were not affected by this event. At 4:55 AM PST, power was restored to EC2 instances and EBS volumes in the affected data center, which allowed the majority of EC2 instances and EBS volumes to recover. However, due to the nature of the power event, some of the underlying hardware experienced failures, which needed to be resolved by engineers within the facility. Engineers worked to recover the remaining EC2 instances and EBS volumes affected by the issue. By 2:30 PM PST, we recovered the vast majority of EC2 instances and EBS volumes. However, some of the affected EC2 instances and EBS volumes were running on hardware that has been affected by the loss of power and is not recoverable. For customers still waiting for recovery of a specific EC2 instance or EBS volume, we recommend that you relaunch the instance or recreate the volume from a snapshot for full recovery. If you need further assistance, please contact AWS Support.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640193970",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:26 AM PST</span>&nbsp;We are investigating increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">10:49 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Some customers may begin to see signs of recovery.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">11:56 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">12:10 PM PST</span>&nbsp;As the root cause of this impact is related to Directory Services, we will continue to provide updates on the new post we have just created for Directory Service in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:56 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. The issue has been resolved and the service is operating normally.  If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Directory Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640203590",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:06 PM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 2:29 PM PST</span>&nbsp;We continue to resolve increased error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication.  We are prioritizing the most impacted directories to expedite resolution.  Additional customers will see recovery as resolution takes place.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;Our mitigation efforts are working as expected and we are making steady progress toward recovery of error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication. We continue to prioritize the most impacted directories to expedite resolution. Additional customers will see recovery as resolution takes place. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 5:57 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Directory Services in US-EAST-1 Region. This also impacted some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. The issue has been resolved and the service is operating normally. Customers using other Active Directory functionality were not impacted by this issue. If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "directoryservice-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Mumbai)",
      "summary": "[RESOLVED] Internet connectivity",
      "date": "1640371277",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:41 AM PST</span>&nbsp;Between 8:59 AM and 9:32 AM PST and between 9:40 AM and 10:16 AM PST we observed Internet connectivity issues with a network provider outside of our network in the AP-SOUTH-1 Region. This impacted Internet connectivity from some customer networks to the AP-SOUTH-1 Region. Connectivity between EC2 instances and other AWS services within the Region was not impacted by this event. The issue has been resolved and we continue to work with the external provider to ensure it does not reoccur.\n</div>",
      "service": "internetconnectivity-ap-south-1"
    },
    {
      "service_name": "Amazon Pinpoint (N. Virginia)",
      "summary": "[RESOLVED] Pinpoint Sending/Receiving Delays",
      "date": "1642185596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:39 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of Pinpoint customers sending and receiving SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">10:53 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:42 AM PST</span>&nbsp;We can confirm that the sending and receiving of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:43 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while sending and receiving SMS messages using US toll-free numbers. Starting at 5:14 AM SMS message delivery receipts were delayed. These delays will continue while we work with our downstream partners through the backlog of delayed delivery receipts.  The issues have been resolved and the service is operating normally.</div>",
      "service": "pinpoint-us-east-1"
    },
    {
      "service_name": "Amazon Simple Notification Service (N. Virginia)",
      "summary": "[RESOLVED] SMS Delivery Delays",
      "date": "1642186756",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of SNS and Pinpoint customers delivering SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">11:00 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We can confirm that the delivery of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:44 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while delivering SMS messages using US toll-free numbers. Also starting at 5:14 AM, SMS message delivery receipts were delayed, which created a backlog of undelivered delivery receipts. We are continuing to work with our downstream partners to clear this backlog. Receipts for new SMS deliveries will also be delayed until this backlog clears. The issues have been resolved and the service is operating normally.</div>",
      "service": "sns-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Increased API Error Rates ",
      "date": "1642432540",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:15 AM PST</span>&nbsp;메시지: AP-NORTHEAST-2 리전에서 API 오류율 증가에 대해 조사하고 있습니다. | We are investigating increased API error rates in the ap-northeast-2 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PST</span>&nbsp;태평양 표준시(PST) 오전 6시 55분부터 오전 7시 40분 사이에 AP-NORTHEAST-2 리전에서 API 오류율이 증가했습니다. 현재 문제가 해결되었으며 서비스가 정상적으로 작동하고 있습니다. 궁금한 점이 있거나 서비스와 관련하여 운영상의 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 AWS 지원 부서에 문의 부탁 드립니다. | Between 6:55 AM and 7:40 AM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS Internet Connectivity (Seoul)",
      "summary": "[RESOLVED] 네트워크 연결 | Network Connectivity",
      "date": "1644150596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:29 AM PST</span>&nbsp;English follows Korean | 한국어버전 뒤에 영어버전이 있습니다\n\n태평양 표준시 (PST) 기준 오전 3시 32분에서 오전 3시 54분사이에 AP-NORTHEAST-2 리전의 단일 가용 영역(apne2-az3)에 있는 일부 EC2 인스턴스의 인터넷 연결에 영향을 미치는 연결 문제가 발생했습니다. 리전 내의 인스턴스 및 서비스에 대한 연결은 영향을 받지 않았습니다. 현재 문제가 해결되었고 연결이 복원되었습니다. 현재 이 문제를 해결하기 위해 필요한 조치는 없습니다. 궁금하신 점이 있으시거나 서비스 운영에 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 문의하시기 바랍니다.\n\nBetween 3:32 AM and 3.54 AM PST we experienced connectivity issues affecting Internet connectivity for some EC2 instances in a single Availability Zone (apne2-az3) in AP-NORTHEAST-2 Region. Connectivity to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored. No action is currently required to address this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "internetconnectivity-ap-northeast-2"
    },
    {
      "service_name": "Amazon Virtual Private Cloud (Frankfurt)",
      "summary": "[RESOLVED] Elevated Error Rates for VPC Console",
      "date": "1646295847",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:24 AM PST</span>&nbsp;We are investigating elevated error rates for the VPC Management Console in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">12:51 AM PST</span>&nbsp;We have identified the root cause and are starting to see recovery of the VPC Management Console in the EU-CENTRAL-1 Region. We recommend signing out and signing back into your account to refresh your session. We continue working towards full recovery and will continue to keep you updated.</div><div><span class=\"yellowfg\"> 1:06 AM PST</span>&nbsp;Between March 2 10:03 PM and March 3 12:30 AM PST, the VPC Management Console experienced elevated error rates in the EU-CENTRAL-1 Region. API and CLI access were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "vpc-eu-central-1"
    },
    {
      "service_name": "AWS Lambda (US-West)",
      "summary": "[RESOLVED] Increased API and Invoke Error Rates",
      "date": "1646856563",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:09 PM PST</span>&nbsp;We are investigating increased invoke and API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">12:37 PM PST</span>&nbsp;We can confirm increased invoke and API error rates in the US-GOV-WEST-1 Region. We have deployed a mitigation strategy and continue to work through full resolution.</div><div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;Between 11:07 AM and 12:50 PM PST, we experienced increased invoke and API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-gov-west-1"
    },
    {
      "service_name": "Multiple services (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1646859683",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.\n</div><div><span class=\"yellowfg\"> 1:16 PM PST</span>&nbsp;We are seeing recovery in API error rates for all services.</div><div><span class=\"yellowfg\"> 1:29 PM PST</span>&nbsp;Between 12:43 PM and 12:59 PM PST, we experienced increased error rates and latencies for some AWS services within the US-EAST-1 Region. All services are now operating normally, but S3 Event Notifications continues to process a backlog of events that developed during the event. \n\nThe root cause of this issue was an update to the SQS and Lambda endpoints that inadvertently prevented some traffic from reaching these endpoints.</div><div><span class=\"yellowfg\"> 1:45 PM PST</span>&nbsp;S3 Event Notifications have delivered the backlog of events. This issue is resolved and all services are now operating normally.</div>",
      "service": "multipleservices-us-east-1"
    },
    {
      "service_name": "AWS DataSync (N. Virginia)",
      "summary": "[RESOLVED] Elevated error rates",
      "date": "1647437391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We are investigating elevated error rates for DataSync tasks with EFS source or destination locations resulting in \"The DataSync destination location is not mounted correctly\".</div><div><span class=\"yellowfg\"> 7:00 AM PDT</span>&nbsp;We continue to investigate elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We'll provide an update at 8:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We have identified the cause of the elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We continue to work towards resolution. We'll provide another update at 9:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 9:11 AM PDT</span>&nbsp;We have started to deploy an update to mitigate the elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The deployment will take approximately 1 hour and 45 minutes to reach all affected regions. We will provide another update once the deployment is complete.</div><div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;Between March 15 10:57 PM and March 16 9:57 AM PDT we experienced elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The issue has been resolved and the service is operating normally.</div>",
      "service": "datasync-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Delayed ENI attachment times",
      "date": "1648779488",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:18 PM PDT</span>&nbsp;We are investigating delayed ENI attachment times for EC2 instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Newly launched EC2 instances or new ENI attachments, may experience delay in establishing network connectivity within the affected Availability Zone. This issue may also affect resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We have identified the root cause and are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;While ENI attachment times have improved, they are still taking longer than normal in the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone. We have identified the root cause for this resource contention and are working to fully resolve the issue. For customers launching instances in the affected Availability Zone or attaching new ENIs to existing instances, full network connectivity on the ENIs may take several minutes to be established, instead of seconds. While we expect attachment times to continue to improve, full recovery here may take up to 2 hours. This issue also affects resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We continue to see an improvement in ENI attachment times, and while they are getting much closer to normal levels, we're still seeing some ENI attachments take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. We are making progress in resolving the resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, and remain on track for full recovery within 1.5 hours. This issue also affects resource provisioning for other services, such as ELB, EMR, ECS and Glue. Some of these services, such as EMR, have mitigated impact by shifting traffic away from the affected Availability Zone, and others like ELB, are now seeing recovery as ENI attachment latencies have improved. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:58 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We remain on track for full recovery within the next hour. At these ENI attachments times, many of the affected services are seeing recovery, or limited impact, as a result of the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 9:31 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We expect to see full recovery within the next 30 to 60 minutes. At these ENI attachments times, ELB and Glue are seeing recovery, while other services - including EMR, EKS, ECS, and RDS - are seeing limited impact within the affected Availability Zone. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:04 PM PDT</span>&nbsp;Over the last 15 minutes, we have seen a further improvement in ENI attachment times within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. The resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone has been resolved, and network state is now beginning to propagate through the affected Availability Zone. As this happens, we would expect to see ENI attachment times return to normal levels over the next 15 - 30 minutes. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:39 PM PDT</span>&nbsp;While we have continued to make some progress over the last 30 minutes, progress has been slower than expected and ENI attachment times have not yet returned to normal levels within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. ENI network state continues to propagate through the affected Availability Zone, but is expected to take another 15 - 30 minutes before we reach normal ENI attachment times. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">11:27 PM PDT</span>&nbsp;ENI network state continues to propagate through the affected Availability Zone (USE1-AZ4) further reducing ENI attachment times. Several affected services - including ELB, Glue and RDS - are now seeing full recovery, while other services - including EMR, EKS, and ECS - are experiencing limited impact at this stage. While we have not yet seen ENI attachment times return to normal levels just yet, we continue to make progress in resolving the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">Apr 1,  1:45 AM PDT</span>&nbsp;Starting at 5:03 PM PDT, we experienced increased ENI attachment times for newly launched EC2 instances and newly attached ENIs within a single Availability Zone  (USE1-AZ4) in the US-EAST-1 Region. The issue was caused by increased resource contention in the subsystem responsible for the propagation of ENI network mappings within the affected Availability Zone. Engineers worked to identify the root cause of the resource contention and took steps to resolve it. By 7:45 PM PDT, ENI attachment times had returned to low single digit minute levels, which allowed most workflows to proceed and limited the impact to other AWS services. Some internal services, such as EMR, weighted away from the affected Availability Zone, which mitigated the impact. ENI attachment times continued to improve as the resource contention issue was addressed, and by 12:50 AM PDT, ENI attachment times had returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Power event impacting some instances",
      "date": "1649238391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:46 AM PDT</span>&nbsp;Starting at 1:23 AM PDT, some EC2 instances experienced a loss of power and some EBS volumes experienced degraded performance within a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. Power was quickly restored to the affected instances and EBS volumes and by 1:40 AM PDT, the majority of EC2 instances and EBS volumes had fully recovered. By 2:05 AM PDT, the vast majority of affected EC2 instances and EBS volumes had fully recovered. Some RDS databases were also affected by the event, and recovered after power was restored. Customers with affected EC2 instances and EBS volumes were notified via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased Launch Failures",
      "date": "1650382771",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:39 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption by Default in the US-EAST-1 Region for three Availability Zones: use1-az2, use1-az6 and use1-az5.  Existing instances are not affected, and no other launch requests are affected.  We have identified the root cause and are working towards recovery. </div><div><span class=\"yellowfg\"> 9:18 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that use EBS Encryption by Default in the US-EAST-1 Region.  Existing instances are not affected, and no other launch requests are affected. We have identified the root cause.  We are seeing recovery in use1-az5.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:46 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT. We began seeing significant recovery in use-az6 at 10:20 AM PDT and are beginning to see recovery in use1-az2.</div><div><span class=\"yellowfg\">11:30 AM PDT</span>&nbsp;Between 6:35 AM and 11:00 AM PDT we experienced increased error rates for new instance launches in the US-EAST-1 Region. Existing instances were unaffected by this issue. By 9:05 AM, we had full recovery in use1-az5. By 10:20 AM, we had full recovery in use1-az6, and at 11:00 AM we had recovered the final Availability Zone, use1-az2. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Increased Launch Error Rates",
      "date": "1650969049",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:30 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption from an unencrypted AMI in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. For a small number of volumes in the Availability Zone, increased EBS I/O latencies may also be experienced. Existing instances are not affected, and no other launch requests are affected. We have identified the root cause and are working towards recovery.</div><div><span class=\"yellowfg\"> 4:30 AM PDT</span>&nbsp;Between 1:32 AM and 3:46 AM PDT, we experienced increased error rates for new launches that used EBS volumes in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. A small number of volumes in the Availability Zone may have experienced increased EBS I/O latencies during this time, and this issue has also been resolved for the vast majority of customers. We will be directly notifying a small subset of customers who may still be affected by increased EBS I/O latencies via the AWS Health Dashboard.</div>",
      "service": "ec2-ap-southeast-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1651071794",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We are investigating increased Amazon Redshift API and Console error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:58 AM PDT</span>&nbsp;We continue to investigate increased Amazon Redshift API and Console error rates in the US-EAST-1 Region that impact cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers using temporary credentials.</div><div><span class=\"yellowfg\"> 9:34 AM PDT</span>&nbsp;We are narrowing in on the root cause of the issue causing increased Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue is impacting cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;Between 7:12 AM and 9:52 AM PDT, we experienced increased error rates in Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue impacted cluster management operations and certain customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Network Connectivity Issues",
      "date": "1651363256",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:00 PM PDT</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ2) 에서 일부 EC2 인스턴스, RDS 데이터베이스의 연결 문제 및 일부 EBS 볼륨의 성능 저하를 조사하고 있습니다.이 문제를 해결하기 위해 노력하고 있습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우  AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We are investigating connectivity issues for some EC2 instances, RDS databases and degraded performance for some EBS volumes in a single Availability Zone (APNE2-AZ2) in the AP-NORTHEAST-2 Region. We are working to resolve the issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;태평양 표준시 기준 오후 4시 29분부터 AP-NORTHEAST-2 리전의 단일 가용 영역 내에 있는 일부 EC2 인스턴스 및 EBS 볼륨의 기본 하드웨어에 대한 전력 손실이 발생했습니다.전력은 태평양 표준시 기준 오후 4시 36분에 복원되었고, 영향을 받은 EC2 인스턴스 및 EBS 볼륨이 복구되기 시작했습니다.이 단계에서 영향을 받는 EC2 인스턴스 및 EBS 볼륨의 대부분이 복구되었으며, AWS에서는 여전히 영향을 받는 소수의 인스턴스에 대한 작업을 계속 진행하고 있습니다.일부 RDS 데이터베이스에서도 이 기간 동안 연결 문제가 발생했습니다.다중 AZ 데이터베이스는 영향을 받는 가용 영역에서 성공적으로 장애가 회복되었지만 단일 AZ 데이터베이스는 전원이 복원될 때까지 손상된 상태로 남아 있었습니다.다른 가용 영역은 이 문제의 영향을 받지 않습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support )를 통해 AWS 지원 부서에 문의하십시오. | Starting at 4:29 PM PDT, we experienced a loss of power to the underlying hardware for some EC2 instances and EBS volumes within a single Availability Zone in the AP-NORTHEAST-2 Region. Power was restored at 4:36 PM PDT, and the affected EC2 instances and EBS volumes began to recover. At this stage, the majority of affected EC2 instances and EBS volumes have recovered, and we continue to work on the small number that are still affected. Some RDS databases also experienced connectivity issues during this time period. Multi-AZ databases successfully failed away from the affected Availability Zone, but Single-AZ databases would remain impaired until the power was restored. Other Availability Zones are not affected by this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:31 PM PDT</span>&nbsp;영향을 받는 모든 EC2 인스턴스 및 EBS 볼륨의 기본 하드웨어의 전원이 복원되었음을 확인하였습니다. EC2 인스턴스와 EBS 볼륨은 계속 복구되며, 지금은 소수의  인스턴스만 남아있습니다 (대부분 i3 인스턴스) 그리고, 영향을 받는 가용 영역에서 소수의 EBS 볼륨의 성능이 저하되었습니다.RDS Aurora를 포함한 단일 AZ RDS 데이터베이스는 영향을 받는 가용 영역에 영향을 주었지만 전원이 복원되면서 복구되고 있습니다.모든 다중 AZ RDS 데이터베이스가 영향을 받는 가용 영역에서 장애 조치가 되었습니다.소수의 단일 AZ 애플리케이션 로드 밸런서가 영향을 받는 가용 영역 내에서 패킷 손실이 증가했지만, 다른 부하 분산 트래픽은 다른 가용 영역으로 이동되었습니다.API Gateway는 영향을 받는 가용 영역 내에서 M-TLS 요청에 대한 패킷 손실이 증가했지만 이제는 완전히 복구되었습니다.AWS에서는 여전히 영향을 받는 소수의 EC2 인스턴스 및 EBS 볼륨에 대해 계속 작업하고 있습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We can confirm that power has been restored to the underlying hardware of all affected EC2 instances and EBS volumes. EC2 instances and EBS volumes continue to recover and we now have a small number of instances – mostly i3 instances – and degraded performance for a small number of EBS volumes in the affected Availability Zone. Single-AZ RDS databases – including Amazon Aurora – experienced impact in the affected Availability Zone, but are recovering as power is restored. All Multi-AZ RDS databases have failed away from the affected Availability Zone. A small number of Single-AZ Application Load Balancers experienced elevated packet loss within the affected Availability Zone, but other load balancing traffic was shifted to other Availability Zones. API Gateway experienced elevated packet loss for M-TLS requests within the affected Availability Zone, but has now fully recovered. We continue to work on the small number of EC2 instances and EBS volumes that are still affected. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 6:00 PM PDT</span>&nbsp;영향을 받은 EC2 인스턴스 및 EBS 볼륨의 대부분을 복구했습니다.아직 성능 저하를 겪고 있는 적은수의 EBS 볼륨이 있으며, 이를 해결하기 위해 계속 노력하고 있습니다.일부 EC2 인스턴스 및 EBS 볼륨은 전력 손실 후 복구할 수 없는 하드웨어에서 호스팅되었을 수 있습니다.고객은 해당하는 EC2 인스턴스 및 EBS 볼륨에 대한 만료 통지를 받을것입니다. EC2 인스턴스, EBS 볼륨 또는 RDS 데이터베이스에서 여전히 영향이 있는 경우 영향을 받는 리소스를 다시 시작하거나 다시 생성하는 단계를 수행하는 것이 좋습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We have recovered the vast majority of the affected EC2 instances and EBS volumes. We have a small number of EBS volumes that are still experiencing degraded performance and continue to work to resolve them. Some EC2 instances and EBS volumes may have been hosted on hardware that was not recoverable after the loss of power. Customers will receive retirement notices for EC2 instances and EBS volumes where that is the case. If you continue to see impact for an EC2 instance, EBS volume, or RDS database, we recommend taking steps to relaunch or recreate the affected resource. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS IoT Core (Oregon)",
      "summary": "[RESOLVED] Increased API Errors and Latencies",
      "date": "1651634279",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for Connect, Subscribe &amp; Publish operations in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:40 PM PDT</span>&nbsp;We have identified the root cause for increased error rates and latency of Connect, Subscribe and Publish operations for new connections in the US-WEST-2 Region and are working towards resolution. A scheduled update to optimize the underlying infrastructure for the AWS IoT registry resulted in lower memory availability than required - this has been corrected, and recovery is in progress. Existing connections are not affected.</div><div><span class=\"yellowfg\"> 8:55 PM PDT</span>&nbsp;Between 7:17 PM and 8:41 PM PDT, we experienced increased error rates and latency for Connect, Subscribe, Publish and Registry operations in the US-WEST-2 Region. Existing connections were not affected by the event. The issue has been resolved and the service is operating normally.</div>",
      "service": "awsiot-us-west-2"
    },
    {
      "service_name": "Amazon API Gateway (Ireland)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1652721211",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:13 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Some other AWS services are also affected by this issue, which we will provide further details on shortly. We are working to determine root cause and resolve the issues.</div><div><span class=\"yellowfg\">10:31 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Both the error rates and latencies are at a low enough level where retries would allow for a request to succeed. We have identified the root cause and are working to fully resolve the issue. The following AWS services are experiencing low levels of error rates as a result of this issue: Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;Between 9:39 AM and 10:27 AM PDT, we experienced increased error rates and latencies for API Gateway in the EU-WEST-1 Region. All AWS services - including Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker - have now recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "apigateway-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API errors and latencies",
      "date": "1652830549",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:35 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. This issue does not affect traffic on running load balancers.</div><div><span class=\"yellowfg\"> 5:09 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic remains unaffected on running load balancers. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact to your account events. We’ll continue to update this post as we have more information to share.</div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for ELB APIs in the US-EAST-1 Region and have taken steps to mitigate the issue. We are now seeing recovery for the affected ELB APIs. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. We will continue to monitor until we can confirm full recovery.</div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;Between 3:58 PM and 5:28 PM PDT, we experienced increased error rates and latencies for some ELB APIs in the US-EAST-1 Region. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Stockholm)",
      "summary": "[RESOLVED] Elevated API latencies and error rates and packetloss",
      "date": "1652985887",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;We are investigating elevated API errors and latencies and elevated packet loss in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\">12:08 PM PDT</span>&nbsp;Between 11:12 AM and 11:45 AM PDT, we experienced elevated API errors and latencies and connectivity issues in the EU-NORTH-1 Region. The issue has been resolved and the services are operating normally.</div>",
      "service": "kinesis-eu-north-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased Errors Managing Payment Methods",
      "date": "1653519230",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:53 PM PDT</span>&nbsp;Between 12:30 PM and 2:10 PM PDT we experienced increased errors managing payment methods. The issue is resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Intermittent DNS resolution failures",
      "date": "1653618095",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:21 PM PDT</span>&nbsp;We are aware of intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside AWS.  DNS queries for domains hosted on Route 53 are operating without any issues at this time.  We are actively working with the third-party DNS provider to resolve the issue as quickly as possible.</div><div><span class=\"yellowfg\"> 8:07 PM PDT</span>&nbsp;We confirm intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside of AWS. The third-party DNS provider is working toward resolution. We are also working toward a resolution that addresses the issues the third-party provider has encountered. Queries for DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 8:42 PM PDT</span>&nbsp;We can confirm the start of recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider.  We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 9:01 PM PDT</span>&nbsp;We can confirm the broad recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider. We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\">10:24 PM PDT</span>&nbsp;Between 5:35 PM and 9:58 PM PDT, we experienced intermittent DNS resolution issues for certain specific AWS endpoints. We can confirm that DNS resolution issues for these AWS names have been resolved. Since DNS answers for some of these names that were affected by issues with the third-party DNS provider could have been cached on AWS DNS resolvers, we are flushing these resolver caches over the next few hours. If you run your own DNS Resolvers, and you experience DNS resolution issues we suggest to flush your DNS Resolver cache. Queries for the DNS records hosted on Route 53 were not affected by this issue.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1654825968",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:52 PM PDT</span>&nbsp;We are investigating an increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. Existing load balancers are not affected by the issue. We are working to identify the root cause and resolve the issue.</div><div><span class=\"yellowfg\"> 7:03 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. This is causing increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We are working to resolve the issue and expect to see an improvement in error rates as that progresses.</div><div><span class=\"yellowfg\"> 7:54 PM PDT</span>&nbsp;We continue to make progress towards resolving the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. While error rates have stabilized, we continue to see increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We will continue to keep you updated on our progress.</div><div><span class=\"yellowfg\"> 8:13 PM PDT</span>&nbsp;Starting at 7:46 PM PDT we experienced higher error rates and latencies for AWS services within the US-EAST-1 region. These error rates and latencies have seen some improvement from 7:55 PM PDT, but remain elevated. The issue is also affecting the AWS Management Console for the US-EAST-1 region. We continue to work towards mitigating the impact and will continue to provide updates as we progress.</div><div><span class=\"yellowfg\"> 8:38 PM PDT</span>&nbsp;We continue to work on addressing the error rates and latencies for AWS services in the US-EAST-1 Region. The issue initially affected the Elastic Load Balancing APIs, with some impact to other services, including Elastic Container Service, Amazon Certificate Manager, and Directory Services. At 7:46 PM PDT, other AWS services began to experience an increase in error rates and latencies. Action was taken and error rates started to improve at 7:55 PM PDT. This issue also affected access to the the AWS Management Console for the US-EAST-1 Region. Error rates and latencies for services in the US-EAST-1 Region remain elevated for a number of services, including Connect, DynamoDB, SQS, SNS, EC2, CloudFormation, CloudFront, amongst others. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\"> 9:13 PM PDT</span>&nbsp;We continue to see a reduction in error rates and latencies for AWS services within the US-EAST-1 Region as we work to resolve the issue. While many AWS services are experiencing elevated error rates and latencies, services that are experiencing higher error rates have been tagged on this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. The AWS Management Console is operating normally, however some customers may observe API Errors at times. AWS Connect error rates have improved as well and we continue to work towards full recovery. We are working on applying mitigations to fully resolve the issue.</div><div><span class=\"yellowfg\"> 9:32 PM PDT</span>&nbsp;We continue to see a reduction in API error rates and latencies for services within the US-EAST-1 Region. Elastic Load Balancer APIs have recovered and returned to normal levels. The AWS Management Console has recovered. AWS Connect error rates have returned to normal levels. CloudWatch metrics have recovered and the EC2 API error rates have returned to normal levels. DynamoDB has recovered and is operating normally. Services that were affected by this event remain tagged to this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. </div><div><span class=\"yellowfg\"> 9:58 PM PDT</span>&nbsp;Starting at 6:01 PM PDT, we experienced elevated error rates and latencies for AWS services within the US-EAST-1 Region. The issue affected AWS service APIs, with no impact to data plane services such as EC2 instances, EBS volumes, or Elastic Load Balancers. We started to see recovery at 7:55 PM PDT and were fully recovered by 9:25 PM PDT. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Incorrect Free Tier Email Alerts",
      "date": "1655342154",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:15 PM PDT</span>&nbsp;We are investigating customers receiving email alerts about Free Tier Usage which does not match what customers are seeing in the billing console.</div><div><span class=\"yellowfg\"> 6:39 PM PDT</span>&nbsp;Between 11:00 AM and 6:00 PM PDT, some customers received incorrect email alerts about their Free Tier Usage. This was caused by an incorrect software deployment, which has since been rolled back. During this time, the AWS Billing Console was reporting the correct information. Customers who received an incorrect email alert will receive a notification via the AWS Health Dashboard Event Log in your AWS account.</div><div><span class=\"yellowfg\"> 6:53 PM PDT</span>&nbsp;We want to clarify our previous post. Between 11:00 AM and 6:00 PM PDT, the Home page and the Bills page on the AWS Billing Console reported correct information. The Free Tier page on the AWS Billing Console displayed the same incorrect information included in the incorrect email alert.  We have corrected the issue that caused both the incorrect emails and the incorrect information in the Free Tier page on the AWS Billing Console, and they are all reporting correct information now.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Availability Decreased",
      "date": "1657032030",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:40 AM PDT</span>&nbsp;We can confirm a decrease in the Cost Explorer Console availability. We are actively working towards resolution.</div><div><span class=\"yellowfg\"> 8:39 AM PDT</span>&nbsp;We have identified the cause of the issue affecting Cost Explorer and are working towards resolution. Customers using the Cost Explorer console may experience long load times, timeouts and errors using the console. Cost Explorer APIs are experiencing elevated latencies and error rates. The issue is limited to the Cost Explorer console and APIs, AWS Billing is not otherwise impacted.</div><div><span class=\"yellowfg\"> 9:19 AM PDT</span>&nbsp;We continue to investigate the issue affecting Cost Explorer and are working towards resolution.  We will provide the next update by 10:00 AM PDT.</div><div><span class=\"yellowfg\">10:01 AM PDT</span>&nbsp;We continue to investigate the issue affecting Cost Explorer and are working towards resolution.  We will provide the next update by 10:30am PDT.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;Between 05:10 AM and 10:27 AM PDT, customers using the Cost Explorer console experienced long load times, timeouts, or errors using the console. Cost Explorer APIs also experienced elevated latencies and error rates. The issue has been resolved and the service is operating normally. AWS Billing was otherwise not impacted.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] Impaired EC2 Instances",
      "date": "1657475798",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:56 AM PDT</span>&nbsp;We are investigating instance impairments in a single Availability Zone (EUW2-AZ1) in the EU-WEST-2 Region. Other Availability Zones are not affected by the event and we are working to resolve the issue.</div><div><span class=\"yellowfg\">11:13 AM PDT</span>&nbsp;Some instances in a single Availability Zone are currently impaired or have lost power in the EU-WEST-2 Region. The root cause is a thermal event within a data center in the affected Availability Zone that we are working to resolve. Some instances may also be experiencing network connectivity issues in the affected Availability Zone. Elastic Load Balancing has shifted traffic away from the Availability Zone. All multi-AZ databases have failed away from the affected Availability Zone, however single-AZ databases will remain affected until we see full recovery. We do not yet have a an ETA for full recovery but expect it to be more than an hour. We will provide further guidance as soon as we have it. For customers that are able to fail away from the affected Availability Zone, we recommend doing so.</div><div><span class=\"yellowfg\">11:55 AM PDT</span>&nbsp;We continue to investigate instance impairments to a single Availability Zone in the EU-WEST-2 Region. We have experienced an increase in temperatures within a single data center in the affected Availability Zone, which in some cases has caused impairments for instances in the Availability Zone. We have engineers within the affected data center and are working to resolve the issue. ELB has shifted traffic away from the affected Availability Zone, and API Gateway has mitigated the majority of impact and continues to work on the remaining endpoints. Elastic File System (EFS) is experiencing errors within the affected Availability Zone. RedShift, OpenSearch, and ElastiCache are experiencing error rates for clusters within the affected Availability Zone. RDS has successfully mitigated impact for all multi-AZ databases, however single-AZ databases will remain affected until we see recovery. Lambda is largely unaffected by the event, however a very small number of functions may be experiencing invocation errors within the affected Availability Zone. The EC2 APIs are experiencing increased error rates within the affected Availability Zone, but instance launches continue to work on other Availability Zones. Some services, like EMR and Fargate, are seeing delays in provisioning new instances in the affected Availability Zone due to the EC2 API impact in that Availability Zone. Our engineering teams continue to work towards identifying the root cause of the thermal event and resolving it. At this stage, we do not have an ETA but still expect it to be more than an hour. We continue to recommend using other Availability Zones in the EU-WEST-2 Region, which remain unaffected by this event.</div><div><span class=\"yellowfg\">12:25 PM PDT</span>&nbsp;We have resolved the root cause of the thermal event and are starting to see recovery for impaired EC2 instances within the EU-WEST-2 Region. At this stage, the vast majority of EC2 instances have recovered and we continue to work on the instances that are still affected. ELB and API Gateway have shifted traffic away from the affected Availability Zone. EFS is only experiencing error rates for one zone filesystems within the affected Availability Zone; standard filesystems, which use multiple Availability Zones, are not affected. Customers should now be seeing recovery for instance impairments, and we expect to see recovery for the vast majority of instances within the next hour. We will continue to provide updates as recovery continues.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;We continue to make progress in resolving the EC2 impaired instances in a single Availability Zone in the EU-WEST-2 Region. At this stage, the vast majority of affected instances have recovered and we continue to work towards full recovery. ELB and API Gateway remain weighted away from the affected Availability Zone for now. EFS has recovered the availability of all One Zone file systems in the affected Availability Zone. RDS multi-AZ databases remain available and we are starting to see recovery for single-AZ databases within the affected Availability Zone. EKS pods within the affected Availability Zone are starting to see recovery, however EKS did experience failures during cluster creation during the event. RedShift, OpenSearch, and ElastiCache are starting to see recovery within the affected Availability Zone. EC2 APIs have fully recovered and new instances can once again be launched in the affected Availability Zone. Customers should be seeing most of their affected instances in recovery, although we continue to work on a small number of affected EC2 instances and EBS volumes that are still impaired. We will continue to provide updates as recovery progresses.</div><div><span class=\"yellowfg\"> 1:53 PM PDT</span>&nbsp;We have resolved the impairments for the vast majority of EC2 instances in the affected Availability Zone (euw2-az1) in the EU-WEST-2 Region. There are a small number of EC2 instances and EBS volumes that were hosted on hardware that have been affected by the loss of power during the thermal event. For these EC2 instances and EBS volumes, we will be opening Personal Health Dashboard notices to track recovery. We have seen full recovery for a number of AWS services, including AWS Transit Gateway, Amazon Connect, Amazon Relational Database Service, Amazon ElastiCache, Amazon Elastic Container Service, Amazon Elastic File System, Amazon, Elastic Kubernetes Service, Amazon Elastic MapReduce, Amazon OpenSearch Service, and Amazon Redshift. The remaining AWS services, including Amazon API Gateway, Amazon CloudFront, Amazon Elastic Load Balancing are very close to full recovery at this stage. Elastic Load Balancing and API Gateway will be shifting traffic back into the affected Availability Zone shortly. We’re working through the remaining EC2 instances and EBS volumes and expect to see full recovery in the next 30 minutes.</div><div><span class=\"yellowfg\"> 2:07 PM PDT</span>&nbsp;Starting at 10:25 AM PDT, some EC2 instances and EBS volumes were impaired in a single Availability Zone (euw2-az1) in the EU-WEST-2 Region. The issue was caused by a thermal event, which caused some of instances to lose power and some EBS volumes to experience degraded IO performance. Recovery started at 12:01 PM PDT and by 12:12 PM PDT, the vast majority of EC2 instances and EBS volumes had fully recovered. Other AWS services also experienced impact during the event, including API Gateway, Elastic Load Balancing, Database Migration Service, Transit Gateway, CloudFront, Connect, ElastiCache, Elastic Container Service, Elastic File System, Elastic Kubernetes Service, Elastic MapReduce, OpenSearch, Redshift, and Relational Database Service. These services are all now operating normally. There are a small number of EC2 instances and EBS volumes that were hosted on hardware that has been affected by the loss of power during the thermal event. For these EC2 instances and EBS volumes, we will be opening Personal Health Dashboard notices to track recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased AWS SSO Management Console Error Rates",
      "date": "1658156924",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:08 AM PDT</span>&nbsp;We are investigating increased error rates in the AWS SSO management console in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:49 AM PDT</span>&nbsp;We can confirm an issue with the AWS Single Sign-On (SSO) service, which is affecting customers who are using an external identity provider that is enabled in the US-EAST-1 Region.  Affected customers experiencing issues when attempting SSO authentication for the AWS console and services.  We are actively working towards identifying the root cause.</div><div><span class=\"yellowfg\"> 9:10 AM PDT</span>&nbsp;We are seeing recovery of this issue since 8:41 AM PDT, and all AWS console and API requests authenticated with an external SSO provider in the US-EAST-1 Region should be working properly now. We are continuing to monitor systems before confirming full recovery.</div><div><span class=\"yellowfg\"> 9:43 AM PDT</span>&nbsp;Between 7:34 AM and 8:41 AM PDT AWS Single Sign-On (SSO) customers using an external identity provider that is enabled in the US-EAST-1 Region experienced issues when attempting SSO authentication for the AWS console and services.  The issue has been identified and mitigated, and all SSO authentication requests are working properly now.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1658178914",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:15 PM PDT</span>&nbsp;We are investigating increased error rates for SSO in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:44 PM PDT</span>&nbsp;We can confirm an issue that is preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available. Existing sessions are not affected by this issue, but will experience difficultly re-authenticating should the existing session expire. This issue is affecting AWS services that use sign-in authentication, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. This issue does not affect authentication using IAM users or roles. We are working to resolve the issue and will provide you with regular updates.</div><div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;We continue to work on resolving the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. We have identified the root cause of the issue, which is within the subsystem responsible for hosting the authentication page. Customers attempting to establish a new session or re-authenticate an expired session may experience increased error rates (503/504) when attempting to access the authentication page. In some cases, retries will succeed and allow for the session to be re-authenticated. Engineers are working to resolve the issue, but we do not yet have an ETA for resolution. We are working on mitigations now, and will provide a crisper ETA in the next update. The issue continues to affect AWS services using sign-in authentication, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. Note that should these services be configured to use other authentication types, such as IAM or SAML, they will not be affected by this issue. This issue does not affect authentication using IAM users or roles. We are working to resolve the issue and will provide you with regular updates.</div><div><span class=\"yellowfg\"> 3:50 PM PDT</span>&nbsp;We continue to work on resolving the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. Over the last 30 minutes we have made progress in addressing the issue and error rates with the authentication page have begun to improve. In many cases, retries are now working, allowing for authentication to be completed. While we have seen an improvement in error rates, they have not yet returned to normal levels, so engineers remain engaged and working to fully resolve the issue. We expect error rates to continue to drop over the next hour, but are still not certain on an exact ETA for full resolution of the event.</div><div><span class=\"yellowfg\"> 4:37 PM PDT</span>&nbsp;We have seen error rates improve substantially for the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. In several cases, customers are seeing recovery and have been able to authenticate their sign-in requests. We continue to work on the remaining error rates, which continue to cause some issues with the access to the authentication page. At this stage, retrying requests should allow for authentication requests to succeed. This issue does not affect authentication using IAM users or roles. We will continue to provide updates as we work to fully resolve the issue.</div><div><span class=\"yellowfg\"> 4:58 PM PDT</span>&nbsp;We have resolved the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page was not available in the US-EAST-1 Region. Beginning at 12:35 PM PDT, customers experienced errors when attempting to authenticate using the sign-in page in the US-EAST-1 Region. Engineers worked to resolve the issue, and at 3:23 PM PDT error rates began to improve, allowing customers to authenticate their sign-in requests. At 3:41 PM PDT, error rates had returned to normal levels and the authentication page was once again available and serving sign-in requests from affected AWS services, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. Between 4:12 PM and 4:38 PM PDT, we experienced an increase in error rates once again, but this has also been resolved. This issue did not affect authentication using IAM users or roles, or SAML. The issue has been resolved and all services are operating normally.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Secrets Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Errors and Latencies",
      "date": "1658948667",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:04 PM PDT</span>&nbsp;We are investigating increased API connection timeout errors and latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:33 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for Secrets Manager APIs in the US-EAST-1 Region. We are working to identify root cause and resolve the issue. \n\nCustomers using Secrets Manager to store secrets may experience errors when using the Secrets Manager APIs, including customers invoking Secrets Manager from Lambda functions. \n\nLambda functions that are not calling Secrets Manager APIs are not affected by this event. We continue to work to understand the issue and will continue to provide updates. </div><div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;We have resolved the issue causing increased error rates and latencies for the Secrets Manager APIs in the US-EAST-1 Region. Customers using Secrets Manager to store secrets may have experienced errors when using the Secrets Manager APIs, including customers invoking Secrets Manager from Lambda functions. Lambda functions that are not using Secrets Manager were not affected by this event. The issue was resolved at 12:30 PM PDT, when error rates and latencies returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "secretsmanager-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Instance Impairments",
      "date": "1659028265",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:11 AM PDT</span>&nbsp;We are investigating network connectivity issues for some instances and increased error rates and latencies for the EC2 APIs within the US-EAST-2 Region.</div><div><span class=\"yellowfg\">10:25 AM PDT</span>&nbsp;We can confirm that some instances within a single Availability Zone (USE2-AZ1) in the US-EAST-2 Region have experienced a loss of power. The loss of power is affecting part of a single data center within the affected Availability Zone. Power has been restored to the affected facility and at this stage the majority of the affected EC2 instances have recovered. We expect to recover the vast majority of EC2 instances within the next hour. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">10:49 AM PDT</span>&nbsp;We continue to see recovery of EC2 instances that were affected by the loss of power in a single Availability Zone in the US-EAST-2 Region. At this stage, the vast majority of affected EC2 instances and EBS volumes have returned to a healthy state and we continue to work on the remaining EC2 instances and EBS volumes. Elastic Load Balancing has shifted traffic away from the affected Availability Zone. Single-AZ RDS databases were also affected and will recover as the underlying EC2 instance recovers. Multi-AZ RDS databases would have mitigated impact by failing away from the affected Availability Zone. While the vast majority of Lambda functions continue operating normally, some functions are experiencing invocation failures and latencies, but we expect this to improve over the next 30 minutes. Power has been restored to all affected resources and remains stable. We expect the recovery of EC2 instances and EBS volumes to continue to improve over the next 45 minutes. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">11:25 AM PDT</span>&nbsp;We continue to make progress in recovering the remaining EC2 instances and EBS volumes affected by the loss of power in a single Availability Zone in the US-EAST-2 Region. The vast majority of EC2 instances are now healthy, but we continue to work on recovering the remaining EBS volumes affected by the issue. EC2 API error rates and latencies have returned to normal levels. Elastic Load Balancing remains weighted away from the affected Availability Zone. Error rates and latencies for Lambda function invocations have now returned to normal levels. Power has been restored to all affected resources and remains stable. We expect the recovery of EC2 instances and EBS volumes to continue to improve over the next 30 minutes. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">11:45 AM PDT</span>&nbsp;At this stage, the vast majority of EC2 instances and EBS volumes, affected by the loss of power in a single Availability Zone in the US-EAST-2 Region, have fully recovered. A small number of EC2 instances and EBS volumes are on hardware that was adversely affected by the loss of power. Engineers continue to work on recovering the EC2 instances and EBS volumes on this hardware and will provide updates via the Personal Health Dashboard if any of these could not be recovered. Elastic Load Balancers affected by the issue have recovered and traffic has been shifted back into the affected Availability Zone. The vast majority of single-AZ databases have also recovered and the remaining databases are running on hardware that was affected by the event. We will provide updates via the Personal Health Dashboard if any of these databases can not be recovered. At this stage, if your EC2 instance or EBS volumes that has still not recovered, attempting a reboot of the EC2 instance could resolve the issue. If not, we recommend relaunching the affected EC2 instance or recreating the affected EBS volume.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;Starting at 9:57 AM PDT some EC2 instances and EBS volumes experienced a loss of power within a single Availability Zone in the US-EAST-2 Region. Power was restored at 10:19 AM PDT and EC2 instances and EBS volumes began to recover. By 10:23 AM PDT, the vast majority of EC2 instances and EBS volumes has fully recovered and by 11:37 AM PDT, all but a very small number of EC2 instances and EBS volumes had recovered. Elastic Load Balancing shifted traffic away from the affected Availability Zone, which has now been shifted back. RDS impact for single-AZ databases, which have also been recovered. Other services (tagged below) saw impact during the event, but most have fully recovered. Those that are still seeing impact, will provide updates via the Personal Health Dashboard as they work towards full recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS CloudTrail (N. Virginia)",
      "summary": "[RESOLVED] CloudTrail event delivery delays",
      "date": "1660680453",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:07 PM PDT</span>&nbsp;We are investigating increased latency of CloudTrail events causing CloudTrail events processing delays in the US-EAST-1 Region starting at 9:33 AM PDT. CloudTrail customers in US-EAST-1 Region will receive events with delay as high as 4 hours.  All new events after 12:57 PM PDT will be processed immediately. ETA for backlog consumption is 3:30 PM PDT.</div><div><span class=\"yellowfg\"> 2:10 PM PDT</span>&nbsp;Between 09:33 AM and 12:57 PM PDT Increased latency of CloudTrail events in the US-EAST-1 Region causing CloudTrail events processing delays. We have resolved the issue all backlog events are in the process of backfilling will be completed by 3:30 PM PDT. The service is operating normally. </div>",
      "service": "cloudtrail-us-east-1"
    },
    {
      "service_name": "Multiple services (Oregon)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1661361799",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies affecting some services within the US-WEST-2 Region. We are working to identify the root cause.</div><div><span class=\"yellowfg\">10:34 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies affecting some services within the US-WEST-2 Region. The root cause appears to be increased error rates and latencies for API Gateway endpoints within the US-WEST-2 Region. Customers with API Gateway endpoints would be experiencing increased error rates and latencies for their requests. Customers calling Lambda via an API Gateway, would also be experiencing increased error rates and latencies for their function invocations. Other AWS services that use API Gateway - for example Batch, EKS, and EventBridge - are also experiencing increased error rates and latencies. Amazon Connect is experiencing increased call failure as well as issues with user login. We have identified the root cause and are working on a mitigation to resolve the issue.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;We have addressed the root cause of the issue causing increased API error rates and latencies in the US-WEST-2 Region and are seeing recovery across affected API Gateway endpoints and AWS services.</div><div><span class=\"yellowfg\">10:53 AM PDT</span>&nbsp;Between 10:00 AM and 10:34 AM PDT we experienced increased API error rates and latencies affecting some services within the US-WEST-2 Region. The root cause of the event was increased error rates and latencies for API Gateway endpoints within the US-WEST-2 Region. Customers calling Lambda functions via an API Gateway, also experienced increased error rates and latencies for their function invocations. Other AWS services that use API Gateway - for example Batch, EKS, and EventBridge - also experienced increased error rates and latencies. Amazon Connect saw increased call failures as well as issues with user logins. We have resolved the root cause of the event and all affected API Gateway endpoints, Lambda functions, and AWS services have fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Amazon Elastic Container Service - Increased Rates of Insufficient Capacity Errors",
      "date": "1661380251",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:30 PM PDT</span>&nbsp;We can confirm increased insufficient capacity error rates for launching new ECS tasks using the Fargate launch type, starting at 1:15 PM PDT. Running tasks and tasks that utilize the EC2 launch type are not impacted. We continue to work through full resolution and will provide another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 4:06 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work through full resolution of the issue. We expect you to begin observing signs of recovery beginning at 4:20 PM PDT. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue. This is taking longer than expected, and we now expect it to be 5:00 PM PDT before you will begin to observe signs of recovery. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:20 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue, however we are experiencing some delays with full recovery. You will still see some task launches succeeding during this event. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:49 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue, however we are experiencing some delays with full recovery. We are working multiple, parallel paths to make additional capacity available. You will still see some task launches succeeding during this event. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 6:49 PM PDT</span>&nbsp;We have identified the cause of the decreased capacity and understand why the Fargate task launch success rate is only 70% at this point. We are working on multiple parallel actions to address the underlying issues and have identified one area in particular that should help us make faster progress towards recovery. We have started work on this and will have an indication of progress by 7:00 PM PDT. Once we have that progress data we will be able to provide an ETA for recovery. We are also making a change to the rate at which ECS launches tasks to reduce load on Fargate and to speed up recovery.\nFor customers with prepared and rehearsed plans for moving to a different region, they should consider exercising those if they are in a place to do so. Customers can also consider using EC2 with ECS and EKS as a mitigation option since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 7:45 PM PDT</span>&nbsp;We have identified the cause of the decreased capacity and understand why the Fargate task launch success rate is only 70% at this point. Our remediation actions are making slower progress than expected, so we are working on additional actions to further reduce load on Fargate. The work started in the previous update is still progressing but we do not yet have a projected ETA for when it will complete or when we will see recovery.\n\nCustomers can switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 8:46 PM PDT</span>&nbsp;The current state is that more than 50% of Fargate task launches in the US-EAST-1 region are succeeding. For the tasks that are failing, we have identified that this is due to a large amount of compute capacity, managed by Fargate, that is in a stuck state. We call these leaked instances. This results in customer tasks and pods not being able to be started, impacting both ECS on Fargate and EKS on Fargate. \n\nWe are driving multiple parallel efforts to address this. First, we're taking action to make sure no additional instances are leaked by reducing the call rates to Fargate, and second we are working to free up these leaked instances so they can be used to run customer tasks. \n\nAs stated in previous updates, the recovery actions are making slower progress than we expected which is preventing us from providing an ETA for recovery at this point. Right now we expect multiple hours before we see recovery. \n\nCustomers who already have Fargate tasks or pods running are recommended to not scale down until we have recovery. Customers can switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 9:19 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances, and we are already seeing faster progress towards this.\n\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will steadily ramp up Fargate task and pod launches and enable normal operations. We don't yet have an ETA, but will communicate one as soon as we have an ETA we feel confident about.\n\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 9:55 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We estimate that we have released 50% of the leaked instances and at the current rate that all leaked instances will have been released by 12:00 AM PDT. \n\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will steadily ramp up Fargate task and pod launches and enable normal operations. We expect to be able to communicate an ETA for recovery soon after we complete releasing all leaked instances. Once this happens, our best estimate is an additional 1 to 2 hours for recovery. \n\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">10:26 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We estimate that we have released 93% of the leaked instances and at the current rate that all leaked instances will have been released by 10:45 PM PDT.\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will slowly ramp up Fargate task and pod launches and enable normal operations. We expect to be able to communicate an ETA for recovery soon after we complete releasing all leaked instances. Once this happens, our best estimate is an additional 1 to 2 hours for recovery.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">11:42 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We have now released effectively all leaked instances and are now starting the process to re-enable Fargate task launches.\nWe will start by enabling Fargate task launches for a small number of accounts and then increase that as we see success. As the pace of recovery is dependent on how fast we increase task launches, it is still too early to provide an ETA for full recovery, but we still expect hours before we are fully recovered.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25, 12:46 AM PDT</span>&nbsp;We have started to enable Fargate task launches in the US-EAST-1 Region again. We are seeing successful task launches and continue to slowly increase the number of tasks being launched. Most customers will still see their task launches failing until we see enough progress to broadly enable task launches. We expect to have enough data to further increase task launches by 1:10 AM PDT.\nThe progress at that point will help inform ETA for recovery, we still estimate this to be hours out.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  1:55 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. At this point, all accounts can launch tasks, but at a lower task launch rate than usual. The lower than normal task launch rates means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. The impact of this is that scaling of services and deployments will take longer than usual. We will incrementally raise task launch rates back to normal levels as we monitor service recovery.\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 4:00 AM PDT.\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  3:08 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. We have just completed a change that increases the task launch rate for tasks running as part of an ECS service. This will reduce the time required both for service deployments and scaling up services. As a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. We will continue to incrementally raise the task launch rate back to normal levels as we monitor service recovery.\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 5:00 AM PDT.\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:03 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. In addition to the earlier change to increase the task launch rate for tasks running as part of an ECS service, we have also increased the task launch rate for the RunTask API from 1 task per second to 2 tasks per second. As a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. We will continue to incrementally raise the task launch rate back to normal levels as we monitor service recovery.\n\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 5:00 AM PDT.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:34 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. The task launch rate for the ECS RunTask API has now been increased from 2 tasks per second to 5 tasks per second and we are seeing a corresponding reduction in task launch failures due to customers exceeding the maximum task launch rate. We are continuing to monitor recovery and will keep incrementally raising the task launch rate until we return to the default of 20 tasks per second.\n\nAs a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed.\n\nWe are no longer seeing elevated task launch failures. It is, however likely to be past 5:00 AM PDT before we have returned to the default task launch rate of 20 tasks per second.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:57 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. All customers now have a task launch rate for the ECS RunTask API of 10 tasks per second and we see further reduction in task launch failures due to customers exceeding the maximum task launch rate. We are continuing to monitor recovery and will keep incrementally raising the task launch rate until we return to the default of 20 tasks per second.\n\nAs a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed.\n\nWe are no longer seeing elevated task launch failures. It is, however likely to be past 5:00 AM PDT before we have returned to the default task launch rate of 20 tasks per second.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  6:05 AM PDT</span>&nbsp;Between August 24 1:07 PM and August 25 6:00 AM PDT, ECS and EKS customers using Fargate experienced increased task and pod launch failure rates. The issue has been resolved and customers are now able to launch tasks and pods normally again. A very small number of customers may still see failing task and pod launches and we will provide updates to those customers using account specific events. ECS and EKS customers using EC2 were not impacted during this event and there was also no impact to any already running tasks or pods on Fargate.</div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "AWS Systems Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latency",
      "date": "1662737760",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:36 AM PDT</span>&nbsp;We are investigating increased API error rates and latency in the US-EAST-1 Region. This will impact customers' ability to make calls to Parameter Store.</div><div><span class=\"yellowfg\"> 9:29 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of data may be affected by the event as they are unable to retrieve the data. We have identified the subsystem where the errors are occurring and are working to resolve the issue. At this stage, we expect recovery to take more than an hour but will keep you updated on our progress.</div><div><span class=\"yellowfg\"> 9:50 AM PDT</span>&nbsp;We continue to work on the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configurations may be affected by the event as they are unable to retrieve the data. Customers invoking Lambda functions that use Environment Variables will also experience increased error rates and latencies. The subsystem responsible for the Parameter Store is experiencing resource contention, which is leading to the increased error rates and latencies. We continue to work toward the resolving the issue, but do not have an ETA on recovery at this stage. We will keep you updated on our progress.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;We continue to work on the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration may be affected by the event as they are unable to retrieve the data. We have confirmed that customers using Lambda Environment Variables are not affected by this event, but rather customers that are attempting to access the Parameter Store directly from a Lambda function. Customers can work around the issue by removing the use of the Parameter Store from their code, or look to use some other store, such as DynamoDB. We are making progress towards resolution but continue to see elevated error rates for the affected subsystem. We do not have an ETA on recovery at this stage and will keep you updated on our progress.</div><div><span class=\"yellowfg\">11:02 AM PDT</span>&nbsp;We are seeing recovery for the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration should be seeing recovery at this stage. We continue to monitor the subsystem that was experiencing resource contention, but expect error rates and latencies to remain at normal levels.</div><div><span class=\"yellowfg\">11:21 AM PDT</span>&nbsp;Starting at 7:34 AM PDT, we experienced increased error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration were affected by the event as they were unable to retrieve the data. Customers using Parameter Store from Lambda functions also experienced increased failure rates. The issue was resolved at 10:45 AM PDT, when error rates and latencies returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2systemsmanager-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Cape Town)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1663153260",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:01 AM PDT</span>&nbsp;We are investigating increased API error rates in the AF-SOUTH-1 Region. Existing instances are unaffected.</div><div><span class=\"yellowfg\"> 4:21 AM PDT</span>&nbsp;We are experiencing elevated API errors and latency with the IAM authentication service in the AF-SOUTH-1 Region.  This is causing impact on services that require IAM authentication.  We are actively investigating root cause to identify a path to mitigation.</div><div><span class=\"yellowfg\"> 5:13 AM PDT</span>&nbsp;We continue to see elevated API and latency for IAM authentication and authorization requests in the AF-SOUTH-1 Region. The IAM errors affect any AWS API call that requires authentication, though some calls may still be succeeding due to API credential caching. We have all teams engaged in investigation of root cause and mitigation, but we do not have an ETA for recovery at this time.</div><div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;We are starting to see recovery of IAM authentication and authorization requests, and customers should be seeing improvements in API error rates in the AF-SOUTH-1 Region.  We are continuing to monitor and work towards full recovery.</div><div><span class=\"yellowfg\"> 5:39 AM PDT</span>&nbsp;We are continuing to see improvement in IAM authentication and authorization API error rates, and a subsequent improvement in AWS service API availability in the AF-SOUTH-1 Region. We continue to monitor this event closely, and will do so until all services have fully recovered.</div><div><span class=\"yellowfg\"> 6:22 AM PDT</span>&nbsp;All service APIs are now operating correctly, and we continue to work towards full recovery of services in the AWS Management Console. We’re continuing to restore normal operations to all AWS consoles in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 6:54 AM PDT</span>&nbsp;Between 3:30 AM and 6:35 AM PDT, customers experienced elevated errors for API and console requests in the AF-SOUTH-1 Region due to an issue with IAM authentication and authorization, which caused elevated error rates for many AWS services in the AF-SOUTH-1 Region.  This issue has been resolved and all services are now working normally.</div>",
      "service": "ec2-af-south-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased Route 53 Health Check API latency",
      "date": "1663585076",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:57 AM PDT</span>&nbsp;We are experiencing delays creating, deleting, and updating Route 53 health checks.  This is also affecting provisioning and scaling workflows for services like ELB, EKS, and RDS that use Route 53 health checks as well.  We have identified the issue and have a path towards recovery, however we do expect that to take at least a couple hours.\n\nExisting Route 53 health checks are operating normally, and are correctly reflecting the health status of their configured resources. \n</div><div><span class=\"yellowfg\"> 4:32 AM PDT</span>&nbsp;We continue to work towards recovery of the Route 53 Health Check API, and expect health check create, update, and delete delays to be resolved within about 90 minutes.  However, it will take a bit longer for other impacted services to fully recover their own provisioning and scaling workflows as they have their own backlogs to work through.</div><div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;The Route 53 health check API has recovered, and all operations are now being processed normally.  Other services that were impacted by this issue are processing their scaling and provisioning backlogs.</div><div><span class=\"yellowfg\"> 6:25 AM PDT</span>&nbsp;Between 12:00 AM and 5:05 AM PDT, customers experienced elevated errors and latency when calling the Route 53 Health Checks API.  This issue also affected provisioning and scaling workflows for some other AWS services.  The Route 53 Health Checks API is now working normally.  ELB, EKS, EMR, and Amazon MQ are still processing backlogs, and affected customers can get more information in Account Specific Service Events.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Mumbai)",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1663816639",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased API error rates for DetachVolume, AttachVolume, and AssociateIamInstanceProfile in the AP-SOUTH-1 Region. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 8:35 PM PDT</span>&nbsp;Between 6:26 PM and 8:23 PM PDT we experienced increased API error rates for DetachVolume, AttachVolume, and AssociateIamInstanceProfile in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-south-1"
    },
    {
      "service_name": "AWS Systems Manager (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1664323892",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;We are experiencing increased latency and error rates when calling the StartSession API for a subset of customers in the US-WEST-2 Region. We have identified the issue and working towards resolution.</div><div><span class=\"yellowfg\"> 5:50 PM PDT</span>&nbsp;Between 2:40 PM and 5:45 PM PDT, we experienced increased latency and error rates when calling the StartSession API for a subset of customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2systemsmanager-us-west-2"
    },
    {
      "service_name": "Amazon API Gateway (Oregon)",
      "summary": "[RESOLVED] Increased Invoke Error Rates",
      "date": "1664385180",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:13 AM PDT</span>&nbsp;We are investigating increased error rates for invokes in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;We are investigating increased error rates for invokes in the US-WEST-2 Region. We do not yet have a root cause, but are investigating multiple potential root causes in parallel. In addition, we are implementing filters on inbound traffic from a set of sources with recent significant traffic shifts, which may help mitigate the impact.  We do not yet have a solid ETA, but will continue to provide updates as we progress.</div><div><span class=\"yellowfg\">10:59 AM PDT</span>&nbsp;We continue to see elevated error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. While engineers continue to work towards root cause, we have deployed traffic filters from sources with significant increases in traffic prior to the event. As a result of these traffic filters, we are seeing a reduction in error rates and latencies, but continue to work towards full recovery. Although error rates are improving, we do not yet have an ETA for full recovery. The issue is also affecting API requests to some AWS services, including those listed below. Amazon Connect is experiencing increased failures in handling new calls, chats, and tasks as well as issues with user login in the US-WEST-2 Region. We will continue to provide updates as we progress.</div><div><span class=\"yellowfg\">11:33 AM PDT</span>&nbsp;We continue to work on resolving the elevated error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. We continue to see a significant improvement in error rates, starting at 10:40 AM PDT, but are not seeing full recovery yet. The issue is caused by contention within the subsystem that is responsible for request processing within the API Gateway service. Engineers are engaged and have applied traffic filters as a precautionary measure, while they work to identify the root cause and resolve the issue. Engineers continue to work to reduce contention within the affected subsystem, which we believe will resolve the elevated error rates and latencies. Customers with applications that use API Gateway, or customers invoking Lambda functions via API Gateway, will be experiencing elevated error rates and latencies as a result of this issue. The AWS services listed below are also experiencing elevated error rates as a result of this issue. While we have seen improvements in error rates since 10:40 AM PDT, recovery has stalled and we do not have a clear ETA on full recovery. For customers that have dependencies on API Gateway and are experiencing error rates, we do not have any mitigations to recommend to address the issue on the customer side. We do expect error rates to continue to improve as contention with the affected subsystem resides, and will provide further updates as recovery progresses. </div><div><span class=\"yellowfg\">12:26 PM PDT</span>&nbsp;We continue to see an improvement in error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region, but have not fully resolved the issue. While our mitigations have improved error rates and latencies, we have also identified the root cause of the event. The subsystem responsible for request processing experienced increased load, which ultimately led to contention of a component within the affected subsystem. Engineers have been working to resolve the contention of the affected component, which has led to a reduction of error rates and latencies. The path to full recovery involves addressing the contention across the subsystem, which we are currently doing. As that progresses over the next two hours, we expect recovery to continue to improve. Customers with applications that use API Gateway will be experiencing elevated error rates and latencies as a result of this issue. Lambda is not affected by this event, but customers using API Gateway as an HTTP endpoint for Lambda will experience increased error rates and latencies. Other AWS services listed below are also experiencing elevated error rates as a result of this issue. For customers that have dependencies on API Gateway and are experiencing error rates, we do not have any mitigations to recommend to address the issue on the customer side. We do expect error rates to continue to improve as contention with the affected subsystem resides, and will provide further updates as recovery progresses. </div><div><span class=\"yellowfg\"> 1:03 PM PDT</span>&nbsp;Error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region, continue to hold steady. Engineers continue to work on resolving the contention affecting the subsystem responsible for request processing. We recently completed a mitigation that should help to reduce error rates and latencies to normal levels and will have further updates on the result of that change in the next update. Although Lambda function invocations are not affected by this issue, the Lambda console is experiencing some error rates which we are investigating. Other AWS services affected by this issue remain in much the same state, waiting on the recovery of API Gateway.</div><div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;Starting at 1:12 PM PDT, we saw a further reduction in error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. This was a result of the latest mitigation which addressed contention within the component in the subsystem responsible for request processing within API Gateway. Error rates are now at levels where some customers may begin to see recovery, and retries will begin to work more consistently. We will be applying the mitigation to the remaining hosts affected by the contention issue and expect to see further recovery from them in the next 30 minutes.</div><div><span class=\"yellowfg\"> 1:42 PM PDT</span>&nbsp;Starting at 1:31 PM PDT, error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region are now close to pre-event levels, and we continue to work on the remaining hosts that are affected by the contention issue. Several AWS services, including AWS Connect and Lambda are seeing signs of strong recovery. We expect all services to recover as API Gateway error rates and latencies return to normal levels. Customers should be seeing recovery at these error levels as well. We will continue to provide updates until the error rates and latencies have returned to normal levels.</div><div><span class=\"yellowfg\"> 2:05 PM PDT</span>&nbsp;As of 1:43 PM PDT, error rates and latencies for invokes for API Gateway endpoints in the US-WEST-2 Region are now at normal levels. The issue began at 9:20 AM PDT when error rates and latencies for API Gateway began to increase. Error rates began to improve at 10:38 AM PDT, when engineers took action to reduce contention within the subsystem that handles request processing for API Gateway. Error rates continued to improve until 1:10 PM PDT, when engineers applied a mitigation to resolve the contention within the affected subsystem. These actions accelerated recovery, and by 1:43 PM PDT, error rates and latencies had returned to normal levels. Affected AWS services have now recovered as well. The issue has been resolved and the service is operating normally.</div>",
      "service": "apigateway-us-west-2"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased API errors",
      "date": "1664397551",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:39 PM PDT</span>&nbsp;We are experiencing elevated errors for calls to the Redshift API and console.  We have identified the root cause of these errors, and are in the process of applying a mitigation to address the issue.  While we work to apply the mitigation, customers are encouraged to re-attempt failed API queries, as they are likely to succeed after multiple attempts.  We do not have a firm ETA for recovery at this time, but will report on the results of the mitigation when it has been applied in 1 hour.</div><div><span class=\"yellowfg\"> 2:42 PM PDT</span>&nbsp;Between 6:15 AM and 2:02 PM PDT, we experienced increased latency and error rates in US-EAST-1 Region impacting customers using Query Editor v2 and the Redshift Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Multiple services (Sao Paulo)",
      "summary": "[RESOLVED] Increased Error rates and Latencies",
      "date": "1664457641",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:20 AM PDT</span>&nbsp;We are investigating increased error rates and latencies affecting multiple services and the AWS Console in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:39 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for some services within the SA-EAST-1 Region. The AWS Management Console is also experiencing elevated error rates as a result of this issue. The root cause of the issue appears to be elevated network packet loss which is affecting some AWS service APIs. The EC2 network, and existing EC2 instances, are not affected by this event. Some services have seen improvements in error rates as a result of mitigations taken by engineers, but we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 6:55 AM PDT</span>&nbsp;Between 5:56 AM and 6:46 AM PDT, we experienced increased error rates and latencies affecting multiple services and the AWS Management Console in the SA-EAST-1 Region. The EC2 network, and existing EC2 instances, were not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-sa-east-1"
    },
    {
      "service_name": "Amazon Managed Grafana (Oregon)",
      "summary": "[RESOLVED] Authentication Issue",
      "date": "1664954146",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:15 AM PDT</span>&nbsp;We are investigating authentication issues affecting workspace availability in the US-WEST-2 Region</div><div><span class=\"yellowfg\">12:50 AM PDT</span>&nbsp;We have identified the cause of the authentication issue affecting some workspaces in the US-WEST-2 Region and continue to work toward resolution. </div><div><span class=\"yellowfg\"> 1:30 AM PDT</span>&nbsp;Between October 4 11:00 PM and October 5 1:07 AM PDT, we experienced authentication issue affecting some workspaces in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "grafana-us-west-2"
    },
    {
      "service_name": "Multiple services (UAE)",
      "summary": "[RESOLVED] Elevated API Error rates",
      "date": "1665092370",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;We are investigating elevated error rates for some services in the UAE (ME-CENTRAL-1) Region including S3, Lambda, and CloudTrail.</div><div><span class=\"yellowfg\"> 3:24 PM PDT</span>&nbsp;Between 2:13 PM PDT and 2:57 PM PDT we experienced increased error rates for Amazon S3 requests in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-me-central-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (UAE)",
      "summary": "[RESOLVED] Elevated API Error rates",
      "date": "1665093648",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:00 PM PDT</span>&nbsp;Beginning at 2:13 PM PDT, we began observing elevated error rates for Amazon S3 PUT and GET APIs in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail.</div><div><span class=\"yellowfg\"> 3:03 PM PDT</span>&nbsp;We are seeing recovery in the elevated error rates for S3 and other AWS services. We are continuing to monitor progress towards full recovery.</div><div><span class=\"yellowfg\"> 3:24 PM PDT</span>&nbsp;Between 2:13 PM PDT and 2:57 PM PDT we experienced increased error rates for Amazon S3 requests in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-me-central-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased API error rates and latency ",
      "date": "1665511560",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:06 AM PDT</span>&nbsp;We are investigating increased API error rates and latency in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">11:16 AM PDT</span>&nbsp;Between 10:33 AM and 11:08 AM PDT, customers may have experienced elevated latency and errors when accessing AWS APIs and consoles in the US-GOV-WEST-1 Region.  We have identified the root cause, which has been addressed, and all services are now operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Route 53 (US-West)",
      "summary": "[RESOLVED] Increased API error rates and latency ",
      "date": "1665513240",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:34 AM PDT</span>&nbsp;Between 10:33 AM and 11:08 AM PDT, customers may have experienced elevated latency and errors when accessing AWS APIs and consoles in the US-GOV-WEST-1 Region. We have identified the root cause, which has been addressed, and all services are now operating normally.  This issue also affected the following services: EC2, Redshift, CloudWatch and OpenSearch.</div>",
      "service": "route53-us-gov-west-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "[RESOLVED] Increased latency for UpdateEnvironment API calls",
      "date": "1665603967",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;We are investigating increased latency for UpdateEnvironment API calls. This issue impacts both the API, and requests made through the Management Console. Additionally, attempts to abort the update (via the AbortEnvironmentUpdate API) may return 5XX errors. We are actively working on identifying the root cause and will provide additional information as soon as possible.</div><div><span class=\"yellowfg\"> 1:28 PM PDT</span>&nbsp;We have identified the root cause of increased latency for UpdateEnvironment API calls in the US-EAST-1 Region. This issue impacts both the API, and requests made through the Management Console. Additionally, attempts to abort the update (via the AbortEnvironmentUpdate API) may return a 5XX error. We are observing partial recovery and continue to work through full recovery of the issue. Another update will be sent by 2:00 PM PDT or as more information becomes available.</div><div><span class=\"yellowfg\"> 1:49 PM PDT</span>&nbsp;Between 10:00 AM and 1:45 PM PDT, we experienced increased latency for UpdateEnvironment API calls in the US-EAST-1 Region.  Existing environments were not impacted.  Impact was limited to updating existing or creating new environments. The issue was related to a partially failed deployment that we were able to mitigate. The issue has been resolved and the service is operating normally.  </div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Cognito (Ohio)",
      "summary": "[RESOLVED] Increased error rates for Cognito Hosted UI",
      "date": "1666632939",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:35 AM PDT</span>&nbsp;Between 8:48 AM and 9:52 AM PDT, we experienced increased error rates for Cognito Hosted UI in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cognito-us-east-2"
    },
    {
      "service_name": "Amazon Simple Notification Service (Milan)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1666912855",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:20 PM PDT</span>&nbsp;We are investigating increased error rates for the Amazon SNS Publish API in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 4:32 PM PDT</span>&nbsp;We are beginning to see recovery in SNS Publish API error rates in the EU-SOUTH-1 Region. </div><div><span class=\"yellowfg\"> 4:55 PM PDT</span>&nbsp;Between 2:57 PM and 4:15 PM PDT we experienced elevated Publish API error rates and delivery latency in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-eu-south-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Increased Launch Failures",
      "date": "1667246817",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:06 PM PDT</span>&nbsp;We are investigating increased failures for newly launched instances within the US-EAST-2 Region. Changes to instance networking state, such as mapping Elastic IP addresses, may also experience increased error rates. We have identified root cause and are working to resolve the issue. Existing instances are not affected by this issue.</div><div><span class=\"yellowfg\"> 1:27 PM PDT</span>&nbsp;Between 12:18 PM and 1:20 PM PDT, we experienced increased failures for newly launched instances within the US-EAST-2 Region. Some APIs, such as those used to attach Elastic IP addresses to instances, were also affected by this event. Some AWS services, such as Elastic Load Balancing and Elastic Map Reduce, experienced provisioning delays due to failures in launching new EC2 instances. Existing instances were not affected by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS Transfer Family (N. Virginia)",
      "summary": "[RESOLVED] Increased Authentication Errors",
      "date": "1667939280",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:28 PM PST</span>&nbsp;We can confirm increased authentication errors when logging into SFTP and FTPS. We have identified the root cause and are in the process of mitigating the issue. Our current mitigation efforts are expected to take 90 minutes. We will provide you with another update by 2:00 PM PST.</div><div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;Between 11:10 AM and 12:40 PM PST, we experienced increased authentication errors when logging into SFTP and FTPS. The issue has been resolved and the service is operating normally.</div>",
      "service": "transfer-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Degraded EBS Volume Performance",
      "date": "1669228100",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:28 AM PST</span>&nbsp;Between 7:58 AM and 8:05 AM PST, some EBS volumes experienced degraded IO performance within the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS QuickSight (N. Virginia)",
      "summary": "[RESOLVED] Missing dashboard and other assets",
      "date": "1669654485",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:54 AM PST</span>&nbsp;We are investigating an issue with missing dashboards and other assets in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:49 AM PST</span>&nbsp;We are observing increased latency and timeouts for loading QuickSight assets such as dashboards and analysis in the homepage. We are observing a scaling bottleneck with one of our dependencies for the search functionality and we are currently working on scaling up the search node. In parallel, we are looking into any recent deployments that could have caused this. If customers know the dashboard and analysis URL, customers can navigate directly from those links. Embedding dashboards will not be affected. We will update you with the current status in 30 minutes. </div><div><span class=\"yellowfg\">10:18 AM PST</span>&nbsp;We are starting to see recovery for listing of Quicksight Dashboards and Analysis from the homepage. Customers with direct links to Quicksight Dashboards and Analysis, or Embedded Dashboards were not affected throughout this event.</div><div><span class=\"yellowfg\">11:02 AM PST</span>&nbsp;Between 7:37 AM and 10:28 AM PST, customers may have seen elevated latency and errors listing QuickSight Dashboards and Analyses from the homepage. The issue has been resolved and the service is operating normally. Customers with direct links to QuickSight Dashboards and Analyses, or Embedded Dashboards were not affected throughout this event.</div>",
      "service": "quicksight-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Ohio)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1670271960",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:26 PM PST</span>&nbsp;We are investigating an issue which may be impacting Internet connectivity between some customer networks and the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:51 PM PST</span>&nbsp;We can confirm an issue which is impacting Internet connectivity for the US-EAST-2 Region, and are attempting multiple parallel mitigation paths. Connectivity between instances within the US-EAST-2 Region, in-between AWS Regions, and Direct Connect traffic is not impacted by the event.  Some customers may be experiencing VPN connectivity due to this issue.</div><div><span class=\"yellowfg\">12:59 PM PST</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;Between 11:34 AM and 12:51 PM PST, customers experienced Internet connectivity issues for some networks to and from the US-EAST-2 Region. Connectivity between instances within the Region, in between Regions, and Direct Connect connectivity were not impacted by this issue. The issue has been resolved and connectivity has been fully restored.</div>",
      "service": "internetconnectivity-us-east-2"
    },
    {
      "service_name": "AWS Internet Connectivity (US-East)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1670273106",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:45 PM PST</span>&nbsp;We are investigating an issue which may be impacting Internet connectivity between some customer networks and the US-GOV-EAST-1 Region.</div><div><span class=\"yellowfg\">12:52 PM PST</span>&nbsp;We can confirm an issue which is impacting Internet connectivity for the US-GOV-EAST-1 Region, and are attempting multiple parallel mitigation paths. Connectivity between instances within the US-GOV-EAST-1 Region, in-between AWS Regions, and Direct Connect traffic is not impacted by the event.  Some customers may be experiencing VPN connectivity due to this issue.</div><div><span class=\"yellowfg\"> 1:00 PM PST</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:07 PM PST</span>&nbsp;Between 11:34 AM and 12:51 PM PST, customers experienced Internet connectivity issues for some networks to and from the US-GOV-EAST-1 Region. Connectivity between instances within the Region, in between Regions, and Direct Connect connectivity were not impacted by this issue. The issue has been resolved and connectivity has been fully restored.</div>",
      "service": "internetconnectivity-us-gov-east-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased errors viewing or updating payment information",
      "date": "1670327310",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:48 AM PST</span>&nbsp;We are investigating errors affecting the payments tab on the Billing console. Impact is limited to viewing or updating payments information. Payments are unaffected. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:13 AM PST</span>&nbsp;Starting December 6, 2:42 AM PST customers began experiencing errors when trying to access the payments tab on the AWS Billing console. Impact is limited to viewing or updating payments information. Existing payments configured in the Payment Console are unaffected. We continue to work on resolving the issue.</div><div><span class=\"yellowfg\"> 4:43 AM PST</span>&nbsp;Between 2:42 AM and 4:30 AM PST customers experienced errors when trying to access the payments tab on the AWS Billing console. Impact was limited to viewing or updating payments information and existing payments configured in the Payments Console were unaffected. The issue has been resolved and the Payment Console is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1670335858",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:10 AM PST</span>&nbsp;We are investigating increased IAM API latency and errors. We are actively working towards resolution.</div><div><span class=\"yellowfg\"> 6:41 AM PST</span>&nbsp;Between 4:45 AM and 6:19 AM PST, we experienced increased API error rates and latency for IAM APIs. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1670921550",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:52 AM PST</span>&nbsp;We are investigating increased API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. We have identified the root cause of the issue and are working towards resolution.</div><div><span class=\"yellowfg\"> 1:37 AM PST</span>&nbsp;We continue to work on resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. This issue prevents the creation of new Amazon Machine Images (AMI) within the US-EAST-1 region but does not prevent the launching of instances using existing AMIs. We have resolved the root cause of the initial impact and are currently working on draining the increased backlog of CreateImage requests. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 2:07 AM PST</span>&nbsp;We continue to make progress in resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. Since the last update, we have seen a steady decrease in the backlog of CreateImage requests. Until the backlog has been resolved, new CreateImage requests will fail with \"request limit exceeded\". Once we have cleared the backlog, we will allow new CreateImage requests to be accepted again. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 2:50 AM PST</span>&nbsp;We continue to make progress in resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. Since the last update, we have cleared the backlog of CreateImage requests and are now working to ensure that new CreateImage requests will be processed successfully. Once that is done, we will be removing the throttling for the CreateImage API, which will address the \"request limit exceeded\" error that is currently being returned. Once completed, we expect CreateImage requests to begin operating normally. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 3:52 AM PST</span>&nbsp;We continue to make progress in resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. Since the last update, we have been working to ensure that new CreateImage requests will be processed successfully, and are nearly complete with that process. We expect to start removing the throttles for CreateImage in the next 45 minutes, which is when we expect CreateImage requests to begin operating normally. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 4:54 AM PST</span>&nbsp;Between December 12 8:09 PM and December 13 4:32 AM PST, we experienced increased error rates and latencies for the CreateImage API in the US-EAST-1 Region. This issue affected the creation of new Amazon Machine Images (AMIs) but did not affect the launching (RunInstances) of existing AMIs. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Multiple services (Singapore)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1670968855",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:00 PM PST</span>&nbsp;We are investigating networking connectivity issues and API error rates and latencies for some services within the AP-SOUTHEAST-1 Region. We are working to identify the root cause and will provide more updates as we have them.</div><div><span class=\"yellowfg\"> 2:21 PM PST</span>&nbsp;We are seeing early signs of recovery for the issue affecting networking connectivity and increased API error rates and latencies for some services within the AP-SOUTHEAST-1 Region. The issue has not affected connectivity to the EC2 network. We are continuing to work towards full recovery.</div><div><span class=\"yellowfg\"> 2:50 PM PST</span>&nbsp;We continue to see recovery for all affected services within the AP-SOUTHEAST-1 Region and continue to monitor. While EC2 APIs were affected during the issue, connectivity to existing EC2 instances was not affected. We do not expect any further impact and will provide an update once we have completed all final validations.</div><div><span class=\"yellowfg\"> 3:15 PM PST</span>&nbsp;Between 1:30 PM and 2:20 PM PST, we experienced network connectivity issues which resulted in increased API error rates and latencies in the AP-SOUTHEAST-1 Region. CloudWatch metrics for some services were delayed. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-ap-southeast-1"
    },
    {
      "service_name": "AWS Amplify (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1671287057",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:24 AM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:43 AM PST</span>&nbsp;Between 5:14 AM and 6:27 AM PST, we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "amplify-us-east-1"
    }
  ],
  "current": [
    {
      "service_name": "Amazon CodeCatalyst (Oregon)",
      "summary": "[RESOLVED] Errors Creating User Aliases ",
      "date": "1671635037",
      "status": "0",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:03 AM PST</span>&nbsp;We are investigating errors while attempting to create a new alias in the US-WEST-2 Region. Existing customers are unaffected. We will provide another update at 7:45 AM PST.</div><div><span class=\"yellowfg\"> 7:43 AM PST</span>&nbsp;We continue to investigate errors while attempting to create new aliases for new customers on boarding in the US-WEST-2 Region. We have identified a potential root cause of this alias creation error and continue to work towards resolution. Existing customers remain unaffected. We will provide another update before 8:45 AM PST.</div><div><span class=\"yellowfg\"> 8:18 AM PST</span>&nbsp;Between 3:45 AM and 8:15 AM PST, customers experienced errors when attempting to create new aliases in the US-WEST-2 Region. Existing users were unaffected. The issue has been resolved and the service is operating normally.</div>",
      "service": "codecatalyst-us-west-2"
    }
  ]
}
