{
  "archive": [
    {
      "service_name": "Auto Scaling (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1525914401",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:06 PM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:26 PM PDT</span>&nbsp;We are continuing to investigate increased API error rates in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 7:24 PM PDT</span>&nbsp;We have identified the root cause of the elevated API error rates and can confirm that most customers have recovered. We are continuing to work towards full resolution.</div><div><span class=\"yellowfg\"> 7:40 PM PDT</span>&nbsp;Between 5:42 PM and 7:04 PM PDT, we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "autoscaling-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Delayed network provisioning",
      "date": "1526397141",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:12 AM PDT</span>&nbsp;We are investigating delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:45 AM PDT</span>&nbsp;Between 5:27 AM and 8:17 AM PDT we experienced delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region. The issue have been resolved and the service is operating normally. </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased provisioning times",
      "date": "1526397279",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:14 AM PDT</span>&nbsp;We are investigating increased provisioning times for load balancers in the US-EAST-1 Region. Registration of back-end instances and connectivity to existing instances are not affected. </div><div><span class=\"yellowfg\"> 8:53 AM PDT</span>&nbsp;Between 6:05 AM and 8:17 AM PDT we experienced delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region. Registration of back-end instances and connectivity to existing instances are not affected. The issue have been resolved and the service is operating normally. </div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic MapReduce (N. Virginia)",
      "summary": "[RESOLVED] Delays in starting clusters",
      "date": "1526397532",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:19 AM PDT</span>&nbsp;We are investigating delays in starting Amazon EMR clusters in the US-EAST-1 Region. Existing clusters are not impacted.</div><div><span class=\"yellowfg\"> 8:40 AM PDT</span>&nbsp;Between 7:00 AM and 8:15 AM PDT we experienced delays in starting Amazon EMR clusters in the US-EAST-1 Region. Existing clusters were not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-us-east-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated error rates",
      "date": "1526466746",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:32 AM PDT</span>&nbsp;Between 2:35 AM and 2:39 AM PDT, we experienced elevated error rates for requests in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-standard"
    },
    {
      "service_name": "AWS CodeBuild (N. Virginia)",
      "summary": "[RESOLVED] Increased build error rates",
      "date": "1526469181",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:13 AM PDT</span>&nbsp;We are investigating increased Build error rates in US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 5:12 AM PDT</span>&nbsp;Between 2:50 AM and 4:55 AM PDT, we experienced elevated Build error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "codebuild-us-east-1"
    },
    {
      "service_name": "Amazon AppStream 2.0 (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1526472913",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:15 AM PDT</span>&nbsp;We are currently investigating increased error rates in provisioning streaming instances in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:22 AM PDT</span>&nbsp;Between 2:50 AM and 5:06 AM PDT, we experienced increased error rates in provisioning streaming instances. The issue has been resolved now and the service is operating normally.</div>",
      "service": "appstream2-us-east-1"
    },
    {
      "service_name": "AWS Systems Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1526473109",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:18 AM PDT</span>&nbsp;Between 2:50 AM and 4:55 AM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2systemsmanager-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sao Paulo)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1526569732",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:09 AM PDT</span>&nbsp;We are investigating connectivity issues for some instances in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:32 AM PDT</span>&nbsp; Between 7:26 AM and 7:34 AM PDT, and between 7:57 AM to 8:05 AM PDT, some instances experienced connectivity issues in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-sa-east-1"
    },
    {
      "service_name": "Amazon WorkMail (N. Virginia)",
      "summary": "[RESOLVED] Increased Outlook Sync Latencies and Errors",
      "date": "1527016380",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:13 PM PDT</span>&nbsp;We are investigating increased WorkMail Outlook synchronization latencies and error rates in the US-EAST-1 Region impacting the sending and receiving of new messages or appointments in Outlook clients.</div><div><span class=\"yellowfg\">12:30 PM PDT</span>&nbsp;Between 11:06 AM and 12:09 PM PDT we experienced increased WorkMail Outlook synchronization latencies and error rates in the US-EAST-1 Region, impacting the sending and receiving of new messages or appointments in Outlook clients. The issue has been resolved and the service is operating normally.</div>",
      "service": "workmail-us-east-1"
    },
    {
      "service_name": "Amazon Cognito (N. Virginia)",
      "summary": "[RESOLVED] Elevated Sign In Error Rates",
      "date": "1527647488",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:31 PM PDT</span>&nbsp;We are investigating reports that some users are unable to sign in or sign up to applications using Facebook identities with Amazon Cognito User Pools.</div><div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;Recently, some users were unable to sign in or sign up to applications using Facebook identities with Amazon Cognito User Pools. The issue has been resolved and the service is operating normally.</div>",
      "service": "cognito-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1527752189",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:36 AM PDT</span>&nbsp;We are currently investigating connectivity issues in the US-EAST-2 Region.</div><div><span class=\"yellowfg\"> 1:01 AM PDT</span>&nbsp;Between 12:11 AM and 12:45 AM PDT we experienced impaired Internet connectivity in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS Internet Connectivity (Ohio)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1527752941",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:49 AM PDT</span>&nbsp;\r\nWe are currently investigating connectivity issues in the US-EAST-2 Region. </div><div><span class=\"yellowfg\">12:59 AM PDT</span>&nbsp;Between 12:11 AM and 12:45 AM PDT we experienced impaired Internet connectivity in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-east-2"
    },
    {
      "service_name": "Amazon Elastic File System (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1527800578",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:03 PM PDT</span>&nbsp;We are investigating increased error rates for file system requests in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;Between 1:00 PM and 2:29 PM PDT, we experienced increased error rates for file system read/write requests in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticfilesystem-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Instance Connectivity Issues",
      "date": "1527804823",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:13 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the US-EAST-1 Region.\r\n</div><div><span class=\"yellowfg\"> 3:42 PM PDT</span>&nbsp;We can confirm that there has been an issue in one of the datacenters that makes up one of US-EAST-1 Availability Zones. This was a result of a power event impacting a small percentage of the physical servers in that datacenter as well as some of the networking devices. Customers with EC2 instances in this availability zone may see issues with connectivity to the affected instances. We are seeing recovery and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 4:29 PM PDT</span>&nbsp;We have restored power to the vast majority of the affected instances and continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 5:36 PM PDT</span>&nbsp;﻿﻿Beginning at 2:52 PM PDT a small percentage of EC2 servers lost power in a single Availability Zone in the US-EAST-1 Region. This resulted in some impaired EC2 instances and degraded performance for some EBS volumes in the affected Availability Zone. Power was restored at 3:22 PM PDT, at which point the vast majority of instances and volumes saw recovery. We have been working to recover the remaining instances and volumes. The small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. While we will continue to work to recover all affected instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1527806609",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:43 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some Redshift clusters in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:07 PM PDT</span>&nbsp;Between 2:52 PM and 3:55 PM PDT we experienced connectivity issues affecting some clusters in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (N. Virginia)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1527806975",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:49 PM PDT</span>&nbsp;We are investigating elevated error rates for connections to some WorkSpaces in a single Availability Zone in the US-EAST-1 Region.\r\n</div><div><span class=\"yellowfg\"> 4:18 PM PDT</span>&nbsp;Between 2:52 PM and 4:15 PM PDT we experienced increased error rates for connections to some WorkSpaces in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workspaces-us-east-1"
    },
    {
      "service_name": "Amazon Relational Database Service (N. Virginia)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1527807509",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:58 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:12 PM PDT</span>&nbsp;Beginning at 2:52 PM PDT, we experienced connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region. Multi-AZ database instances functioned correctly and remain available. A small number of Single-AZ database instances remain unavailable and we are working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:42 PM PDT</span>&nbsp;Beginning at 2:52 PM PDT, we experienced connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region. Multi-AZ database instances functioned correctly and remained available. Connectivity was restored at 3:22 PM PDT, at which point the vast majority of database instances recovered. A small number of Single-AZ database instances remain unavailable and we are continuing to work to recover all affected database instances.</div>",
      "service": "rds-us-east-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased Propagation Time",
      "date": "1528291058",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:17 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\"> 6:47 AM PDT</span>&nbsp;Between 5:42 AM and 6:45 AM PDT, we experienced increased propagation times of DNS edits to the Route 53 DNS servers. This did not impact queries to existing DNS records. The issue has been resolved and the service is operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Auto Scaling (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1528330928",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:22 PM PDT</span>&nbsp;We are investigating increased error rates for API calls in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:33 PM PDT</span>&nbsp;Between 5:05 PM and 5:26 PM PDT we experienced increased error rates for API calls in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "autoscaling-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Delays in propagating certain configuration changes to a few CloudFront Edge locations ",
      "date": "1528895774",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:16 AM PDT</span>&nbsp;We are investigating longer than usual propagation times to propagate certain configuration changes to a few of our edge locations: the creation and deletion of CloudFront distributions and the updating of certificates for a CloudFront distribution. All other CloudFront configuration changes are propagating normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally. </div><div><span class=\"yellowfg\"> 6:59 AM PDT</span>&nbsp;Between 3:25 AM and 6:45 AM PDT, CloudFront experienced longer than usual propagation times to propagate certain configuration changes to a few of our edge locations. This issue has been resolved and the service is operating normally. During this time, end user requests for content from our edge locations were not affected by this issue and were being served normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates ",
      "date": "1528907930",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:39 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic to existing load balancers is not affected.</div><div><span class=\"yellowfg\">10:11 AM PDT</span>&nbsp;Between 8:59 AM and 9:58 AM PDT we experienced increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. The issue affected load balancer provisioning and target registration for Application and Network Load Balancer. Classic Load Balancer, as well as traffic to all existing load balancers, was not affected. The issue has been resolved and the service is operating normally. </div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS CodeDeploy (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1529347567",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:46 AM PDT</span>&nbsp;We are investigating elevated error rates for APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:31 PM PDT</span>&nbsp;Between 11:11 AM and 12:00 PM PDT we experienced elevated error rates for APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "codedeploy-us-east-1"
    },
    {
      "service_name": "AWS Cloud9 (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1529609394",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:30 PM PDT</span>&nbsp;Between 11:01 AM and 11:20 AM PDT we experienced increased error rates in creating and accessing Cloud9 environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "cloud9-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased Error Rates and Latencies",
      "date": "1529622992",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:16 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the IAM APIs.</div><div><span class=\"yellowfg\"> 4:24 PM PDT</span>&nbsp;Between 3:38 PM and 4:11 PM PDT we experienced increased IAM API error rates and latencies. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Elastic File System (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1530065273",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:08 PM PDT</span>&nbsp;Between 5:14 PM and 6:09 PM PDT, we experienced increased error rates for API requests in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally.</div>",
      "service": "elasticfilesystem-us-east-1"
    },
    {
      "service_name": "AWS IoT Core (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1530296421",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:20 AM PDT</span>&nbsp;We are investigating increased error rates for connections to AWS IoT Core, affecting some customers in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">11:38 AM PDT</span>&nbsp;We continue to investigate increased error rates for connections to AWS IoT Core, affecting some customers in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">12:02 PM PDT</span>&nbsp;Error rates for connections to AWS IoT Core in the US-EAST-1 Region are improving. We continue to work towards full resolution. </div><div><span class=\"yellowfg\">12:44 PM PDT</span>&nbsp;Between 9:26 AM and 11:56 AM PDT we experienced intermittent elevated error rates for connections to AWS IoT Core in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "awsiot-us-east-1"
    },
    {
      "service_name": "Amazon Chime",
      "summary": "[RESOLVED] Availability",
      "date": "1530900123",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:02 AM PDT</span>&nbsp;We are investigating meeting availability issues in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:18 AM PDT</span>&nbsp;We continue to investigate meeting availability issues in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">11:49 AM PDT</span>&nbsp;Between 10:24 AM PDT and 11:31 AM PDT, we experienced meeting availability issues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "chime"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased Propagation Time",
      "date": "1531395752",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:44 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\"> 5:15 AM PDT</span>&nbsp;We can confirm increased propagation times of DNS edits to the Route 53 DNS servers and continue to work towards resolution. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\"> 6:02 AM PDT</span>&nbsp;Propagation of DNS edits for Route 53 records in Public Hosted Zones has recovered. We are still working towards recovery for DNS edits for records in Route 53 Private Hosted Zones.</div><div><span class=\"yellowfg\"> 6:24 AM PDT</span>&nbsp;Between 4:00 AM and 6:14 AM PDT we experienced increased propagation times of DNS edits to a single Route 53 edge location. This does not impact queries to existing DNS records. The issue has been resolved and the service is operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1531774261",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:51 PM PDT</span>&nbsp;We are currently experiencing intermittent errors accessing the AWS Management Console. AWS services are operating normally.</div><div><span class=\"yellowfg\"> 2:10 PM PDT</span>&nbsp;We are currently experiencing intermittent errors accessing the AWS Management Console when using root account login credentials. The underlying AWS services and console logins using IAM users are operating normally.</div><div><span class=\"yellowfg\"> 2:47 PM PDT</span>&nbsp;We have identified the root cause for the intermittent errors accessing the AWS Management Console when using root account login credentials. We are beginning to see recovery, and continue to work toward full resolution. The underlying AWS services and console logins using IAM users continue to operate normally.</div><div><span class=\"yellowfg\"> 3:44 PM PDT</span>&nbsp;Between 12:28 PM and 3:13 PM PDT we experienced intermittent errors accessing the AWS Management Console. AWS services were not affected by this event. This issue has been resolved and the service is operating normally.</div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Frankfurt)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1531962885",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:14 PM PDT</span>&nbsp;We are investigating connectivity issues to a single Availability Zone in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 6:34 PM PDT</span>&nbsp;Between 5:39 PM and 5:49 PM PDT we experienced network connectivity issues for some instances in a single Availability Zone in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-central-1"
    },
    {
      "service_name": "Amazon Simple Queue Service (Tokyo)",
      "summary": "[解決済み]SQSでのコネクションリセット及びソケットタイムアウトの発生 | [RESOLVED] Connection reset and socket timeout for SQS ",
      "date": "1533208334",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:12 AM PDT</span>&nbsp;日本時間 2018/08/02 17:30 から 17:55 にかけて、東京リージョンの SQS においてコネクションリセット及びソケットタイムアウトの発生率が上昇しました。問題は既に解消し、現在サービスは正常に稼働しております。| Between 1:30AM and 1:55 AM PDT we experienced connection resets and socket timeout rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. </div><div><span class=\"yellowfg\"> 4:22 AM PDT</span>&nbsp;日本時間 2018/08/02 17:30 から 17:55 にかけて、東京リージョンの SQS においてコネクションリセット及びソケットタイムアウトの発生率が上昇しました。問題は既に解消し、現在サービスは正常に稼働しております。| Between 1:30AM and 1:55 AM PDT we experienced connection resets and socket timeout rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sqs-ap-northeast-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED]  Increased Console and API error rates",
      "date": "1533545240",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:47 AM PDT</span>&nbsp;Between 12:49 AM and 1:04 AM PDT customers using the Route 53 console and API experienced an elevated error rate. The issue has been resolved and both the Route 53 console and API are operating normally. Route 53 DNS and health checks were both operating normally during this time.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Instance launch errors and connectivity issues",
      "date": "1533674185",
      "status": 0,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:49 PM PDT</span>&nbsp;Between 1:06 PM and 1:33 PM PDT we experienced increased error rates for new instance launches and intermittent network connectivity issues for some running instances in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Simple Notification Service (Tokyo)",
      "summary": "[RESOLVED] SNSのエラーレート上昇について | Elevated error rates for SNS",
      "date": "1533695504",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:31 PM PDT</span>&nbsp;東京リージョンにおきまして、現在SNS Publish APIのエラーレートが上昇しており、原因を調査しております。| We are currently investigating elevated error rates for the SNS Publish API in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 8:34 PM PDT</span>&nbsp;東京リージョンにおきまして、日本時間 2018年8月8日 10:47-12:08の間、SNS Publish APIのエラーレートが上昇しておりました。事象は復旧し、サービスは正常に稼働しております。| Between 6:47 PM and 8:08 PM PDT we experienced elevated error rates for the SNS Publish API in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "sns-ap-northeast-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] エラーレートの上昇について | Elevated Error Rates",
      "date": "1533696595",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:50 PM PDT</span>&nbsp;東京リージョンにおきまして、AWS マネジメントコンソールのエラーレートが上昇しており、原因を調査しております。| We are investigating elevated error rates for the AWS Management Console in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 8:26 PM PDT</span>&nbsp;東京リージョンにおきまして、日本時間 2018年8月8日 10:47-12:08にかけて AWS マネージメントコンソールのエラーレートが上昇しておりました。事象は復旧し、コンソールは正常に稼働しております。| Between 6:47 PM and 8:08 PM PDT, users experienced elevated error rates for the AWS Management Console in the AP-NORTHEAST-1 Region. The issue has been resolved and the Console is operating normally. </div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Mumbai)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533722524",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:02 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 4:13 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-ap-south-1"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Paris)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533722689",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:05 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the EU-WEST-3 Region.</div><div><span class=\"yellowfg\"> 4:16 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-WEST-3 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-eu-west-3"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Ohio)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533722825",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:07 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the US-EAST-2 Region.</div><div><span class=\"yellowfg\"> 4:18 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-us-east-2"
    },
    {
      "service_name": "Amazon Elastic MapReduce (N. Virginia)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533722962",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:09 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:19 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-us-east-1"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Ireland)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533723112",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:12 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 4:21 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Frankfurt)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533723257",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:14 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the EU-CENTRAL-1 Region</div><div><span class=\"yellowfg\"> 4:23 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-eu-central-1"
    },
    {
      "service_name": "Amazon Elastic MapReduce (N. California)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533723374",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:16 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 4:26 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-us-west-1"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Singapore)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533723484",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:18 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 4:29 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-ap-southeast-1"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Sydney)",
      "summary": "[RESOLVED] Failures in starting clusters",
      "date": "1533723818",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:23 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 4:31 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-ap-southeast-2"
    },
    {
      "service_name": "AWS CodeDeploy (Ireland)",
      "summary": "[RESOLVED] Increased Deployment Latencies",
      "date": "1534782477",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:28 AM PDT</span>&nbsp;We are investigating increased deployment latencies in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 9:59 AM PDT</span>&nbsp;Between 7:27 AM and 9:56 AM PDT we experienced increased deployment latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "codedeploy-eu-west-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates and Latencies",
      "date": "1534799678",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:15 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for the SSO service in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;Between 10:30 AM and 2:18 PM PDT we experienced increased error rates and latencies for the SSO service in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Call quality degradation",
      "date": "1534866429",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:47 AM PDT</span>&nbsp;We are investigating degraded call quality in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:21 AM PDT</span>&nbsp;Between 7:10 AM and 8:55 AM PDT, we experienced degraded call quality in the US-EAST-1 Region. Agents and end-customers may have experienced increased call latency or static on calls during this time. The issue has been resolved and the service is operating normally. </div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon Cognito (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1535399349",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:49 PM PDT</span>&nbsp;We are investigating increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 1:16 PM PDT</span>&nbsp;We can confirm increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 1:34 PM PDT</span>&nbsp;Between 11:33 AM and 1:25 PM PDT we experienced increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cognito-us-west-2"
    },
    {
      "service_name": "Amazon Simple Queue Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated error rates for SQS",
      "date": "1536163992",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:13 AM PDT</span>&nbsp;We are currently investigating elevated error rates for SQS in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:30 AM PDT</span>&nbsp;Between 7:38 AM and 9:18 AM PDT, we experienced elevated API error rates and connection failures in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "sqs-us-east-1"
    },
    {
      "service_name": "Amazon WorkMail (N. Virginia)",
      "summary": "[RESOLVED] Degraded WorkMail performance",
      "date": "1536725279",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:08 PM PDT</span>&nbsp;We are investigating degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages.\r\nThere is no impact to Outlook, EWS client applications or mobile clients.</div><div><span class=\"yellowfg\"> 9:40 PM PDT</span>&nbsp;We have identified the cause of degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages and continue working towards resolution. There is no impact to Outlook, EWS client applications or mobile clients.</div><div><span class=\"yellowfg\"> 9:59 PM PDT</span>&nbsp;Between 7:10 PM and 9:46 PM PDT we experienced degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages. The issue has been resolved and the service is operating normally.</div>",
      "service": "workmail-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ireland)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1537949151",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:06 AM PDT</span>&nbsp;We are investigating increased error rates for new launches in the EU-WEST-1 Region.\r\n\r\n</div><div><span class=\"yellowfg\"> 1:37 AM PDT</span>&nbsp;We can confirm increased API error rates and connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:14 AM PDT</span>&nbsp;We continue to investigate connectivity issues from some instances to some AWS services in the EU-WEST-1 Region. We have identified the root cause and are taking steps to resolve the issue.</div><div><span class=\"yellowfg\"> 3:03 AM PDT</span>&nbsp;We have resolved the connectivity issues from EC2 instances to the affected AWS services in the EU-WEST-1 Region. We continue to see elevated error rates for the RunInstances EC2 API, which we are working to resolve. Internet connectivity and connectivity between EC2 instances remain unaffected.</div><div><span class=\"yellowfg\"> 3:28 AM PDT</span>&nbsp;Starting at 12:15 AM PDT we experienced increased API error rates for the EC2 API, and connectivity issues between EC2 instances and AWS services in the EU-WEST-1 Region. At 2:29 AM PDT, the connectivity issues between EC2 instances and AWS services were resolved. At 2:59 AM PDT, the increased API error rates for the EC2 APIs were fully resolved. Internet connectivity and connectivity between EC2 instances was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-1"
    },
    {
      "service_name": "AWS CloudHSM (Ireland)",
      "summary": "[RESOLVED] Connectivity issues",
      "date": "1537956232",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:11 AM PDT</span>&nbsp;We are continuing to investigate connectivity issues impacting CloudHSM and CloudHSM Classic HSMs in a single Availability Zone in EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 3:44 AM PDT</span>&nbsp;We can confirm that current generation CloudHSM instances are operating normally. CloudHSM Classic instances in a single Availability Zone in the EU-WEST-1 Region are experiencing network connectivity errors. As we work to recover the remaining CloudHSM Classic instances, we will continue to provide updates via the Personal Health Dashboard.</div>",
      "service": "cloudhsm-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic File System (Ireland)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1537958697",
      "status": 0,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:13 AM PDT</span>&nbsp;Between 12:15 AM and 2:12 AM PDT, we experienced increased error rates for console requests in EU-WEST-1 Region. The issue has been resolved and the service is now operating normally.</div>",
      "service": "elasticfilesystem-eu-west-1"
    },
    {
      "service_name": "AWS Direct Connect (Ireland)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1537960452",
      "status": 0,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:19 AM PDT</span>&nbsp;Between 12:15 AM and 2:29 AM PDT, we experienced increased packet loss, impacting AWS Direct Connect connectivity for some customers in the EU-WEST-1 Region. The issue has been resolved and the service is now operating normally.</div>",
      "service": "directconnect-eu-west-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1538097782",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:23 PM PDT</span>&nbsp;Between 5:45 PM and 6:10 PM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Frankfurt)",
      "summary": "[RESOLVED] Network connectivity",
      "date": "1538413958",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:12 AM PDT</span>&nbsp;We are investigating intermittent connectivity issues between the EU-CENTRAL-1 Region and other AWS Regions.</div><div><span class=\"yellowfg\">10:24 AM PDT</span>&nbsp;We can confirm intermittent issues for Internet and Inter-region network connectivity within the EU-CENTRAL-1 Region. We have mitigated the impact but continue to work towards full resolution.</div><div><span class=\"yellowfg\">10:59 AM PDT</span>&nbsp;Between 9:21 AM and 9:31 AM PDT and between 9:41 AM and 9:51 AM PDT we experienced Internet connectivity issues for the EU-CENTRAL-1 Region and Inter-region connectivity issues between the EU-CENTRAL-1 Region and other AWS regions. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-eu-central-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Oregon)",
      "summary": "[RESOLVED] Increased API Errors ",
      "date": "1538424731",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:12 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 2:06 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for the EC2 APIs and EC2 Management Console in the US-WEST-2 Region. Existing EC2 instances are not affected.</div><div><span class=\"yellowfg\"> 3:14 PM PDT</span>&nbsp;We are seeing recovery for the increased error rates and latencies affecting the EC2 APIs in the US-WEST-2 Region. Some requests may return a \"request limit exceeded\" error as we work toward full recovery. We recommend retrying failed requests.</div><div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;Error rates and latencies for the EC2 APIs in the US-WEST-2 Region remain at normal levels. A small percentage of requests are still returning a \"request limit exceeded\" error as we work toward full recovery. We recommend retrying failed requests.</div><div><span class=\"yellowfg\"> 5:33 PM PDT</span>&nbsp;Starting at 11:20 AM PDT, we experienced periods of increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region. At 2:02 PM PDT, error rates and latencies recovered for the majority of the affected APIs. At 2:55 PM PDT error rates and latencies returned to normal levels but some customers continued to experience \"request limit exceeded\" errors. At 5:10 PM PDT, all error rates, including \"request limit exceeded\" errors, returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-west-2"
    },
    {
      "service_name": "Auto Scaling (Oregon)",
      "summary": "[RESOLVED] Increased Launch Latencies ",
      "date": "1538428181",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:09 PM PDT</span>&nbsp;We are investigating increased launch times for EC2 instances managed by Auto Scaling in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 3:29 PM PDT</span>&nbsp;Between 12:32 PM PDT and 2:40 PM PDT, we experienced elevated launch times for EC2 instance managed by Auto Scaling in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "autoscaling-us-west-2"
    },
    {
      "service_name": "AWS Lambda (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1538428351",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:12 PM PDT</span>&nbsp;We are investigating increased error rates for newly created functions in the US-WEST-2 Region. This includes functions created via the console as well as those created via the API or CLI.</div><div><span class=\"yellowfg\"> 3:51 PM PDT</span>&nbsp;We continue to investigate increased error rates for newly created VPC functions in the US-WEST-2 Region. This includes VPC functions created via the console as well as those created via the API or CLI. The issue has been resolved for non-VPC functions. </div><div><span class=\"yellowfg\"> 5:14 PM PDT</span>&nbsp;Between 12:47 PM PDT and 4:55 PM PDT, we experienced increased error rates for newly created functions in US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Oregon)",
      "summary": "[RESOLVED] Increased Provisioning Latencies ",
      "date": "1538428470",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:14 PM PDT</span>&nbsp;We are investigating increased provisioning times for load balancers in the US-WEST-2 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\"> 3:49 PM PDT</span>&nbsp;We are seeing an improvement in the increased provisioning times affecting load balancers in the US-WEST-2 Region. We continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 5:25 PM PDT</span>&nbsp;Between 1:00 PM and 4:48 PM PDT, we experienced increased provisioning times for load balancers in the US-WEST-2 Region. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-west-2"
    },
    {
      "service_name": "Amazon Elastic MapReduce (Oregon)",
      "summary": "[RESOLVED] Delays in starting clusters",
      "date": "1538429178",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:26 PM PDT</span>&nbsp;We are investigating delays in starting clusters in the US-WEST-2 Region. Existing clusters are not affected.</div><div><span class=\"yellowfg\"> 3:53 PM PDT</span>&nbsp;We continue to investigate delays in starting clusters in the US-WEST-2 Region. Existing clusters are not affected. </div><div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;Between 1:10 PM and 4:20 PM PDT we experienced delays in starting clusters in US-West-2 Region. Existing clusters were not affected. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\"> 5:17 PM PDT</span>&nbsp;Between 1:10 PM and 4:20 PM PDT we experienced delays in starting clusters in US-WEST-2 Region. Existing clusters were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-us-west-2"
    },
    {
      "service_name": "Amazon WorkSpaces (Oregon)",
      "summary": "[RESOLVED] Increased Provisioning Error Rates",
      "date": "1538430961",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:56 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for provisioning new WorkSpaces in the US-WEST-2 Region</div><div><span class=\"yellowfg\"> 3:21 PM PDT</span>&nbsp;Between 1:00 PM and 3:00 PM PDT, we experienced increased error rates for new WorkSpaces provisioning in the US-WEST-2 Region. Existing WorkSpaces were unaffected. The issue has been resolved and the service is operating normally. </div>",
      "service": "workspaces-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Oregon)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1538626579",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:16 PM PDT</span>&nbsp;We're investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 9:45 PM PDT</span>&nbsp;Between 8:48 PM and 9:25 PM PDT we experienced increased error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-west-2"
    },
    {
      "service_name": "AWS Secrets Manager (Oregon)",
      "summary": "[RESOLVED] Elevated latencies and API Error Rates",
      "date": "1538645129",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:25 AM PDT</span>&nbsp;We are investigating elevated latencies and API error rates in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 3:09 AM PDT</span>&nbsp;We continue to investigate elevated latencies and API error rates in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:04 AM PDT</span>&nbsp;Between 2:04 AM and 7:50 AM PDT we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "secretsmanager-us-west-2"
    },
    {
      "service_name": "Amazon Simple Storage Service (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1538766090",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:01 PM PDT</span>&nbsp;We are investigating increased error rates for Amazon S3 requests in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:16 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates in the US-EAST-2 Region and are working towards resolution.</div><div><span class=\"yellowfg\">12:42 PM PDT</span>&nbsp;We have identified the root cause and are seeing some recovery. We are continuing to monitor and work towards full resolution.</div><div><span class=\"yellowfg\">12:51 PM PDT</span>&nbsp;Between 11:47 AM and 12:33 PM PDT we experienced increased error rates for Amazon S3 requests in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-east-2"
    },
    {
      "service_name": "Amazon Glacier (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1538768132",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:35 PM PDT</span>&nbsp;We are investigating increased error rates and latency in the US-EAST-2 Region. </div><div><span class=\"yellowfg\">12:48 PM PDT</span>&nbsp;Between 11:47 AM PDT and 12:33 PM PDT we experienced increased API error rates and latencies in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "glacier-us-east-2"
    },
    {
      "service_name": "Amazon Kinesis Firehose (Ohio)",
      "summary": "[RESOLVED] Delayed Data Delivery",
      "date": "1538768432",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:40 PM PDT</span>&nbsp;We are investigating delays in S3 delivery for Kinesis Firehose in the US-EAST-2 Region.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;Between 11:47 AM and 12:38 PM PDT we experienced increased Firehose to S3 delivery delays in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "firehose-us-east-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Degraded Volume Performance for EBS Volumes / Insufficient-Data for EC2 Instance Status Checks",
      "date": "1538769418",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;Between 11:47 AM and 12:33 PM PDT we experienced degraded performance for some EBS volumes in the US-EAST-2 Region. During this time, some customers also experienced insufficient-data in the results from EC2 instance status checks. CloudWatch alarms may have transitioned into \"INSUFFICIENT_DATA\" state if set on the delayed metrics. EC2 instances operated normally during this time; only EC2 CloudWatch metrics were affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS CodeCommit (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1538769625",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:00 PM PDT</span>&nbsp;Between 11:47 AM PDT and 12:33 PM PDT we experienced an increased error rate for APIs and Git Push/Pull in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "codecommit-us-east-2"
    },
    {
      "service_name": "Amazon Elastic Container Service (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1538770316",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:12 PM PDT</span>&nbsp;Between 11:47 AM and 12:33 PM PDT we experienced increased error rates when launching Fargate tasks in the US-EAST-2 Region. Running tasks were not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "ecs-us-east-2"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Increased Errors",
      "date": "1538776626",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:57 PM PDT</span>&nbsp;We are currently investigating increased latencies and errors for the Agent Call Control Panel in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:08 PM PDT</span>&nbsp;Between 11:27 AM and 2:52 PM PDT, we experienced increased latencies and errors for the Agent Call Control Panel in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Title: Increased Launch Errors ",
      "date": "1539093649",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:01 AM PDT</span>&nbsp;Between 5:31 AM and 6:42 AM PDT we experienced increased error rates for instance launches in a single availability zone in the US-EAST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon OpenSearch Service (N. California)",
      "summary": "[RESOLVED] Domain Connectivity",
      "date": "1539650824",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:50 PM PDT</span>&nbsp;We are investigating connectivity issues for some domains in the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 6:35 PM PDT</span>&nbsp;Between 4:50 PM and 6:20 PM PDT we experienced domain connectivity issues in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticsearch-us-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1539804061",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:22 PM PDT</span>&nbsp;We are investigating network connectivity issues for some instances within the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:30 PM PDT</span>&nbsp;Between 11:27 AM and 11:28 AM PDT some instances experienced connectivity issues for network traffic between Availability Zones within the US-EAST-1 Region. The issue was automatically mitigated and we continue to monitor all affected instances. </div><div><span class=\"yellowfg\"> 1:06 PM PDT</span>&nbsp;Between 11:27 AM and 11:28 AM PDT some instances experienced connectivity issues for network traffic between Availability Zones within the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased provisioning and registration times ",
      "date": "1539806007",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:53 PM PDT</span>&nbsp;We are investigating increased provisioning and back-end target registration times for load balancers within the US-EAST-1 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\"> 2:18 PM PDT</span>&nbsp;We continue to work on resolving the increased provisioning and back-end target registration times for Classic and Application Load Balancers within the US-EAST-1 Region. Provisioning and back-end target registration times are not affected for Network Load Balancers. Connectivity to all load balancers is not affected.</div><div><span class=\"yellowfg\"> 2:59 PM PDT</span>&nbsp;Beginning at 11:50 AM PDT we experienced increased provisioning times for Classic and Application Load Balancers within the US-EAST-1 Region. At 12:40 PM PDT, registration times for new back-end targets began to increase. At 2:05 PM PDT, both load balancer provisioning and back-end target registration latencies began to recover. By 2:25 PM PDT, back-end target registration times had returned to normal levels and by 2:44 PM PDT load balancer provisioning had fully recovered. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Relational Database Service (N. Virginia)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1539806429",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:02 PM PDT</span>&nbsp;Between 11:28 AM and 11:31 AM PDT some Multi-AZ and Single-AZ RDS instances experienced network connectivity issues in the US-EAST-1 Region. All affected Multi-AZ RDS instances failed over to a second Availability Zone, which restored connectivity. The majority of the Single-AZ RDS instances have recovered and we continue to work on restoring connectivity to a small number of remaining Single-AZ RDS instances.\r\n</div><div><span class=\"yellowfg\"> 1:28 PM PDT</span>&nbsp;Between 11:28 AM and 11:31 AM PDT some Multi-AZ and Single-AZ RDS instances experienced network connectivity issues in the US-EAST-1 Region. All affected Multi-AZ RDS instances failed over to a second Availability Zone, which restored connectivity. The remaining Single-AZ RDS instances have now restored connectivity. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-us-east-1"
    },
    {
      "service_name": "Auto Scaling (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates ",
      "date": "1539810083",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:02 PM PDT</span>&nbsp;We are experiencing increased API error rates and delayed group scaling in the US-EAST-1 Region. Connectivity to existing EC2 Instances is not affected.</div><div><span class=\"yellowfg\"> 3:45 PM PDT</span>&nbsp;We have identified the root cause of the increased API error rates and delayed group scaling in the US-EAST-1 Region. We are beginning to see recovery and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:07 PM PDT</span>&nbsp;Between 11:28 AM and 4:05 PM PDT we experienced increased API error rates and delayed group scaling in the US-EAST-1 Region. Connectivity to existing EC2 Instances was not affected. The issue has been resolved and the service is operating normally. </div>",
      "service": "autoscaling-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. California)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1539971754",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:56 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-WEST-1 Region. </div><div><span class=\"yellowfg\">11:27 AM PDT</span>&nbsp;Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced increased API error rates and latencies in the US-WEST-1 Region. APIs also returned insufficient-data in the results from EC2 instance status checks and CloudWatch alarms transitioned into \"INSUFFICIENT_DATA\" state if set on the delayed metrics. EC2 instances operated normally during this time. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-us-west-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (N. California)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1539972265",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:04 AM PDT</span>&nbsp;We are investigating increased error rates for Amazon S3 requests in the US-WEST-1 Region.</div><div><span class=\"yellowfg\">11:19 AM PDT</span>&nbsp;Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced elevated failure rates for some Amazon S3 requests in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-west-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. California)",
      "summary": "[RESOLVED] Elevated Failure Rates",
      "date": "1539973333",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:22 AM PDT</span>&nbsp;Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced elevated failure rates for some Amazon DynamoDB requests in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "dynamodb-us-west-1"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Metrics Performance",
      "date": "1540316843",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:47 AM PDT</span>&nbsp;We are investigating delays in forming contact trace records, and degraded service for the metrics and call recording UI in the US-EAST-1 Region. Call handling is not affected.</div><div><span class=\"yellowfg\">11:16 AM PDT</span>&nbsp;We continue to investigate delays in forming contact trace records, and degraded performance of the metrics and call recording search experience in the US-EAST-1 region. Call handling is not affected.</div><div><span class=\"yellowfg\">12:11 PM PDT</span>&nbsp;Between 9:50 AM and 11:40 AM PDT, we experienced delays in forming contact trace records, and degraded performance of the metrics and call recording search experience in the US-EAST-1 Region. No metrics or call recordings were lost, and call handling was not affected. The issue has been resolved and the service is operating normally. </div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (Ireland)",
      "summary": "[RESOLVED] Connection errors",
      "date": "1540431788",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:43 PM PDT</span>&nbsp;We are investigating increased error rates when connecting to WorkSpaces in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 7:18 PM PDT</span>&nbsp;We can confirm increased error rates when connecting to WorkSpaces in the EU-WEST-1 Region. Connections using WorkSpaces Web Access are not affected by this issue.</div><div><span class=\"yellowfg\"> 7:57 PM PDT</span>&nbsp;We have identified the cause of increased error rates when connecting to some WorkSpaces in the EU-WEST-1 Region and continue working towards resolution. Connections using WorkSpaces Web Access are not affected by this issue.</div><div><span class=\"yellowfg\">10:05 PM PDT</span>&nbsp;We have identified the cause of increased error rates when connecting to some WorkSpaces in the EU-WEST-1 Region and are in process of resolving. Connections using WorkSpaces Web Access are not affected by this issue.</div><div><span class=\"yellowfg\">10:29 PM PDT</span>&nbsp;We experienced an issue causing increased error rates when connecting to WorkSpaces from native clients in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workspaces-eu-west-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased API Latencies",
      "date": "1540486732",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:59 AM PDT</span>&nbsp;We are investigating an increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services whose features require IAM roles will also be impacted. User authentications and authorizations are not impacted.</div><div><span class=\"yellowfg\">10:40 AM PDT</span>&nbsp;We continue to investigate increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services like AWS CloudFormation and AWS Lambda that use IAM roles may also be impacted. User authentications and authorizations are not impacted. </div><div><span class=\"yellowfg\">11:31 AM PDT</span>&nbsp;We have identified the root cause and are working towards a resolution for increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services like AWS CloudFormation and AWS Lambda that use IAM roles may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=\"yellowfg\">12:40 PM PDT</span>&nbsp;We are seeing improvement in the latency for administrative APIs (Create, Delete, List, Get, and Update). We are continuing to work towards a full resolution. User authentications and authorizations are not impacted. </div><div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;Between 8:28 AM and 12:31 PM PDT, we experienced increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions were impacted in multiple regions. User authentications and authorizations were not impacted. The issue has been resolved, and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED]  Increased Console Error Rates",
      "date": "1542102750",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:52 AM PST</span>&nbsp;We are investigating increased error rates in the IAM console impacting policy operations for all IAM entities.</div><div><span class=\"yellowfg\"> 2:27 AM PST</span>&nbsp;Between 12:33 AM and 1:33 AM PST we experienced increased error rates in the IAM console impacting policy operations for all IAM entities. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "AWS Certificate Manager (Ireland)",
      "summary": "[RESOLVED] Validation and Issuance Delays for New Certificates",
      "date": "1542220203",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:31 AM PST</span>&nbsp;We are investigating delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\">11:23 AM PST</span>&nbsp;We can confirm delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. Existing certificates and previously deployed certificates are unaffected.</div><div><span class=\"yellowfg\">12:22 PM PST</span>&nbsp;We have identified the root cause of delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. We are beginning to see recovery and continue to work toward full resolution. Existing certificates and previously deployed certificates are unaffected.</div><div><span class=\"yellowfg\"> 2:40 PM PST</span>&nbsp;Between 5:45 AM and 1:20 PM PST we experienced delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. Existing certificates and previously deployed certificates were unaffected. The issue has been resolved and the service is operating normally.</div>",
      "service": "certificatemanager-eu-west-1"
    },
    {
      "service_name": "Amazon Transcribe (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1542530957",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:49 AM PST</span>&nbsp;We are investigating increased API error rates in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 1:23 AM PST</span>&nbsp;Between 9:51 PM on November 17th, and 1:02 AM PST on November 18th, we experienced increased API error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "transcribe-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Container Registry (Oregon)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1542674869",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:48 PM PST</span>&nbsp;We are experiencing elevated error rates for all APIs in the US-WEST-2 Region. </div><div><span class=\"yellowfg\"> 5:14 PM PST</span>&nbsp;We have identified the root cause of the increased API error rates. We are beginning to see recovery, and continue to work towards full resolution.</div><div><span class=\"yellowfg\"> 5:27 PM PST</span>&nbsp;Between 4:02 PM and 5:04 PM PST we experienced increased API error rates for the ECR APIs in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ecr-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API and Launch Error Rates",
      "date": "1542721813",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:50 AM PST</span>&nbsp;We are investigating increased API and launch error rates in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:20 AM PST</span>&nbsp;Between 5:06 AM and 5:50 AM PST we experienced elevated error rates for instance related APIs and new instance launches in a single Availability Zone in the US-EAST-1 Region. Existing instances were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542766836",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:20 PM PST</span>&nbsp;Between 5:15 PM and 5:55 PM PST, we experienced intermittent increased error rates for a subset of requests made to the Amazon S3 APIs in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-east-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] DNS Resolution Issues",
      "date": "1542844952",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:02 PM PST</span>&nbsp;We are investigating intermittent DNS resolution issues for some instances in the AP-NORTHEAST-2 Region. </div><div><span class=\"yellowfg\"> 4:36 PM PST</span>&nbsp;We have identified the cause of the DNS resolution issues for some instances in the AP-NORTHEAST-2 Region and continue working towards resolution.\r\n</div><div><span class=\"yellowfg\"> 5:01 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced DNS resolution issues for some instances in the AP-NORTHEAST-2 Region. Some AWS services experienced elevated error rates as a result of this issue. The issue has been resolved and the service is operating normally. <br> <br>Additional details have been provided <a href=\"https://aws.amazon.com/message/74876/\">here</a>. Customers who have questions about this event can contact the AWS Support team by <a href=\"https://aws.amazon.com/support\">opening a case in the AWS Support Center</a>. </div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS Lambda (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542846560",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:29 PM PST</span>&nbsp;We are investigating increased API error rates in the AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:18 PM PST</span>&nbsp;We have identified the root cause and are seeing recovery. We continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 6:19 PM PST</span>&nbsp;Between 3:19 PM and 5:58 PM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-ap-northeast-2"
    },
    {
      "service_name": "AWS Elastic Beanstalk (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542847301",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:41 PM PST</span>&nbsp;We are currently investigating increased error rates to Elastic Beanstalk APIs in the AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:09 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-ap-northeast-2"
    },
    {
      "service_name": "Amazon API Gateway (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542847420",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:43 PM PST</span>&nbsp;We are investigating increased error rates for invokes in the AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:00 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "apigateway-ap-northeast-2"
    },
    {
      "service_name": "Amazon MQ (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542847452",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:44 PM PST</span>&nbsp;We are investigating increased API error rates in AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:20 PM PST</span>&nbsp;We have identified the root cause and are seeing recovery. We continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 5:40 PM PST</span>&nbsp;Between 3:19 PM and 5:28 PM PST we experienced increased API error rates in AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "mq-ap-northeast-2"
    },
    {
      "service_name": "Amazon Kinesis Firehose (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542847565",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:46 PM PST</span>&nbsp;We are investigating increased API fault rates in the AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:05 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased API fault rates and latencies in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "firehose-ap-northeast-2"
    },
    {
      "service_name": "AWS IoT Core (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542848046",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:54 PM PST</span>&nbsp;We are currently investigating increased error rates for AWS IoT Core in the AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:09 PM PST</span>&nbsp;Between 3:19 PM and 4:51 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "awsiot-ap-northeast-2"
    },
    {
      "service_name": "Amazon WorkSpaces (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542848776",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:06 PM PST</span>&nbsp;Between 3:15 PM and 4:40 PM PST we experienced increased errors for connections to WorkSpaces in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "workspaces-ap-northeast-2"
    },
    {
      "service_name": "Amazon Redshift (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542849499",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:18 PM PST</span>&nbsp;Between 3:19 PM and 4:56 PM PST we experienced API and S3 connectivity failures in AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-ap-northeast-2"
    },
    {
      "service_name": "AWS X-Ray (Seoul)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1542849575",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:19 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased error rates for API calls in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "xray-ap-northeast-2"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased Billing Console Error Rates",
      "date": "1543787631",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:54 PM PST</span>&nbsp;We are investigating errors affecting the Billing Console including the Dashboard and the Bills page.</div><div><span class=\"yellowfg\"> 2:22 PM PST</span>&nbsp;Between 12:28 PM and 2:10 PM PST we experienced increased error rates affecting the Billing Console including the Dashboard and the Bills page. The issue has been resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Elastic File System (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1543880790",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:46 PM PST</span>&nbsp;We are investigating increased error rates for file system creation requests in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:13 PM PST</span>&nbsp;We are continuing to experience increased error rates for file system creation requests in the US-EAST-1 Region, and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:02 PM PST</span>&nbsp;Between 2:40 PM and 4:45 PM PST, we experienced increased error rates for file system creation requests in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally.</div>",
      "service": "elasticfilesystem-us-east-1"
    },
    {
      "service_name": "AWS Management Console (US-West)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1544570280",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:18 PM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 3:56 PM PST</span>&nbsp;Between 2:11 PM and 3:41 PM PST we experienced increased error rates for the AWS Management Console in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "management-console-us-gov-west-1"
    },
    {
      "service_name": "AWS Key Management Service (Sao Paulo)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1544584045",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:07 PM PST</span>&nbsp;We are investigating increased error rates on the KMS CreateKey API in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:34 PM PST</span>&nbsp;We have identified the root cause for the increased error rates on the KMS CreateKey API in the SA-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 8:16 PM PST</span>&nbsp;Between 6:23 PM and 8:03 PM PST, we experienced increased error rates on the KMS CreateKey API in the SA-EAST-1 Region. This issue has been resolved and the service is operating normally.</div>",
      "service": "kms-sa-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Registry (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1545248645",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:44 AM PST</span>&nbsp;We are investigating increased API error rates for the ECR APIs in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">11:58 AM PST</span>&nbsp;Between 11:14 AM and 11:51 AM PST we experienced increased API error rates for the ECR APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "ecr-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (N. Virginia)",
      "summary": "[RESOLVED] Increased errors terminating WorkSpaces",
      "date": "1546532318",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:18 AM PST</span>&nbsp;We are investigating increased errors when terminating WorkSpaces in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:23 AM PST</span>&nbsp;Between 6:40 AM and 9:14 AM PST Amazon WorkSpaces experienced increased errors when terminating WorkSpaces in the US-EAST-1 Region. The issue causing increased errors has been resolved and the service is operating normally.</div>",
      "service": "workspaces-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1547258341",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:59 PM PST</span>&nbsp;We are investigating network connectivity issues for some instances in a single Available Zone in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 6:29 PM PST</span>&nbsp;We can confirm that some instances have experienced a loss of power in a single Availability Zone in the EU-WEST-2 Region. Some EBS volumes within the affected Availability Zone are experiencing degraded performance. We continue to work on resolving the issue. </div><div><span class=\"yellowfg\"> 7:15 PM PST</span>&nbsp;Between 5:33 PM and 7:06 PM PST some instances experienced connectivity issues within a single Availability Zone in the EU-WEST-2 Region. The affected instances did not lose power but became unreachable due to a loss of power to networking devices within the affected Availability Zone. Some EBS volumes experienced degraded performance during this time period and launches of new EC2 instances and the creation of new EBS volumes experienced increased error rates. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "AWS Lambda (London)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1547258876",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:08 PM PST</span>&nbsp;We are investigating increased error rates and elevated latencies for AWS Lambda requests in the EU-WEST-2 Region. Newly created functions and console editing is also affected.</div><div><span class=\"yellowfg\"> 7:19 PM PST</span>&nbsp;Between 5:33 PM and 7:12 PM PST Lambda experienced elevated error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally</div>",
      "service": "lambda-eu-west-2"
    },
    {
      "service_name": "Amazon Relational Database Service (London)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1547262903",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:15 PM PST</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 7:49 PM PST</span>&nbsp;Beginning at 5:33 PM PST, we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. Connectivity was restored at 7:11 PM PST, at which point the majority of database instances recovered. A small number of Single-AZ databases remain unavailable and we are continuing to work to recover all affected database instances.</div><div><span class=\"yellowfg\"> 8:08 PM PST</span>&nbsp;Between 5:33 PM PST and 7:11 PM PST, we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-eu-west-2"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Elevated Lambda@Edge errors",
      "date": "1547501684",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:34 PM PST</span>&nbsp;We are investigating elevated Lambda@Edge error rates that may be affecting some customers. CloudFront customers who do not have Lambda@Edge functions are unaffected by this issue. </div><div><span class=\"yellowfg\"> 2:11 PM PST</span>&nbsp;Between 11:40 AM PST and 2:07 PM PST, customers may have experienced elevated Lambda@Edge error rates. CloudFront customers who do not have Lambda@Edge functions were unaffected by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Relational Database Service (Ireland)",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1548423741",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:42 AM PST</span>&nbsp;Between 5:03 AM and 5:25 AM PST we experienced increased API error rates and latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-eu-west-1"
    },
    {
      "service_name": "Amazon Chime",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1548450005",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:00 PM PST</span>&nbsp;We are investigating contact search latency issues in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:17 PM PST</span>&nbsp;We are investigating latency in the ability to search contacts, and the inability to add contacts to rooms and calls in the US-EAST-1 Region. Existing users, rooms, and scheduled calls are not impacted.</div><div><span class=\"yellowfg\"> 2:42 PM PST</span>&nbsp;Between 11:38 AM and 2:19 PM PST, we experienced latency in the ability to search contacts, and the inability to add contacts to rooms and calls in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "chime"
    },
    {
      "service_name": "AWS Direct Connect (N. Virginia)",
      "summary": "[RESOLVED] Increased API errors and latencies",
      "date": "1548593864",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:57 AM PST</span>&nbsp;We are investigating increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:30 AM PST</span>&nbsp;We continue to investigate increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:06 AM PST</span>&nbsp;We can confirm increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 6:31 AM PST</span>&nbsp;Between 4:02 AM and 6:08 AM PST, we experienced increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region. Connectivity over existing Direct Connect connections was not affected, however, customers may have experienced delays using the Direct Connect console and APIs during this period. The issue has been resolved and the service is operating normally.</div>",
      "service": "directconnect-us-east-1"
    },
    {
      "service_name": "AWS Marketplace",
      "summary": "[RESOLVED] Increased Subscription Error Rates ",
      "date": "1549280664",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:44 AM PST</span>&nbsp;We are investigating increased AWS Marketplace subscription error rates.</div><div><span class=\"yellowfg\"> 4:34 AM PST</span>&nbsp;Between 1:36 AM and 4:07 AM PST we experienced increased AWS Marketplace subscription error rates. The issue has been resolved and the service is operating normally.</div>",
      "service": "marketplace"
    },
    {
      "service_name": "Amazon Relational Database Service (Oregon)",
      "summary": "[RESOLVED] Small Number of Amazon Aurora Instances Unavailable",
      "date": "1549679132",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:25 PM PST</span>&nbsp;We are investigating connectivity issues affecting a small number of Amazon Aurora instances in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 7:20 PM PST</span>&nbsp;The majority of database instances that experienced connectivity issues in the US-WEST-2 Region have recovered. We are continuing to work to resolve the instances still experiencing connectivity issues.</div><div><span class=\"yellowfg\"> 8:05 PM PST</span>&nbsp;Between 5:14 PM and 7:40 PM PST, we experienced connectivity issues affecting some Amazon Aurora instances in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-us-west-2"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased Error Rates and Latencies",
      "date": "1550029704",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:48 PM PST</span>&nbsp;We are investigating increased error rates and latencies for IAM APIs.</div><div><span class=\"yellowfg\"> 8:03 PM PST</span>&nbsp;Between 6:20 PM and 7:49 PM PST, we experienced increased error rates and latencies for IAM APIs. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "AWS Elastic Beanstalk (Sydney)",
      "summary": "[RESOLVED] Increased API Failure Rates",
      "date": "1550092874",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:21 PM PST</span>&nbsp;We are currently investigating increased error rates to Elastic Beanstalk APIs in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 2:11 PM PST</span>&nbsp;We can confirm increased error rates for the Elastic Beanstalk APIs in the AP-SOUTHEAST-2 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:31 PM PST</span>&nbsp;Between 12:39 PM and 2:12 PM PST we experienced elevated error rates in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-ap-southeast-2"
    },
    {
      "service_name": "Amazon Relational Database Service (Sydney)",
      "summary": "[RESOLVED] Increased Create Times and Cluster Unavailability",
      "date": "1550094163",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:42 PM PST</span>&nbsp;We are investigating increased database create times and cluster unavailability for Amazon Aurora in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 2:09 PM PST</span>&nbsp;We can confirm increased database create times and cluster unavailability for Amazon Aurora in the AP-SOUTHEAST-2 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:49 PM PST</span>&nbsp;We have resolved the issue resulting in increased database create times, and continue to work toward full resolution on Amazon Aurora Cluster Availability.</div><div><span class=\"yellowfg\"> 4:06 PM PST</span>&nbsp;We are beginning to see recovery for some Amazon Aurora Clusters and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 6:11 PM PST</span>&nbsp;Beginning at 11:54 AM PST some Amazon Aurora clusters experienced increased database create times and cluster unavailability in the AP-SOUTHEAST-2 Region. Elevated create times were resolved at 2:27 PM PST, at which point some existing clusters continued to experience availability issues. As of 5:35 PM PST both issues have been resolved and the service is operating normally. In total, the event impacted a little less than 3% of the Aurora databases in the region.</div>",
      "service": "rds-ap-southeast-2"
    },
    {
      "service_name": "Amazon Relational Database Service (Montreal)",
      "summary": "[RESOLVED] Increased Create Times and Cluster Unavailability",
      "date": "1550107156",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:19 PM PST</span>&nbsp;We wanted to provide an update for Amazon Aurora in the CA-CENTRAL-1 Region that we previously communicated to affected customers on their Personal Health Dashboards. Beginning at 11:43 AM PST some Amazon Aurora clusters experienced increased database create times and cluster unavailability in the CA-CENTRAL-1 Region. Elevated create times were resolved at 2:14 PM PST, at which point some existing clusters continued to experience availability issues. As of 4:48 PM PST both issues have been resolved and the service is operating normally.</div>",
      "service": "rds-ca-central-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased Change Propagation Time ",
      "date": "1550635211",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:00 PM PST</span>&nbsp;We are investigating increased propagation times of DNS edits to Route 53 DNS servers. Queries to existing DNS records are not affected by this issue and are being answered normally.</div><div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We continue to investigate increased propagation times of DNS edits to the Route 53 DNS servers. This issue will also affect provisioning of services such as EFS, ElasticBeanstalk, CloudFormation, and Amazon MQ. Queries to existing DNS records are not affected by this issue. </div><div><span class=\"yellowfg\"> 8:51 PM PST</span>&nbsp;Between 7:34 PM and 8:45 PM PST we experienced increased propagation times of DNS edits. Queries to existing DNS records were not affected by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Chime",
      "summary": "[RESOLVED] Availability ",
      "date": "1550790706",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:11 PM PST</span>&nbsp;We are investigating meeting and chat availability issues.</div><div><span class=\"yellowfg\"> 3:45 PM PST</span>&nbsp;We continue to investigate meeting and chat availability issues. </div><div><span class=\"yellowfg\"> 4:41 PM PST</span>&nbsp;We have identified the root cause of meeting and chat availability issues and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 5:19 PM PST</span>&nbsp;Between 2:51 PM and 5:04 PM PST we experienced meeting and chat availability issues. The issue has been resolved and the service is operating normally.</div>",
      "service": "chime"
    },
    {
      "service_name": "AWS Certificate Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1551126347",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:25 PM PST</span>&nbsp;We are experiencing increased error rates for AWS Certificate Manager in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:59 PM PST</span>&nbsp;We can confirm increased error rates for AWS Certificate Manager in the US-EAST-1 Region. We have identified the root cause and are working toward resolution.</div><div><span class=\"yellowfg\"> 1:50 PM PST</span>&nbsp;Between 11:48 AM and 1:26 PM PST, we experienced increased error rates for AWS Certificate Manager in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "certificatemanager-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation Delays",
      "date": "1551301596",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;We’re investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 1:30 PM PST</span>&nbsp;Between 10:09 AM and 1:23 PM PST customers may have experienced longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Container Service (Ireland)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1551383330",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:49 AM PST</span>&nbsp;We are investigating elevated API error rates in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\">12:30 PM PST</span>&nbsp;Between 11:05 AM and 12:05 PM PST we experienced elevated API error rates in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ecs-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased Launch Failures",
      "date": "1551977827",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:57 AM PST</span>&nbsp;Between 5:40 AM and 8:20 AM PST, new launches of EC2 instances were erroneously disabled in a single Availability Zone within the US-EAST-1 Region. This caused new launches to fail when targeting the affected Availability Zone and also resulted in health checks reporting instances in the affected Availability Zone as impaired. Customers with Auto Scaling Groups configured to replace instances on impaired EC2 health checks may have had instances replaced as a result of this issue. The Availability Zone has been re-enabled for new launches and Auto Scaling has automatically replaced affected instances. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1552347514",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:38 PM PDT</span>&nbsp;We are investigating increased errors when calling WorkSpaces APIs and connecting to WorkSpaces in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;Between 3:35 PM and 4:57 PM PDT we experienced increased API error rates and connection errors to WorkSpaces in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workspaces-us-east-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. Virginia)",
      "summary": "[RESOLVED] Latencies in DynamoDB control plane operations",
      "date": "1552426531",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:35 PM PDT</span>&nbsp;We are currently investigating increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:12 PM PDT</span>&nbsp;We have identified the cause of increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\"> 3:52 PM PDT</span>&nbsp;We continue to work on resolution for create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:05 PM PDT</span>&nbsp;New requests to create, update, and delete tables with Amazon DynamoDB are processing normally in the US-EAST-1 Region. We continue to work towards resolution for backlogged requests.</div><div><span class=\"yellowfg\"> 4:20 PM PDT</span>&nbsp;Between 1:15 PM and 4:14 PM PDT, we experienced increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "dynamodb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1552507000",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:56 PM PDT</span>&nbsp;We are investigating increased API error rates and launch failures in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 1:13 PM PDT</span>&nbsp;Between 12:29 PM and 12:56 PM PDT, we experienced increased API error rates and launch failures in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-us-west-2"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1553118849",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:54 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the AWS Management Console in the EU-WEST-3 Region.</div><div><span class=\"yellowfg\"> 3:18 PM PDT</span>&nbsp;Between 2:13 PM and 3:08 PM PDT we experienced increased error rates and latencies for the AWS Management Console in the EU-WEST-3 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Launch Failures",
      "date": "1553278711",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:18 AM PDT</span>&nbsp;We are investigating increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.</div><div><span class=\"yellowfg\">12:06 PM PDT</span>&nbsp;We continue to investigate increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.</div><div><span class=\"yellowfg\">12:44 PM PDT</span>&nbsp;We continue to investigate increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.</div><div><span class=\"yellowfg\"> 1:11 PM PDT</span>&nbsp;Between 9:28 AM and 12:52 PM PDT we experienced increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (Oregon)",
      "summary": "[RESOLVED] Delayed Metrics",
      "date": "1553710396",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:14 AM PDT</span>&nbsp;We are investigating increased delays for CloudWatch metrics in the US-WEST-2 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics.</div><div><span class=\"yellowfg\">11:33 AM PDT</span>&nbsp;We can confirm increased connection errors and latencies for CloudWatch APIs. Some CloudWatch metrics are delayed. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">12:30 PM PDT</span>&nbsp;We have identified the root cause of the increased connection errors and latencies for CloudWatch APIs, as well as the CloudWatch metric delays. We continue to work toward recovery.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;We are beginning to see recovery and continue to work toward full recovery. </div><div><span class=\"yellowfg\"> 1:03 PM PDT</span>&nbsp;Between 10:40 AM and 12:44 PM PDT we experienced increased connection errors and latencies for CloudWatch APIs. During this time CloudWatch alarms may have transitioned into \"INSUFFICIENT_DATA\" state if set on delayed metrics. The issue has been resolved and the service is operating normally. </div>",
      "service": "cloudwatch-us-west-2"
    },
    {
      "service_name": "AWS Certificate Manager (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1553712991",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:57 AM PDT</span>&nbsp;We are investigating increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. </div><div><span class=\"yellowfg\">12:34 PM PDT</span>&nbsp;We have identified the root cause of the increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. We are beginning to see recovery and continue to work toward full resolution.</div><div><span class=\"yellowfg\">12:50 PM PDT</span>&nbsp;Between 10:40 AM and 12:40 PM PDT we experienced increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "certificatemanager-us-west-2"
    },
    {
      "service_name": "Auto Scaling (Oregon)",
      "summary": "[RESOLVED] API Faults and Latencies",
      "date": "1553715184",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:33 PM PDT</span>&nbsp;We are investigating increased elevated faults and latencies across the DescribePolicies, PutScalingPolicy and RegisterScalableTarget APIs in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:54 PM PDT</span>&nbsp;Between 10:40 AM and 12:40 PM PDT we experienced increased elevated faults and latencies across the DescribePolicies, PutScalingPolicy and RegisterScalableTarget APIs in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "autoscaling-us-west-2"
    },
    {
      "service_name": "AWS Lambda (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1553757070",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:11 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-WEST-2 Region. </div><div><span class=\"yellowfg\">12:52 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API latencies and errors in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:21 AM PDT</span>&nbsp;We continue to see recovery in error rates for AWS Lambda API requests in the US-WEST-2 Region. Some customers may still experience errors in new function creation, updating existing functions and console editing. We continue to work towards full resolution. </div><div><span class=\"yellowfg\"> 2:46 AM PDT</span>&nbsp;Between March 27 11:20 PM and March 28 2:35 AM PDT, AWS Lambda experienced API latencies and errors in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "lambda-us-west-2"
    },
    {
      "service_name": "Amazon API Gateway (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1553757540",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:19 AM PDT</span>&nbsp;We are investigating increased error rates for Lambda integrations and Lambda Authorizers for invokes in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:58 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased error rates for Lambda integrations and Lambda Authorizers for invokes in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:36 AM PDT</span>&nbsp;Between March 27 11:20 PM and March 28 2:23 AM PDT customers using Lambda with API Gateway for Integrations and Authorizers experienced elevated API error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "apigateway-us-west-2"
    },
    {
      "service_name": "AWS Resource Access Manager (Oregon)",
      "summary": "[RESOLVED] API Latencies and Error Rate",
      "date": "1553757596",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:20 AM PDT</span>&nbsp;We are investigating increased API latencies and error rates in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:55 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API latencies and errors in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:36 AM PDT</span>&nbsp;\r\nBetween March 27 11:15 PM and March 28 1:49 AM PDT, we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ram-us-west-2"
    },
    {
      "service_name": "AWS Batch (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1553757962",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:26 AM PDT</span>&nbsp;We are investigating increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs are not affected.</div><div><span class=\"yellowfg\">12:57 AM PDT</span>&nbsp;We have identified the root cause of increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs are not affected.</div><div><span class=\"yellowfg\"> 2:14 AM PDT</span>&nbsp;We have identified the root cause of increased error rates for API calls in the US-WEST-2 Region impacting new API calls. Compute Resource connectivity and running jobs are not affected. We continue to work towards recovery. </div><div><span class=\"yellowfg\"> 2:39 AM PDT</span>&nbsp;Between March 27 11:21 PM and March 28 2:18 AM PDT, AWS Batch experienced increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally. </div>",
      "service": "batch-us-west-2"
    },
    {
      "service_name": "AWS AppSync (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1553758789",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:40 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-WEST-2 Region. </div><div><span class=\"yellowfg\"> 1:14 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API error rates in the US-WEST-2 Region and continue to work toward resolution. </div><div><span class=\"yellowfg\"> 2:40 AM PDT</span>&nbsp;Between March 27 11:20 PM and March 28 1:57 AM PDT, we experienced increased API error rates for the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "appsync-us-west-2"
    },
    {
      "service_name": "AWS Certificate Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1553876764",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:26 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:46 AM PDT</span>&nbsp;We continue to investigate increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;We have identified the root cause of the increased API error rates in the US-EAST-1 Region. We continue to work toward resolution.</div><div><span class=\"yellowfg\">11:28 AM PDT</span>&nbsp;We are beginning to see recovery for the increased API error rates in the US-EAST-1 Region. We continue to work toward full recovery.</div><div><span class=\"yellowfg\">11:53 AM PDT</span>&nbsp;Between 8:41 AM and 11:10 AM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "certificatemanager-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation Delays",
      "date": "1553885279",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:48 AM PDT</span>&nbsp;We’re investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">12:52 PM PDT</span>&nbsp;We have identified the root cause of the longer than usual propagation times for changes to CloudFront configurations. We continue to work toward resolution. </div><div><span class=\"yellowfg\"> 1:18 PM PDT</span>&nbsp;Between 8:50 AM and 12:59 PM PDT, we experienced longer than usual propagation times for changes to CloudFront configurations. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "AWS Lambda (Ireland)",
      "summary": "[RESOLVED] Increased Async API Latencies",
      "date": "1554044749",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:06 AM PDT</span>&nbsp;We are investigating increased Async API latencies in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:41 AM PDT</span>&nbsp;We have identified the root cause of increased Async API latencies in the EU-WEST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 9:32 AM PDT</span>&nbsp;Between 6:59 AM and 8:56 AM PDT we experienced increased latencies for the Lambda Async API in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-eu-west-1"
    },
    {
      "service_name": "AWS IoT Core (Ireland)",
      "summary": "[RESOLVED] Increased API Errors and Latencies",
      "date": "1554201606",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:40 AM PDT</span>&nbsp;We are investigating increased API error rates when establishing new connections to AWS IoT Core in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 4:23 AM PDT</span>&nbsp;Between 1:52 AM and 3:35 AM PDT we experienced increased API error rates and latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "awsiot-eu-west-1"
    },
    {
      "service_name": "Amazon Chime",
      "summary": "[RESOLVED] Authentication Availability",
      "date": "1554263486",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:51 PM PDT</span>&nbsp;We are investigating availability issues with authenticating using Active Directory in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:27 PM PDT</span>&nbsp;Between 6:30 PM and 9:10 PM PDT we experienced increased Active Directory authentication failures in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "chime"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (US-East)",
      "summary": "[RESOLVED] Network Connectivity ",
      "date": "1554336479",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:08 PM PDT</span>&nbsp;We are investigating internet connectivity issues in the US-GOV-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:27 PM PDT</span>&nbsp;We can confirm Internet and EC2 Public IP address connectivity issues for newly launched EC2 instances within the US-GOV-EAST-1 Region. Connectivity for existing instances is not affected.</div><div><span class=\"yellowfg\"> 5:59 PM PDT</span>&nbsp;Between 3:29 PM and 5:50 PM PDT some newly launched instances experienced Internet and EC2 Public IP address connectivity issues in the US-GOV-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-gov-east-1"
    },
    {
      "service_name": "AWS CodeBuild (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates ",
      "date": "1554742713",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:58 AM PDT</span>&nbsp;We are investigating increased API and Build error rates in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for the CodeBuild Management Console and Build APIs in the US-EAST-1 Region and continue to work towards resolution. </div><div><span class=\"yellowfg\">10:34 AM PDT</span>&nbsp;Between 8:35 AM and 10:05 AM PDT we experienced increased error rates for the CodeBuild Management Console and Build APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "codebuild-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1554964842",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:40 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the ELB APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">Apr 11, 12:07 AM PDT</span>&nbsp;Between 11:13 PM and 11:54 PM PDT we experienced increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Elevated Route 53 API error rates ",
      "date": "1555178246",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:57 AM PDT</span>&nbsp;We are currently investigating elevated error rates for some Route 53 API calls in the US-EAST-1 Region. There is no impact to answering DNS queries and they continue to work normally.</div><div><span class=\"yellowfg\">11:22 AM PDT</span>&nbsp;Between 10:13 AM and 10:58 AM PDT we experienced intermittent elevated error rates for some Route 53 API calls in the US-EAST-1 Region. There was no impact to answering DNS queries throughout this time. The issue has been resolved and the service is operating normally</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sao Paulo)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1555527672",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:01 PM PDT</span>&nbsp;We are investigating increased RunInstance API error rates in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\">12:21 PM PDT</span>&nbsp;Between 11:30 AM and 12:09 PM PDT we experienced increased RunInstance API error rates and latencies in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-sa-east-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased Billing Console Error Rates",
      "date": "1555578361",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:06 AM PDT</span>&nbsp;We are investigating increased error rates in the Billing Console.</div><div><span class=\"yellowfg\"> 2:56 AM PDT</span>&nbsp;We continue to investigate intermittent availability and increased error rates in the Billing Console.</div><div><span class=\"yellowfg\"> 3:53 AM PDT</span>&nbsp;We continue to investigate intermittent availability and increased error rates in the Billing Console and are working towards resolution. </div><div><span class=\"yellowfg\"> 4:45 AM PDT</span>&nbsp;We have identified the cause of the intermittent availability issues and increased error rates in the Billing Console and continue working towards resolution.</div><div><span class=\"yellowfg\"> 5:17 AM PDT</span>&nbsp;Between 2:06 AM and 5:00 AM PDT we experienced intermittent availability issues and increased error rates in the Billing Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "AWS Management Console (US-East)",
      "summary": "Increased Console Error Rates",
      "date": "1556210519",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:20 AM PDT</span>&nbsp;We are investigating increased error rates when loading the AWS Management Console.</div>",
      "service": "management-console-us-gov-east-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Console Error Rates",
      "date": "1556210754",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:50 AM PDT</span>&nbsp;We are currently investigating increased errors for loading the AWS Management Console. Alternate link is available at: <a href=\"https://us-west-2.console.aws.amazon.com/ec2/v2/home\">https://us-west-2.console.aws.amazon.com/ec2/v2/home</a></div><div><span class=\"yellowfg\">10:10 AM PDT</span>&nbsp;We can confirm impact to the AWS Management Console in US-EAST-1 specific to the /home link. AWS services are not impacted, and access to service-specific consoles is working normally. Please use alternate link: <a href=\"https://us-west-2.console.aws.amazon.com/ec2/v2/home\">https://us-west-2.console.aws.amazon.com/ec2/v2/home</a></div><div><span class=\"yellowfg\">11:28 AM PDT</span>&nbsp;We continue to work towards resolution in US-EAST-1 for issues specific to the /home Management Console link. AWS services are not impacted, and access to service-specific consoles is working normally. Please use alternate link: <a href=\"https://us-east-1.console.aws.amazon.com/ec2/v2/home\">https://us-east-1.console.aws.amazon.com/ec2/v2/home</a> and select a service from the Services menu above.</div><div><span class=\"yellowfg\">12:18 PM PDT</span>&nbsp;Between 9:11 AM and 12:13 PM PDT we experienced increased error rates for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "management-console"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Single Sign-On",
      "date": "1556215997",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:13 AM PDT</span>&nbsp;We can confirm impact to the AWS SSO user portal specific to single-sign on to the AWS Management Console. SAML assertions are not impacted but users may be directed to a website unavailable page when authenticating with SAML. Please use alternate link: <a href=\"https://us-west-2.console.aws.amazon.com/ec2/v2/home\">https://us-west-2.console.aws.amazon.com/ec2/v2/home</a> after authenticating. There is no impact to application sign-in.</div><div><span class=\"yellowfg\">11:43 AM PDT</span>&nbsp;We continue to work towards resolution for issues specific to the AWS Management Console. For SSO customers who are seeing an error message while attempting to sign into AWS console through the AWS SSO User Portal, clicking on “login again” provides access</div><div><span class=\"yellowfg\">12:18 PM PDT</span>&nbsp;Between 9:11 AM and 12:13 PM PDT we experienced increased error rates for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (N. Virginia)",
      "summary": "[RESOLVED]  Increased Error Rates ",
      "date": "1556557390",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:03 AM PDT</span>&nbsp;We are investigating increased error rates for Amazon S3 GET and PUT requests in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:32 AM PDT</span>&nbsp;We have identified the cause of the increased error rates in the US-EAST-1 Region and are working towards resolution.</div><div><span class=\"yellowfg\">10:42 AM PDT</span>&nbsp;Between 9:18 AM and 10:18 AM PDT we experienced slightly increased GET and PUT error rates for Amazon S3 requests in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-standard"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1556576555",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:22 PM PDT</span>&nbsp;We are investigating increased error rates for requests served by certain edge locations in South East Asia.</div><div><span class=\"yellowfg\"> 3:59 PM PDT</span>&nbsp;Between 2:16 PM and 3:43 PM PDT we experienced increased error rates for requests served by edge locations in South East Asia. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sydney)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1556589414",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:57 PM PDT</span>&nbsp;We are investigating network connectivity issues for some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 7:35 PM PDT</span>&nbsp;We can confirm network connectivity issues for some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region. Connectivity for affected instances can be restored using a stop/start command from the EC2 Management Console or EC2 API. We continue to work towards restoring network connectivity for the affected instances.</div><div><span class=\"yellowfg\"> 8:36 PM PDT</span>&nbsp;We have resolved the network connectivity issue affecting some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region. For instances that are still experiencing connectivity issues, we recommend issuing a reboot or stop/start command from the EC2 Management Console or EC2 API. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Hong Kong)",
      "summary": "[RESOLVED] EC2 Recursive DNS Resolution Issues",
      "date": "1556669136",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:06 PM PDT</span>&nbsp;We are investigating DNS resolution issues in a single Availability Zone in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:23 PM PDT</span>&nbsp;We are investigating DNS resolution issues, increased API error rates, and error rates for new launches in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:04 PM PDT</span>&nbsp;Between 4:39 PM and 5:50 PM PDT we experienced DNS resolution issues, increased API error rates, impaired instances, and error rates for new launches in the AP-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-east-1"
    },
    {
      "service_name": "Amazon Simple Queue Service (Ohio)",
      "summary": "[RESOLVED]  Increased Error Rates ",
      "date": "1556905071",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;We are investigating increased error rates in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:18 AM PDT</span>&nbsp;Between 10:14 AM and 11:09 AM PDT, we experienced elevated API error rates in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",
      "service": "sqs-us-east-2"
    },
    {
      "service_name": "AWS Lambda (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1556906015",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:53 AM PDT</span>&nbsp;We are investigating increased error rates and latency in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:54 AM PDT</span>&nbsp;We have identified the cause of increased error rates and latency in the US-EAST-2 Region and are working towards resolution. </div><div><span class=\"yellowfg\">12:28 PM PDT</span>&nbsp;Between 10:14 AM and 12:25 PM PDT, we experienced elevated API error rates in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",
      "service": "lambda-us-east-2"
    },
    {
      "service_name": "Amazon CloudWatch (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1556906325",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:58 AM PDT</span>&nbsp;We are investigating increased error rates and delays for CloudWatch Logs Insights queries in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:15 AM PDT</span>&nbsp;Between 10:14 AM and 11:09 AM PDT, we experienced elevated error rates and delays when running CloudWatch Logs Insights queries in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",
      "service": "cloudwatch-us-east-2"
    },
    {
      "service_name": "AWS IoT Core (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1556906633",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:04 AM PDT</span>&nbsp;AWS IoT Core is investigating increased error rates and latency in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:24 AM PDT</span>&nbsp;Between 10:14 AM and 11:09 AM PDT, we experienced elevated error rates and increased latency when trying to Connect to and Publish Messages on AWS IoT Core in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",
      "service": "awsiot-us-east-2"
    },
    {
      "service_name": "AWS Elastic Beanstalk (Ireland)",
      "summary": "[RESOLVED] Increased API Error Rates ",
      "date": "1557476843",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:27 AM PDT</span>&nbsp;We are currently investigating increased error rates for the Elastic Beanstalk APIs in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:15 AM PDT</span>&nbsp;We have identified the root cause of increased error rates to Elastic Beanstalk APIs in the EU-WEST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 2:27 AM PDT</span>&nbsp;Between 12:43 AM and 1:24 AM PDT, we experienced increased error rates in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-eu-west-1"
    },
    {
      "service_name": "Amazon Relational Database Service (Ireland)",
      "summary": "[RESOLVED] Increased API Error Rates and Connectivity Issues",
      "date": "1557477953",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:46 AM PDT</span>&nbsp;We are investigating increased API error rates in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:14 AM PDT</span>&nbsp;We are investigating increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:34 AM PDT</span>&nbsp;We have identified the root cause of increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 4:09 AM PDT</span>&nbsp;Beginning at 12:43 AM PDT we experienced increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region. Elevated API error rates were resolved at 1:33 AM PDT, at which point some existing clusters continued to experience connectivity issues. As of 3:50 AM PDT both issues have been resolved and the service is operating normally.</div>",
      "service": "rds-eu-west-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Change Propagation Delays & API Errors",
      "date": "1557512464",
      "status": 2,
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:21 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\">11:28 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\">12:06 PM PDT</span>&nbsp;We continue to investigate increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\">12:35 PM PDT</span>&nbsp;We have identified the root cause and continue to work toward recovery. This may also impact provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\"> 2:01 PM PDT</span>&nbsp;DNS updates are being accepted by the Route 53 API and Console, however an error is preventing the updates from being propagated from the Route53 API system to the live DNS servers. We have identified the root cause of this propagation error and are actively working towards resolution. This issue is impacting provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to DNS records present at the beginning of this issue are not affected and are being answered normally.</div><div><span class=\"yellowfg\"> 4:00 PM PDT</span>&nbsp;Some DNS changes are now propagating to the live DNS servers.  We continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 5:18 PM PDT</span>&nbsp;We are observing steady recovery, with the oldest changes propagating first. We are continuing to work through the backlog of queued changes.</div><div><span class=\"yellowfg\"> 6:30 PM PDT</span>&nbsp;The backlog of queued changes continues to propagate. We have taken steps to accelerate recovery.</div><div><span class=\"yellowfg\"> 7:35 PM PDT</span>&nbsp;We have now propagated more than half of the backlog of queued changes and are continuing to work on the remainder.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We have processed the majority of the backlog of queued changes for Route 53 Public DNS. Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers may receive an error for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">10:08 PM PDT</span>&nbsp;As of 8:34 PM PDT, we have resolved the issue resulting in Route 53 Public DNS propagation delays. At this time, Route 53 Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers will continue to receive errors for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone.</div><div><span class=\"yellowfg\">11:51 PM PDT</span>&nbsp;We continue to make progress towards restoring propagation for Route 53 Private DNS changes and resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls. A majority of our hosts have processed all updates, and we are working to accelerate recovery of the remaining hosts.</div><div><span class=\"yellowfg\">May 11,  1:41 AM PDT</span>&nbsp;We have restored propagation for Route 53 Private DNS changes. We continue to work on resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls.</div><div><span class=\"yellowfg\">May 11,  2:39 AM PDT</span>&nbsp;Between May 10 10:50 AM and May 11 2:18 AM PDT we experienced increased change propagation latency and elevated error rates for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone and DisassociateVPCFromHostedZone calls to the Route 53 API. During this entire duration there were no issues serving DNS queries. The issue has been resolved and the service is now operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Change Propagation Delays & API Errors",
      "date": "1557569326",
      "status": 1,
      "details": "",
      "description": "<div><span class=\"yellowfg\">May 10, 11:21 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\">May 10, 11:28 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\">May 10, 12:06 PM PDT</span>&nbsp;We continue to investigate increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\">May 10, 12:35 PM PDT</span>&nbsp;We have identified the root cause and continue to work toward recovery. This may also impact provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\">May 10,  2:01 PM PDT</span>&nbsp;DNS updates are being accepted by the Route 53 API and Console, however an error is preventing the updates from being propagated from the Route53 API system to the live DNS servers. We have identified the root cause of this propagation error and are actively working towards resolution. This issue is impacting provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to DNS records present at the beginning of this issue are not affected and are being answered normally.</div><div><span class=\"yellowfg\">May 10,  4:00 PM PDT</span>&nbsp;Some DNS changes are now propagating to the live DNS servers.  We continue to work towards full recovery.</div><div><span class=\"yellowfg\">May 10,  5:18 PM PDT</span>&nbsp;We are observing steady recovery, with the oldest changes propagating first. We are continuing to work through the backlog of queued changes.</div><div><span class=\"yellowfg\">May 10,  6:30 PM PDT</span>&nbsp;The backlog of queued changes continues to propagate. We have taken steps to accelerate recovery.</div><div><span class=\"yellowfg\">May 10,  7:35 PM PDT</span>&nbsp;We have now propagated more than half of the backlog of queued changes and are continuing to work on the remainder.</div><div><span class=\"yellowfg\">May 10,  8:23 PM PDT</span>&nbsp;We have processed the majority of the backlog of queued changes for Route 53 Public DNS. Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers may receive an error for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">May 10, 10:08 PM PDT</span>&nbsp;As of 8:34 PM PDT, we have resolved the issue resulting in Route 53 Public DNS propagation delays. At this time, Route 53 Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers will continue to receive errors for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone.</div><div><span class=\"yellowfg\">May 10, 11:51 PM PDT</span>&nbsp;We continue to make progress towards restoring propagation for Route 53 Private DNS changes and resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls. A majority of our hosts have processed all updates, and we are working to accelerate recovery of the remaining hosts.</div><div><span class=\"yellowfg\"> 1:41 AM PDT</span>&nbsp;We have restored propagation for Route 53 Private DNS changes. We continue to work on resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls.</div><div><span class=\"yellowfg\"> 2:39 AM PDT</span>&nbsp;Between May 10 10:50 AM and May 11 2:18 AM PDT we experienced increased change propagation latency and elevated error rates for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone and DisassociateVPCFromHostedZone calls to the Route 53 API. During this entire duration there were no issues serving DNS queries. The issue has been resolved and the service is now operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Call Handling Experience",
      "date": "1558724798",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:06 PM PDT</span>&nbsp;Between 10:40 AM and 11:04 AM PDT we experienced failures in call handling operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon Relational Database Service (N. Virginia)",
      "summary": "[RESOLVED] Increased API Latencies and Management Console Error Rates",
      "date": "1560790279",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:51 AM PDT</span>&nbsp;We are investigating increased API latencies and increased error rates for the RDS Management Console in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:09 AM PDT</span>&nbsp;Between 8:21 AM and 10:01 AM PDT, we experienced increased API latencies and increased error rates for the RDS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-us-east-1"
    },
    {
      "service_name": "Amazon Virtual Private Cloud (Singapore)",
      "summary": "[RESOLVED] AWS Site-to-Site VPN Connectivity Issues",
      "date": "1561413948",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:05 PM PDT</span>&nbsp;We are investigating network connectivity issues affecting some AWS Site-to-Site VPN customers in the AP-SOUTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 3:24 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity for some AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=\"yellowfg\"> 4:11 PM PDT</span>&nbsp;We can confirm network connectivity issues for some AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region and continue to work towards resolution. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=\"yellowfg\"> 4:37 PM PDT</span>&nbsp;We can confirm network connectivity issues for existing AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region and are starting to see recovery. Newly created AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region are not affected by this issue. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=\"yellowfg\"> 5:52 PM PDT</span>&nbsp;We continue to work towards full recovery of the network connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=\"yellowfg\"> 7:07 PM PDT</span>&nbsp;We continue to work towards full recovery of the network connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=\"yellowfg\"> 9:08 PM PDT</span>&nbsp;Some VPN connections have been restored. We continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 9:54 PM PDT</span>&nbsp;We have now restored connectivity for the majority of the primary tunnels associated with all affected VPN connections in the AP-SOUTHEAST-1 Region. At this stage, VPN connections should no longer be experiencing connectivity issues. We continue to work on restoring connectivity for all secondary tunnels.</div><div><span class=\"yellowfg\">10:35 PM PDT</span>&nbsp;While the majority of VPN tunnels are once again able to establish a connection, route updates are not yet propagating for all affected VPN connections in the AP-SOUTHEAST-1 Region. For affected tunnels, this will result in network connectivity issues which we continue to work on resolving. </div><div><span class=\"yellowfg\">Jun 25,  1:34 AM PDT</span>&nbsp;We continue to make progress in restoring network connectivity for VPN connections in the AP-SOUTHEAST-1 Region. We are also seeing an improvement in route propagation times as we work towards full recovery.</div><div><span class=\"yellowfg\">Jun 25,  6:32 AM PDT</span>&nbsp;We have resolved the connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect was not affected by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "vpc-ap-southeast-1"
    },
    {
      "service_name": "AWS IoT Analytics (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latency",
      "date": "1561652460",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:32 PM PDT</span>&nbsp;We can confirm increased error rates and latencies in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:33 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=\"yellowfg\"> 2:45 PM PDT</span>&nbsp;Between 9:21 AM and 2:27 PM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "iotanalytics-us-east-1"
    },
    {
      "service_name": "AWS Glue (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1561655416",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:10 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:34 AM PDT</span>&nbsp;We can confirm increased API error rates in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution. </div><div><span class=\"yellowfg\">11:52 AM PDT</span>&nbsp;We are beginning to see intermittent recovery for Glue and continue to work toward full recovery.</div><div><span class=\"yellowfg\"> 1:04 PM PDT</span>&nbsp;We want to give you more information on the issue affecting AWS Glue. Glue Workflow APIs, Orchestration APIs, and ETL jobs that do not require the AWS Glue Data Catalog APIs continue to operate normally. The issue with the Data Catalog APIs started with a software update in the US-EAST-1 Region that completed at 9:21 AM PDT. The software update was immediately rolled back, and we are now working towards stabilizing the Data Catalog API subsystem. We continue to work towards a full recovery.</div><div><span class=\"yellowfg\"> 1:17 PM PDT</span>&nbsp;As we work toward resolution we are temporarily suspending traffic for the Data Catalog APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:06 PM PDT</span>&nbsp;We are beginning to work on removing throttles and restoring API availability but are proceeding cautiously.</div><div><span class=\"yellowfg\"> 2:23 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=\"yellowfg\"> 2:42 PM PDT</span>&nbsp;Between 9:21 AM and 2:06 PM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "glue-us-east-1"
    },
    {
      "service_name": "Amazon Athena (N. Virginia)",
      "summary": "[RESOLVED] Increased Query Failures and Latency",
      "date": "1561655746",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:15 AM PDT</span>&nbsp;We are investigating increased query failures and latency in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:37 AM PDT</span>&nbsp;We can confirm increased query failures and latency in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:01 AM PDT</span>&nbsp;We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\">11:52 AM PDT</span>&nbsp;We are beginning to see signs of recovery for Athena and continue to work toward full recovery.</div><div><span class=\"yellowfg\">12:32 PM PDT</span>&nbsp;We continue to experience increased error rates, as Athena depends on the Glue Data Catalog APIs. We will be throttling the Athena APIs as we work toward recovery. Customers may receive a \"Rate Exceeded\" error as recovery progresses.</div><div><span class=\"yellowfg\"> 1:35 PM PDT</span>&nbsp;We continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:14 PM PDT</span>&nbsp;We are beginning to work on removing throttles and restoring query availability but are proceeding cautiously.</div><div><span class=\"yellowfg\"> 2:32 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=\"yellowfg\"> 2:43 PM PDT</span>&nbsp;Between 9:21 AM and 2:36 PM PDT we experienced increased query failures and latency in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "athena-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1561658605",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:03 AM PDT</span>&nbsp;We are investigating increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:04 PM PDT</span>&nbsp;We can confirm increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 1:09 PM PDT</span>&nbsp;We continue to work toward resolution.</div><div><span class=\"yellowfg\"> 2:33 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=\"yellowfg\"> 2:45 PM PDT</span>&nbsp;Between 9:21 AM and 2:06 PM PDT we experienced increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1561864621",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 9:23 PM PDT</span>&nbsp;We continue to investigate the increased error rates and latencies for the EC2 APIs in the EU-WEST-2 Region. The health of existing EC2 instances is not affected.</div><div><span class=\"yellowfg\">10:00 PM PDT</span>&nbsp;Between 7:40 PM and 9:35 PM PDT we experienced increased API error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (London)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1561866166",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:42 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 9:51 PM PDT</span>&nbsp;We have identified the root cause of the increase in error rates and latencies for the ELB APIs in the EU-WEST-2 Region and continue to work towards full recovery.</div><div><span class=\"yellowfg\">10:45 PM PDT</span>&nbsp;Between 8:25 PM and 10:15 PM PDT, we experienced increased API error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-eu-west-2"
    },
    {
      "service_name": "Amazon CloudWatch (London)",
      "summary": "[RESOLVED] Elevated API faults and alarm delays",
      "date": "1561867337",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:02 PM PDT</span>&nbsp;We are investigating increased faults for CloudWatch alarms APIs and delays in processing some alarms in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 9:55 PM PDT</span>&nbsp;We can confirm an elevated rate of CloudWatch Alarms API faults and delays in processing some alarms in EU-WEST-2 Region and continue to work towards full recovery.</div><div><span class=\"yellowfg\">10:04 PM PDT</span>&nbsp;Between 8:27 PM and 9:55 PM PDT, customers may have experienced elevated alarms API faults and delayed alarms in the EU-WEST-2 Region. We have resolved the issue and the service is operating normally.</div>",
      "service": "cloudwatch-eu-west-2"
    },
    {
      "service_name": "Amazon Simple Workflow Service (London)",
      "summary": "[RESOLVED] Elevated error rates for Simple Workflow",
      "date": "1561871879",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:18 PM PDT</span>&nbsp;We are experiencing elevated API failures for AWS Simple Workflow in the EU-WEST-2 Region and continue to work towards full recovery.</div><div><span class=\"yellowfg\">10:55 PM PDT</span>&nbsp;Between 9:15 PM and 10:30 PM PDT we experienced elevated API failures for AWS Simple Workflow in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "swf-eu-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (US-East)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1562900564",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:02 PM PDT</span>&nbsp;We are investigating increased error rates for the EC2 APIs and new instance launches in the US-GOV-EAST-1 Region. Existing instances are unaffected. </div><div><span class=\"yellowfg\"> 8:35 PM PDT</span>&nbsp;We can confirm increased error rates for the CreateVolume and CreateSnapshot EBS APIs in the US-GOV-EAST-1 Region. This may impact the launching of new EBS backed EC2 instances. Existing instances are unaffected. </div><div><span class=\"yellowfg\"> 9:00 PM PDT</span>&nbsp;Between 7:07 PM and 8:47 PM PDT we experienced increased error rates for the CreateVolume and CreateSnapshot EBS APIs in the US-GOV-EAST-1 Region, which also impacted the launching of new EBS backed EC2 instances. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-gov-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] EBS Volumes Degraded Performance",
      "date": "1563902591",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;We are investigating degraded performance for EBS volumes in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\">11:11 AM PDT</span>&nbsp;We have determined root cause of the degraded performance for EBS volumes in a single Availability Zone in the EU-WEST-2 Region. The degraded performance has been resolved for some of the affected volumes and we continue to work towards full recovery.</div><div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;Between 9:30 AM and 11:37 AM PDT we experienced degraded performance for some EBS volumes in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "Amazon Relational Database Service (London)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1563904324",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:52 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\">11:26 AM PDT</span>&nbsp;We have determined root cause for the connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The connectivity issues have been resolved for some database instances and we continue to work towards full recovery.</div><div><span class=\"yellowfg\">11:51 AM PDT</span>&nbsp;Between 9:30 AM and 11:40 AM PDT we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-eu-west-2"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Increased Call Failures",
      "date": "1566235755",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:29 AM PDT</span>&nbsp;We are investigating call failures and issues accessing Amazon Connect in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:58 AM PDT</span>&nbsp;Call handling has recovered for agents who can access Amazon Connect in the US-EAST-1 Region. We continue to investigate problems accessing the Amazon Connect Console.</div><div><span class=\"yellowfg\">11:46 AM PDT</span>&nbsp;Between 10:02 AM and 11:13 AM PDT, some Amazon Connect users experienced issues logging in or performing actions in the Connect application in the US-EAST-1 Region. Some calls may have failed during this time. The issue has been resolved and the service is operating normally.</div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies ",
      "date": "1566264383",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:26 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies for the EC2 APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:48 PM PDT</span>&nbsp;Between 6:00 PM and 6:41 PM PDT we experienced increased API error rates and latencies for the EC2 APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Tokyo)",
      "summary": "インスタンスの接続性について | Instance Availability",
      "date": "1566533746",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:18 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 9:47 PM PDT</span>&nbsp;We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the AP-NORTHEAST-1 Region. Some EC2 APIs are also experiencing increased error rates and latencies. We are working to resolve the issue.</div><div><span class=\"yellowfg\">10:27 PM PDT</span>&nbsp;We have identified the root cause and are working toward recovery for the instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\">11:40 PM PDT</span>&nbsp;We are starting to see recovery for instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances and EBS volumes.</div><div><span class=\"yellowfg\">Aug 23,  1:54 AM PDT</span>&nbsp;Recovery is in progress for instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances and EBS volumes.</div><div><span class=\"yellowfg\">Aug 23,  2:39 AM PDT</span>&nbsp;The majority of impaired EC2 instances and EBS volumes experiencing degraded performance have now recovered. We continue to work on recovery for the remaining EC2 instances and EBS volumes that are affected by this issue. This issue affects EC2 instances and EBS volumes in a single Availability Zone in the AP-NORTHEAST-1 region.</div><div><span class=\"yellowfg\">Aug 23,  4:18 AM PDT</span>&nbsp;日本時間 2019年8月23日 12:36 より、AP-NORTHEAST-1 の単一のアベイラビリティゾーンで、一定の割合の EC2 サーバのオーバーヒートが発生しました。この結果、当該アベイラビリティゾーンの EC2 インスタンス及び EBS ボリュームのパフォーマンスの劣化が発生しました。このオーバーヒートは、影響を受けたアベイラビリティゾーン中の一部の冗長化された空調設備の管理システム障害が原因です。日本時間 15:21 に冷却装置は復旧し、室温が通常状態に戻り始めました。温度が通常状態に戻ったことで、影響を受けたインスタンスの電源が回復しました。日本時間 18:30 より大部分の EC2 インスタンスと EBS ボリュームは回復しました。 我々は残りの EC2 インスタンスと EBS ボリュームの回復に取り組んでいます。少数の EC2 インスタンスと EBS ボリュームが電源が落ちたハードウェア ホスト上に残されています。我々は影響をうけた全ての EC2 インスタンスと EBS ボリュームの回復のための作業を継続しています。 早期回復の為、可能な場合残された影響を受けている EC2 インスタンスと EBS ボリュームのリプレースを推奨します。いくつかの影響をうけた EC2 インスタンスはお客様側での作業が必要になる可能性がある為、 後ほどお客様個別にお知らせすることを予定しています。<br><br>詳細は <a href=\"https://aws.amazon.com/message/56489/\">こちら</a> をご参照ください。追加のご質問がある場合は、<a href=\"https://aws.amazon.com/support\">AWS サポート</a>までご連絡ください。 | Beginning at 8:36 PM PDT a small percentage of EC2 servers in a single Availability Zone in the AP-NORTHEAST-1 Region shutdown due to overheating. This resulted in impaired EC2 instances and degraded EBS volume performance for resources in the affected area of the Availability Zone. The overheating was caused by a control system failure that caused multiple, redundant cooling systems to fail in parts of the affected Availability Zone. The chillers were restored at 11:21 PM PDT and temperatures in the affected areas began to return to normal. As temperatures returned to normal, power was restored to the affected instances. By 2:30 AM PDT, the vast majority of instances and volumes had recovered. We have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. Some of the affected instances may require action from customers and we will be reaching out to those customers with next steps.<br><br>Additional details are available <a href=\"https://aws.amazon.com/message/56489/\">here</a>. If you have additional questions, please contact <a href=\"https://aws.amazon.com/support\">AWS Support</a>.</div>",
      "service": "ec2-ap-northeast-1"
    },
    {
      "service_name": "Amazon Relational Database Service (Tokyo)",
      "summary": "インスタンスの接続性について | Instance Availability",
      "date": "1566537770",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:22 PM PDT</span>&nbsp;AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで発生している、複数インスタンスに対する接続性の問題について調査を進めております。| We are investigating connectivity issues affecting some instances in a single Availability Zone in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\">11:25 PM PDT</span>&nbsp;AWSでは、東京リージョンの1つのアベイラビリティゾーンで発生しているインスタンスの接続性の問題について原因を特定し、現在復旧に向けて対応を進めております。| We have identified the root cause of instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region and are working toward recovery.</div><div><span class=\"yellowfg\">Aug 23, 12:01 AM PDT</span>&nbsp;AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで発生しているインスタンスの接続性の問題ついて、復旧を開始しております。影響を受けている全てのインスタンスの復旧に向け、対応を継続いたします。| We are starting to see recovery for instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances.</div><div><span class=\"yellowfg\">Aug 23,  2:16 AM PDT</span>&nbsp;AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで接続性の問題が生じている全てのインスタンスの復旧に向け、対応を進めております。| We continue to see recovery for instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region and are working towards recovery for all affected instances.</div><div><span class=\"yellowfg\">Aug 23,  6:19 AM PDT</span>&nbsp;日本時間 2019年8月23日 12:36 から 22:05 にかけて、東京リージョンの単一のアベイラビリティゾーンで一部の RDS インスタンスに接続性の問題が発生しました。現在、この問題は解消しており、サービスは正常稼働しております。<br><br>詳細は <a href=\"https://aws.amazon.com/message/56489/\">こちら</a> をご参照ください。追加のご質問がある場合は、<a href=\"https://aws.amazon.com/support\">AWS サポート</a>までご連絡ください。| Between August 22 8:36 PM and August 23 6:05 AM PDT, some RDS instances experienced connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. <br><br>Additional details are available <a href=\"https://aws.amazon.com/message/56489/\">here</a>. If you have additional questions, please contact <a href=\"https://aws.amazon.com/support\">AWS Support</a>.</div>",
      "service": "rds-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Instances Unavailable",
      "date": "1567257765",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:22 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:54 AM PDT</span>&nbsp;We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the US-EAST-1 Region. Some EC2 APIs are also experiencing increased error rates and latencies. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:37 AM PDT</span>&nbsp;We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the US-EAST-1 Region. We are investigating increased error rates for new launches within the same Availability Zone. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 8:06 AM PDT</span>&nbsp;We are starting to see recovery for instance impairments and degraded EBS volume performance within a single Availability Zone in the US-EAST-1 Region. We are also starting to see recovery of EC2 APIs. We continue to work towards recovery for all affected EC2 instances and EBS volumes.</div><div><span class=\"yellowfg\"> 9:04 AM PDT</span>&nbsp;Recovery is in progress for instance impairments and degraded EBS volume performance within a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all remaining affected instances and EBS volumes. </div><div><span class=\"yellowfg\">10:47 AM PDT</span>&nbsp;We want to give you more information on progress at this point, and what we know about the event. At 4:33 AM PDT one of 10 datacenters in one of the 6 Availability Zones in the US-EAST-1 Region saw a failure of utility power. Backup generators came online immediately, but for reasons we are still investigating, began quickly failing at around 6:00 AM PDT. This resulted in 7.5% of all instances in that Availability Zone failing by 6:10 AM PDT. Over the last few hours we have recovered most instances but still have 1.5% of the instances in that Availability Zone remaining to be recovered. Similar impact existed to EBS and we continue to recover volumes within EBS. New instance launches in this zone continue to work without issue.</div><div><span class=\"yellowfg\"> 1:30 PM PDT</span>&nbsp;At 4:33 AM PDT one of ten data centers in one of the six Availability Zones in the US-EAST-1 Region saw a failure of utility power. Our backup generators came online immediately but began failing at around 6:00 AM PDT. This impacted 7.5% of EC2 instances and EBS volumes in the Availability Zone. Power was fully restored to the impacted data center at 7:45 AM PDT. By 10:45 AM PDT, all but 1% of instances had been recovered, and by 12:30 PM PDT only 0.5% of instances remained impaired. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes and will be communicating to the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (N. Virginia)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1567259752",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:55 AM PDT</span>&nbsp;We are investigating connectivity issues in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PDT</span>&nbsp;We can confirm that some WorkSpaces instances are impaired within a single Availability Zone in the US-EAST-1 Region. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 8:33 AM PDT</span>&nbsp;We are starting to see recovery of WorkSpaces instance impairments within a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all affected WorkSpaces instances.</div><div><span class=\"yellowfg\"> 2:02 PM PDT</span>&nbsp;Beginning at 6:15 AM PDT, we experienced connectivity issues affecting some WorkSpaces in a single Availability Zone in the US-EAST-1 Region. By 12:30 PM, the majority of the impaired WorkSpaces had been recovered. A small number of the impaired WorkSpaces were hosted on hardware which may have been adversely affected by the loss of power. We continue to work to recover all affected WorkSpaces. Some of the remaining impacted WorkSpaces may require action from customers and we will be communicating to the remaining impacted customers via the Personal Health Dashboard. </div>",
      "service": "workspaces-us-east-1"
    },
    {
      "service_name": "Amazon Relational Database Service (N. Virginia)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1567260966",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:16 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some single-AZ RDS instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:25 AM PDT</span>&nbsp;We are starting to see recovery for connectivity issues impacting some single-AZ instances in a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all impacted instances. </div><div><span class=\"yellowfg\"> 2:15 PM PDT</span>&nbsp;Beginning at 5:40 AM PDT, some RDS single-AZ instances experienced connectivity issues within a single Availability Zone in the US-EAST-1 Region. By 11:55 AM, 1% of instances in the affected AZ were still experiencing connectivity issues. We continue to work to recover all affected instances and volumes and will be communicating to the remaining impacted customers via the Personal Health Dashboard. For customers who need immediate recovery, we recommend performing a <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html\">point in time</a> or <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html\">snapshot recovery</a> of your DB instance from the AWS Management Console or RDS APIs.</div>",
      "service": "rds-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (Ireland)",
      "summary": "[RESOLVED] WorkSpaces Launch Errors",
      "date": "1568280002",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:20 AM PDT</span>&nbsp;We are investigating increased errors when launching new WorkSpaces in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:49 AM PDT</span>&nbsp;Between September 11 6:35 PM and September 12 2:24 AM PDT we experienced increased errors when launching new WorkSpaces in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workspaces-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1568389951",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:52 AM PDT</span>&nbsp;Between 7:49AM PDT and 8:27AM PDT EC2 experienced elevated API errors for instance related APIs in a single Availability Zone in the US-EAST-1 region. Existing instances were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1568742586",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:49 AM PDT</span>&nbsp;Between 9:45 AM and 10:25 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Increased Invalidation Error Rates",
      "date": "1568887640",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:07 AM PDT</span>&nbsp;We are investigating elevated error rates for content invalidation. Some customers may also receive \"Rate exceeded\" exceptions when attempting to invalidate content. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 3:36 AM PDT</span>&nbsp;Between September 18 10:58 PM and September 19 2:44 AM PDT, we experienced increased API error rates impacting CreateInvalidation API. Some customers may have also received \"Rate exceeded\" exceptions when attempting to invalidate content. End-user requests for content were not affected by this issue, and content from our edge locations were not affected and continue to be served normally. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Relational Database Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1569184140",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:32 PM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 3:34 PM PDT</span>&nbsp;We continue to investigate increased error rates with Console and API calls in the US-EAST-1 Region. Connectivity to currently running instances is not affected. </div><div><span class=\"yellowfg\"> 4:39 PM PDT</span>&nbsp;We continue to see increased error rates with Console and API calls in the US-EAST-1 Region and are working towards recovery. Connectivity to currently running instances is not affected.</div><div><span class=\"yellowfg\"> 7:07 PM PDT</span>&nbsp;We have identified the issue causing increased error rates with RDS Console and API calls in the US-EAST-1 Region and are working towards recovery.</div><div><span class=\"yellowfg\"> 8:06 PM PDT</span>&nbsp;Between 1:29 PM and 7:34 PM PDT we experienced increased error rates for the RDS Management Console and API calls. The issue is resolved and the service is operating normally.</div>",
      "service": "rds-us-east-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1569187417",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:23 PM PDT</span>&nbsp;We are investigating increased error rates in the US-EAST-1 Region for RDS and Autoscaling Management Consoles.</div><div><span class=\"yellowfg\"> 3:33 PM PDT</span>&nbsp;We are continuing to investigate elevated error rates with the RDS Management Console in the US-EAST-1 Region, Autoscaling Console has recovered and is operating normally.</div><div><span class=\"yellowfg\"> 4:43 PM PDT</span>&nbsp;We are continuing to investigate elevated error rates with the RDS Management Console in the US-EAST-1 Region. All other consoles are operating normally.</div><div><span class=\"yellowfg\"> 6:32 PM PDT</span>&nbsp;We continue to see increased error rates with the RDS Console in the US-EAST-1 Region and are working towards recovery</div><div><span class=\"yellowfg\"> 7:10 PM PDT</span>&nbsp;We have identified the issue causing increased error rates with the RDS Management Console in the US-EAST-1 Region and are working towards recovery. </div><div><span class=\"yellowfg\"> 8:12 PM PDT</span>&nbsp;Between 1:29 PM and 7:34 PM PDT, we experienced increased error rates when working with the RDS Management Console in the US-EAST-1 region. The issue has been resolved and the Management Console is operating normally.</div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon API Gateway (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1569529335",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;We are investigating increased error rates for API Management calls via the console, CLI, and CloudFormation in the US-EAST-1 Region. There is no impact to invocations of deployed APIs.</div><div><span class=\"yellowfg\"> 2:04 PM PDT</span>&nbsp;Between 12:22 PM and 1:47 PM PDT we experienced increased error rates for API Management calls via the console, CLI, and CloudFormation in the US-EAST-1 Region. There was no impact to invocations of deployed APIs. The issue is resolved, and the service is operating normally.</div>",
      "service": "apigateway-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Singapore)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1569965515",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:31 PM PDT</span>&nbsp;We are investigating network connectivity issues for some AWS services in the AP-SOUTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 3:07 PM PDT</span>&nbsp; Between 1:51 PM  and 2:41 PM PDT we experienced network connectivity issues for some AWS APIs in the AP-SOUTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-ap-southeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Network Connectivity and API Error Rates",
      "date": "1569966223",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:43 PM PDT</span>&nbsp;We are investigating connectivity issues and increased API error rates in the AP-SOUTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;Between 1:51 PM and 2:41 PM PDT we experienced API error rates for the EC2 APIs and the EC2 Management Console in the AP-SOUTHEAST-1 Region. Connectivity to EC2 instances was not impacted during this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-1"
    },
    {
      "service_name": "AWS Batch (Ireland)",
      "summary": "[RESOLVED] Increased API error rates and latencies",
      "date": "1570041989",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:46 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies for the AWS Batch APIs in the EU-WEST-1 Region. Compute Resource connectivity and running jobs are not affected. </div><div><span class=\"yellowfg\">12:15 PM PDT</span>&nbsp;Between 10:35 AM and 11:59 AM PDT we experienced increased error rates and latencies for all AWS Batch APIs in the EU-WEST-1 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "batch-eu-west-1"
    },
    {
      "service_name": "AWS IoT Core (Ireland)",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1570042403",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:53 AM PDT</span>&nbsp;We are investigating increased API error rates for AWS IoT Core APIs for managing Identities, Policies, Things and Rules in the EU-WEST-1 region. Device connections, messaging and rules executions are not impacted.</div><div><span class=\"yellowfg\">12:18 PM PDT</span>&nbsp;Between 10:52 AM and 11:57 AM PDT, we experienced elevated API error rates for managing Identities, Policies, Things and Rules in the EU-WEST-1 region. We have resolved the issue and the service is operating normally. </div>",
      "service": "awsiot-eu-west-1"
    },
    {
      "service_name": "Amazon API Gateway (Ireland)",
      "summary": "[RESOLVED] Increase API error rates",
      "date": "1570043240",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:07 PM PDT</span>&nbsp;We are investigating increased error rates for API invokes in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 1:19 PM PDT</span>&nbsp;We have identified the root cause of the issue causing increased error rates in the EU-WEST-1 Region and are working on a resolution.</div><div><span class=\"yellowfg\"> 1:32 PM PDT</span>&nbsp;Between 10:23 AM and 1:11 PM PDT we experienced increased error rates in the EU-WEST-1 Region. The issue is resolved and the service is operating normally.</div>",
      "service": "apigateway-eu-west-1"
    },
    {
      "service_name": "Amazon CloudWatch (N. Virginia)",
      "summary": "[RESOLVED] Elevated Error Rates",
      "date": "1570238568",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:22 PM PDT</span>&nbsp;We are investigating elevated rate of API faults and delays in processing some alarms in the US-EAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 6:42 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-EAST-1 Region. We are actively working to resolve the issue. </div><div><span class=\"yellowfg\"> 6:49 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-EAST-1 Region. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:34 PM PDT</span>&nbsp;We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery.</div><div><span class=\"yellowfg\"> 8:49 PM PDT</span>&nbsp;We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. </div><div><span class=\"yellowfg\"> 9:48 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 7:31 PM PDT, and by 9:28 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally.</div>",
      "service": "cloudwatch-us-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (Ireland)",
      "summary": "[RESOLVED] Elevated Error Rates ",
      "date": "1570239838",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:43 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the EU-WEST-1 Region. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:36 PM PDT</span>&nbsp;We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery. </div><div><span class=\"yellowfg\"> 8:46 PM PDT</span>&nbsp;We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. </div><div><span class=\"yellowfg\"> 9:44 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 8:55 PM PDT, and we are currently working to deliver the backlog of all previously scheduled CloudWatch events. All other kinds of CloudWatch events and other Lambda function invocations continue to operate normally.</div><div><span class=\"yellowfg\">10:19 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 8:55 PM PDT, and by 10:15 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally.</div>",
      "service": "cloudwatch-eu-west-1"
    },
    {
      "service_name": "Amazon CloudWatch (Oregon)",
      "summary": "[RESOLVED] Elevated Error Rates",
      "date": "1570239912",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:45 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-WEST-2 Region. We are actively working to resolve the issue. </div><div><span class=\"yellowfg\"> 6:51 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-WEST-2. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.\n\n</div><div><span class=\"yellowfg\"> 7:35 PM PDT</span>&nbsp;We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery. </div><div><span class=\"yellowfg\"> 8:46 PM PDT</span>&nbsp;We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. </div><div><span class=\"yellowfg\"> 9:37 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 7:36 PM PDT, and by 9:22 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally.</div>",
      "service": "cloudwatch-us-west-2"
    },
    {
      "service_name": "AWS Backup (Frankfurt)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1570493040",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:00 PM PDT</span>&nbsp;We are investigating increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 7:36 PM PDT</span>&nbsp;We continue to investigate increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 9:32 PM PDT</span>&nbsp;We have identified the cause of the increased API error rates in the EU-CENTRAL-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\">10:09 PM PDT</span>&nbsp;Between 5:04 PM and 9:17 PM PDT we experienced increased API error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "backup-eu-central-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates for Bucket Operations",
      "date": "1570551545",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:19 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for customers creating and managing Amazon S3 buckets and are working towards resolution. Only a small subset of requests are affected and retries are working.</div><div><span class=\"yellowfg\"> 9:30 AM PDT</span>&nbsp;Between 7:54 AM and 9:18 AM PDT, S3 customers saw elevated errors and latency for a subset of API requests to create and manage Amazon S3 buckets. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-standard"
    },
    {
      "service_name": "AWS Backup (Frankfurt)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1570579440",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">Oct 7,  7:00 PM PDT</span>&nbsp;We are investigating increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">Oct 7,  7:36 PM PDT</span>&nbsp;We continue to investigate increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">Oct 7,  9:32 PM PDT</span>&nbsp;We have identified the cause of the increased API error rates in the EU-CENTRAL-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\">Oct 7, 10:09 PM PDT</span>&nbsp;Between 5:04 PM and 9:17 PM PDT we experienced increased API error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "backup-eu-central-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1571588700",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:16 AM PDT</span>&nbsp;We are investigating increased errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered as normal.</div><div><span class=\"yellowfg\">10:46 AM PDT</span>&nbsp;We are continuing to investigate elevated API errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=\"yellowfg\">11:26 AM PDT</span>&nbsp;We have identified root cause and are working towards recovery of elevated error rates using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=\"yellowfg\">11:33 AM PDT</span>&nbsp;Between 9:25 AM and 11:30 AM PDT, we experienced increased errors when making changes to some hosted zones, and creating new hosted zones using the Route 53 API and Console. The issue has been recovered, and all services are operating normally. DNS queries were not affected by this issue, which were all answered normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1571591808",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:16 AM PDT</span>&nbsp;We are investigating increased errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered as normal.</div><div><span class=\"yellowfg\">10:46 AM PDT</span>&nbsp;We are continuing to investigate elevated API errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=\"yellowfg\">11:26 AM PDT</span>&nbsp;We have identified root cause and are working towards recovery of elevated error rates using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=\"yellowfg\">11:33 AM PDT</span>&nbsp;Between 9:25 AM and 11:30 AM PDT, we experienced increased errors when making changes to some hosted zones, and creating new hosted zones using the Route 53 API and Console. The issue has been recovered, and all services are operating normally. DNS queries were not affected by this issue, which were all answered normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Further information regarding DNS resolution",
      "date": "1571877848",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:44 PM PDT</span>&nbsp;On October 22, 2019, we detected and then mitigated a DDoS (Distributed Denial of Service) attack against Route 53. Due to the way that DNS queries are processed, this attack was first experienced by many other DNS server operators as the queries made their way through DNS resolvers on the internet to Route 53. The attack targeted specific DNS names and paths, notably those used to access the global names for S3 buckets.\n\nBecause this attack was widely distributed, a small number of ISPs operating affected DNS resolvers implemented mitigation strategies of their own in an attempt to control the traffic. This is causing DNS lookups through these resolvers for a small number of AWS names to fail. We are doing our best to identify and contact these operators, as quickly as possible, and working with them to enhance their mitigations so that they do not cause impact to valid requests. If you are experiencing issues, please contact us so we can work with your operator to help resolve.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Call Handling",
      "date": "1572019433",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:03 AM PDT</span>&nbsp;We are investigating degraded call handling by agents using the Amazon Connect softphone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:48 AM PDT</span>&nbsp;Between 8:17 AM and 9:30 AM PDT we experienced degraded call handling by agents using the softphone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.\n</div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1572377331",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:28 PM PDT</span>&nbsp;We are investigating increased Create, Update, and Delete Stream API error rates for Amazon Kinesis Data Streams in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 1:06 PM PDT</span>&nbsp;Between 10:45 AM and 12:52 PM PDT, we experienced increased error rates for Create, Update, and Delete Stream API operations for Amazon Kinesis Data Streams in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally.</div>",
      "service": "kinesis-us-east-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased Console Error Rates and latencies",
      "date": "1572963155",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:12 AM PST</span>&nbsp;We are investigating increased AWS Billing Console error rates and latencies. We have identified the root-cause and are working on remediation.</div><div><span class=\"yellowfg\"> 6:42 AM PST</span>&nbsp;We continue to investigate increased AWS Billing Console error rates and latencies. We have identified the root-cause and are working on remediation.</div><div><span class=\"yellowfg\"> 7:41 AM PST</span>&nbsp;Remediation work for increased AWS Billing Console error rates and latencies continues. The issue affects accessing and reviewing billing information on the Billing Dashboard and Bills pages on the console.\n</div><div><span class=\"yellowfg\"> 8:48 AM PST</span>&nbsp;We continue to work on the remediation for elevated Billing Dashboard and Bills page errors. As a work-around to retrieve your invoices, please navigate to Orders and Invoices page on the AWS Billing Console and, in order to review your cost and usage information, please navigate to Cost Explorer Console.</div><div><span class=\"yellowfg\">10:17 AM PST</span>&nbsp;As of 9:50 am, we are seeing recovery of the service on Billing Dashboard and Bills pages as we continue to work through the mitigation activities. </div><div><span class=\"yellowfg\">10:44 AM PST</span>&nbsp;Between 5:06 AM and 9:50 AM PST we experienced elevated error rates and latencies affecting Billing Dashboard and Bills pages on the AWS Billing Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation Delays",
      "date": "1573033383",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:43 AM PST</span>&nbsp;We are investigating longer than usual propagation times to propagate certain configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 2:39 AM PST</span>&nbsp;We can confirm longer than usual propagation times to propagate configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations, and continue to work toward resolution. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 4:03 AM PST</span>&nbsp;We continue to work on mitigating the cause of longer than usual propagation times to propagate configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 6:23 AM PST</span>&nbsp;Between November 5 9:52 PM and November 6 6:00 AM PST, we experienced intermittent delays in propagating CloudFront distribution changes to some edge locations. End-user requests for content from our edge locations were not affected. The issue has been resolved and the service is operating normally. </div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Auto Scaling (US-West)",
      "summary": "[RESOLVED] Increased Auto Scaling API errors",
      "date": "1573088299",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:58 PM PST</span>&nbsp;We are investigating increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 5:31 PM PST</span>&nbsp;We continue to investigate increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 6:06 PM PST</span>&nbsp;We can confirm increased API error rates in the US-GOV-WEST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 6:20 PM PST</span>&nbsp;Between 4:15 PM and 6:11 PM PST we experienced increased API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "autoscaling-us-gov-west-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Hong Kong)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1573150026",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:07 AM PST</span>&nbsp;We are investigating increased error rates for Kinesis Data Streams APIs in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">10:38 AM PST</span>&nbsp;We can confirm Kinesis Data Streams is experiencing increased error rates on all APIs in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">11:06 AM PST</span>&nbsp;Between 9:19 AM and 10:52 AM PST Kinesis Data Streams customers experienced increased error rates on all APIs in the AP-EAST-1 Region. The issue is resolved and the service is operating normally.\n</div>",
      "service": "kinesis-ap-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (Hong Kong)",
      "summary": "[RESOLVED] Increased Faults and Latencies",
      "date": "1573150195",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:09 AM PST</span>&nbsp;We are investigating increased faults and latencies for CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;We can confirm increased faults and latencies for CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region, we are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:52 AM PST</span>&nbsp;We can confirm recovery for the CloudWatch Logs PutLogEvents API and continue to work towards recovery for the Metrics PutMetricsData API in the AP-EAST-1 Region. </div><div><span class=\"yellowfg\">11:04 AM PST</span>&nbsp;Between 09:15 AM and 10:51 AM PST, some customers experienced elevated faults when calling CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region. Some metrics were delayed, CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state, and some metric data may be missing. The issue is resolved and the service is operating normally.</div>",
      "service": "cloudwatch-ap-east-1"
    },
    {
      "service_name": "Amazon Kinesis Firehose (Hong Kong)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1573150212",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:10 AM PST</span>&nbsp;We are investigating increased error rates for Kinesis Data Firehose APIs in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">10:41 AM PST</span>&nbsp;We can confirm Kinesis Data Firehose is experiencing increased error rates on all APIs in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">11:09 AM PST</span>&nbsp;Between 9:19 AM and 10:52 AM PST Kinesis Data Firehose customers experienced increased error rates on all APIs in the AP-EAST-1 Region. The issue is resolved and the service is operating normally.</div>",
      "service": "firehose-ap-east-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. Virginia)",
      "summary": "[RESOLVED] Increased Update Table Processing Times",
      "date": "1573196668",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:04 PM PST</span>&nbsp;We are investigating increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:44 PM PST</span>&nbsp;We have identified the cause of the increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\">Nov 8, 12:03 AM PST</span>&nbsp;Between 8:45 PM and 11:55 PM PST, we experienced increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "dynamodb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Frankfurt)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1573546089",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:08 AM PST</span>&nbsp;We are investigating increased network connectivity errors to instances in a single Availability Zone in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">12:28 AM PST</span>&nbsp;We are experiencing elevated API error rates and network connectivity errors in a single Availability Zone. We have identified the root cause and are working to resolve the issue. </div><div><span class=\"yellowfg\"> 1:04 AM PST</span>&nbsp;Most network connectivity to instances within the impacted Availability Zone has been restored, and we are completing recovery for the remaining connectivity errors. We are continuing to experience elevated API error rates and are working to resolve this issue.</div><div><span class=\"yellowfg\"> 2:13 AM PST</span>&nbsp;Some EBS volumes are experiencing degraded performance in a single Availability Zone in the EU-CENTRAL-1 Region. We are working to resolve this issue. Network connectivity errors and elevated API error rates in the Availability Zone have been resolved. </div><div><span class=\"yellowfg\"> 6:06 AM PST</span>&nbsp;Between November 11 11:38 PM and November 12 12:48 AM PST we experienced API errors, network connectivity errors, and degraded EBS volume performance in a single Availability Zone in the EU-CENTRAL-1 Region. The network connectivity errors were resolved at 12:48 AM PST, the API errors were resolved at 1:26 AM PST. We have recovered the majority of the degraded volumes. A small number of volumes remain degraded. We continue to work to recover all affected volumes and will notify customers with impacted volumes via their Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected volumes if possible. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-central-1"
    },
    {
      "service_name": "Amazon Relational Database Service (Frankfurt)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1573549998",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:13 AM PST</span>&nbsp;We are continuing to investigate connectivity issues affecting some instances in a single Availability Zone in the EU-CENTRAL-1 Region. \n</div><div><span class=\"yellowfg\"> 2:23 AM PST</span>&nbsp;We are starting to see recovery for the connectivity issues affecting a small number of instances in a single Availability Zone in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 5:09 AM PST</span>&nbsp;Between November 11 11:39 PM and November 12 5:00 AM PST, a small number of instances experienced connectivity issues in a single Availability Zone in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-eu-central-1"
    },
    {
      "service_name": "AWS CloudFormation (Frankfurt)",
      "summary": "[RESOLVED] Increased Stack Create, Delete and Update Times. ",
      "date": "1573552926",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:02 AM PST</span>&nbsp;We are investigating increased latencies for creating, deleting and updating stacks in the EU-CENTRAL-1 Region. </div><div><span class=\"yellowfg\"> 3:29 AM PST</span>&nbsp;Between November 11 11:35 PM and November 12 3:25 AM PST we experienced increased latencies for creating, deleting and updating stacks in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "cloudformation-eu-central-1"
    },
    {
      "service_name": "Auto Scaling (Frankfurt)",
      "summary": "[RESOLVED] Increased Auto Scaling API errors",
      "date": "1573556991",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:09 AM PST</span>&nbsp;We are investigating elevated API fault rate and scaling delays for EC2 Auto Scaling in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 4:10 AM PST</span>&nbsp;Between November 11 11:44 PM PST and November 12 3:35 AM PST we experienced elevated API fault rates and scaling delays for EC2 Auto Scaling in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "autoscaling-eu-central-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1573583181",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:26 AM PST</span>&nbsp;We are investigating increased API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances is not affected.</div><div><span class=\"yellowfg\">10:50 AM PST</span>&nbsp;We continue to work towards resolution of the issue affecting API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances remains unaffected. </div><div><span class=\"yellowfg\">11:31 AM PST</span>&nbsp;We are beginning to see recovery for the increased API error rates and latencies in the US-EAST-1 Region and continue to work toward full recovery. </div><div><span class=\"yellowfg\"> 1:18 PM PST</span>&nbsp;Between 8:57 AM and 10:52 AM PST, we experienced increased API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances was not affected during this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1573586011",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:13 AM PST</span>&nbsp;We are investigating increased API error rates and latency for some IAM APIs.</div><div><span class=\"yellowfg\">11:47 AM PST</span>&nbsp;We are beginning to see recovery for the increased API error rates and latencies and continue to work toward full recovery. </div><div><span class=\"yellowfg\"> 1:33 PM PST</span>&nbsp;Between 8:57 AM and 10:55 AM PST we experienced increased API error rates and latencies. The issue has been resolved and the service is operating normally. </div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1573587595",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:31 AM PST</span>&nbsp;We are investigating increased API error rates and call handling issues the US-EAST-1 Region. </div><div><span class=\"yellowfg\">12:19 PM PST</span>&nbsp;We are beginning to see recovery for the increased API error rates and call handling issues in the US-EAST-1 Region and continue to work toward full recovery. </div><div><span class=\"yellowfg\"> 1:24 PM PST</span>&nbsp;Between 8:57 AM and 12:09 PM PST we experienced increased API error rates and call handling issues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "AWS Lambda (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1573591030",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:37 PM PST</span>&nbsp;We are experiencing increased error rates and latencies for actions on the AWS Lambda Management Console in the US-EAST-1 Region. AWS Lambda API calls via the SDK and CLI remain unaffected. Some invocations of scheduled Lambda function are also experiencing error rates in the US-EAST-1 Region. The root cause of these issues has been identified and we are working towards full resolution.</div><div><span class=\"yellowfg\"> 1:21 PM PST</span>&nbsp;Between 8:56 AM and 12:15 PM PST, some customers experienced increased error rates and latencies for actions on the AWS Lambda Management Console in the US-EAST-1 Region. AWS Lambda API calls via the SDK and CLI were unaffected. Between 11:22 AM and 12:40 PM PST, some invocations of scheduled Lambda functions also experienced elevated error rates in the US-EAST-1 Region. Any unprocessed invocations for affected functions have been reprocessed based on the retry policy on the function. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-1"
    },
    {
      "service_name": "Amazon Kinesis Analytics (N. Virginia)",
      "summary": "[RESOLVED] Applications not processing data",
      "date": "1573673937",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We are currently investigating issues with Amazon Kinesis Data Analytics applications that are not processing data in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:08 AM PST and 12:10 PM PST a subset of Amazon Kinesis Data Analytics applications were not processing data in US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesisanalytics-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates for EC2 Spot",
      "date": "1573890252",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:44 PM PST</span>&nbsp;We are investigating increased API error rates for EC2 Spot-related APIs in the US-EAST-1 Region. Affected customers will not be able to launch new EC2 Spot instances, but existing instances are unaffected. Launching new EC2 On-Demand Instances is unaffected.</div><div><span class=\"yellowfg\">Nov 16, 12:03 AM PST</span>&nbsp;We are investigating increased API error rates for EC2 Spot-related APIs in the US-EAST-1 Region. Affected customers will not be able to launch new EC2 Spot instances, but existing instances are unaffected. Launching new EC2 On-Demand Instances is unaffected. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\">Nov 16,  1:41 AM PST</span>&nbsp;Between November 15 10:30 PM and November 16 1:19 AM PST, we experienced increased API latencies and error rates for EC2 Spot APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "Increased API Error Rates ",
      "date": "1574595711",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:41 AM PST</span>&nbsp;We are currently investigating increased error rates for the Elastic Beanstalk Health APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:07 AM PST</span>&nbsp;Between 1:20 AM and 3:50 AM PST, we experienced increased error rates for the Elastic Beanstalk Health APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sydney)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1574731819",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:30 PM PST</span>&nbsp;We are investigating increased Console and API error rates in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:55 PM PST</span>&nbsp;We have identified the root cause of the issue causing increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. Existing instances are not affected by this issue. We are working towards resolution of the issue.</div><div><span class=\"yellowfg\"> 6:28 PM PST</span>&nbsp;We are seeing recovery for the increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. We continue to monitor error rates and latencies as we work towards full recovery.</div><div><span class=\"yellowfg\"> 7:33 PM PST</span>&nbsp;Between 4:52 PM and 6:22 PM PST we experienced increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-2"
    },
    {
      "service_name": "Auto Scaling (Sydney)",
      "summary": "[RESOLVED] Increased Launch Times ",
      "date": "1574732926",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:48 PM PST</span>&nbsp;We are investigating increased launch times for EC2 instances managed by Auto Scaling in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 6:13 PM PST</span>&nbsp;We continue to work towards resolving the issue causing increased launch times for Auto Scaling in the AP-SOUTHEAST-2 Region. All affected launches will complete successfully once the increased error rates and latencies for the EC2 APIs have been resolved.</div><div><span class=\"yellowfg\"> 6:29 PM PST</span>&nbsp;With the improvement in error rates and latencies for the EC2 APIs, Auto Scaling launches are once again succeeding. We're now working through the backlog of pending launches.</div><div><span class=\"yellowfg\"> 7:33 PM PST</span>&nbsp;Between 4:52 PM and 6:22 PM PST we experienced increased launch latencies for Auto Scaling in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "autoscaling-ap-southeast-2"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased SAML Sign-In Error Rates",
      "date": "1576790068",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:14 PM PST</span>&nbsp;We are investigating increased error rates for SAML-based sign-in requests. Sign-in requests for other Roles and users are not affected.</div><div><span class=\"yellowfg\"> 2:14 PM PST</span>&nbsp;We can confirm increased error rates for SAML-based sign-in requests. Sign-in requests for other Roles and users are not affected. We continue to investigate increased error rates for SAML-based sign-in requests.</div><div><span class=\"yellowfg\"> 2:37 PM PST</span>&nbsp;We are beginning to see recovery for the increased error rates impacting SAML-based sign-in requests. We continue to work toward full recovery. </div><div><span class=\"yellowfg\"> 3:21 PM PST</span>&nbsp;Between 11:23 AM and 2:42 PM PST we experienced increased error rates impacting SAML-based sign-in requests. During this time customers may have been unable to authenticate to the AWS Console when using SAML for sign-in requests. The issue has been resolved and the service is operating normally. </div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Elevated CloudFront API errors",
      "date": "1577471528",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:32 AM PST</span>&nbsp;We are investigating increased CloudFront API errors and longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations are not affected.</div><div><span class=\"yellowfg\">11:16 AM PST</span>&nbsp;We have resolved the issues related to CloudFront APIs. We have identified the root cause of the longer than usual change propagation delays for invalidations and CloudFront configurations and are actively working towards resolution. End-user requests for content from edge locations are not affected and continue to be served normally.</div><div><span class=\"yellowfg\">12:04 PM PST</span>&nbsp;Between 9:30 AM and 11:01 AM PST, customers may have seen elevated CloudFront API errors. These have now been resolved. Due to these API errors there was a backlog of changes that resulted in longer than usual change propagation delays for CloudFront configurations and invalidations. This backlog of changes is actively being processed. End-user requests for content from edge locations are not affected and continue to be served normally.</div><div><span class=\"yellowfg\"> 2:01 PM PST</span>&nbsp;Between 9:30 AM and 11:01 AM PST, customers may have seen elevated CloudFront API errors. Due to these API errors there was a backlog of changes that resulted in longer than usual change propagation delays for CloudFront configurations and invalidations. The backlog of Invalidation changes were fully processed by 11:35 AM. The backlog of CloudFront configuration changes was fully processed by 2:00 PM PST. All issues have been fully resolved and the system is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "AWS Lambda (Ireland)",
      "summary": "[RESOLVED] Increased delays in event processing from Streams ",
      "date": "1578446772",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:26 PM PST</span>&nbsp;Between 4:05 PM PST and 5:20 PM PST, customers using Lambda functions to process events from Kinesis Data Streams and DynamoDB Streams experienced significant delays in event processing for a subset of functions in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally. The backlogged events will be processed by the function over the next few hours as per the retry policy on the event source mapping on the affected functions.</div>",
      "service": "lambda-eu-west-1"
    },
    {
      "service_name": "Amazon Chime",
      "summary": "[RESOLVED] Availability",
      "date": "1579119985",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:26 PM PST</span>&nbsp;We are currently investigating increased application faults and decreased availability for Amazon Chime.</div><div><span class=\"yellowfg\">12:45 PM PST</span>&nbsp;We have identified the root cause of the increased application faults and decreased availability for Amazon Chime. Customers who are already signed into the Chime Client are not advised to logout, or select the re-connection option.</div><div><span class=\"yellowfg\"> 1:36 PM PST</span>&nbsp;We have identified the root cause of the increased application faults and decreased availability for Amazon Chime, and can confirm recovery for some users. Customers who are already signed into the Chime Client are not advised to logout, or select the re-connection option, as we continue to work towards resolution.</div><div><span class=\"yellowfg\"> 2:30 PM PST</span>&nbsp;We have identified the root cause of the increased application faults and decreased availability for Amazon Chime, and can confirm recovery for audio meeting and chat features. We continue to work towards resolving decreased video conferencing availability.</div><div><span class=\"yellowfg\"> 3:11 PM PST</span>&nbsp;Between 11:49 AM and 3:08 PM PST we experienced increased application faults and decreased availability for Amazon Chime. The issue has been resolved and the service is operating normally.</div>",
      "service": "chime"
    },
    {
      "service_name": "AWS Certificate Manager (N. Virginia)",
      "summary": "[RESOLVED] Certificate Issuance Delays",
      "date": "1579134535",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:28 PM PST</span>&nbsp;We have identified the cause of the increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region and are working to mitigate the issue. Certificates can be requested but will be delayed in issuance.</div><div><span class=\"yellowfg\"> 4:58 PM PST</span>&nbsp;We continue to work towards mitigating the increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region. Certificates that are DNS validated can be requested but will be delayed in issuance.</div><div><span class=\"yellowfg\"> 6:20 PM PST</span>&nbsp;Between 3:05 PM and 6:11 PM PST we experienced increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. Certificates waiting for DNS validation which have proper CNAME entries in DNS will validate within the next 3 hours.</div>",
      "service": "certificatemanager-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "Increased Error Rates and Latencies",
      "date": "1579369796",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:49 AM PST</span>&nbsp;Starting at 7:30 AM PST, we have been experiencing increased error rates due to higher latency for IAM requests. We are actively investigating the issue. </div><div><span class=\"yellowfg\">10:05 AM PST</span>&nbsp;Between 7:30 AM and 9:45 AM PST, we experienced increased error rates due to higher latency for IAM requests. The system has recovered and is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Relational Database Service (Mumbai)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1579421028",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:03 AM PST</span>&nbsp;We are investigating increased API error rates in the AP-SOUTH-1 Region.</div><div><span class=\"yellowfg\">12:35 AM PST</span>&nbsp;Between January 18 11:18 PM and January 19 12:14 AM PST we experienced increased API error rate in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-ap-south-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Paris)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1579717555",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:25 AM PST</span>&nbsp;We are investigating an issue which is affecting internet connectivity to a single availability zone in EU-WEST-3 Region.</div><div><span class=\"yellowfg\">11:05 AM PST</span>&nbsp;We have identified the root cause of the issue that is affecting connectivity to a single availability zone in EU-WEST-3 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\">11:45 AM PST</span>&nbsp;Between 10:00 AM and 11:28 AM PST we experienced an issue affecting network connectivity to AWS services in a single Availability Zone in EU-WEST-3 Region. The issue has been resolved and connectivity has been restored.</div>",
      "service": "internetconnectivity-eu-west-3"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Paris)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1579723478",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:04 PM PST</span>&nbsp;Between 10:00 AM and 11:28 AM PST we experienced network connectivity issues affecting EC2 instances in a single Availability Zone in the EU-WEST-3 Region. Instances in the affected Availability Zone were able to connect to the Internet but were unable to resolve DNS records during this time. New instance launches into the affected Availability Zone were also affected by the event. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-3"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sydney)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1579740075",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:41 PM PST</span>&nbsp;We are investigating increased API error rates and latencies in the AP-SOUTHEAST-2 Region. Connectivity to existing instances is not impacted.</div><div><span class=\"yellowfg\"> 5:18 PM PST</span>&nbsp;We have identified the root cause of the issue causing increased API error rates and latencies in the AP-SOUTHEAST-2 Region and continue working towards resolution. This issue mainly affects EC2 RunInstances and VPC related API requests. Customer using the EC2 Management Console will also experience error rates for instance and network related functions. Connectivity to existing instances remains unaffected.</div><div><span class=\"yellowfg\"> 6:25 PM PST</span>&nbsp;We continue to experience increased API error rates for the EC2 APIs in the AP-SOUTHEAST-2 Region. We have confirmed the root cause, and are working on multiple paths toward recovering the subsytem that is impaired, which is responsible for networking related API calls. This issue mainly affects EC2 RunInstance, and VPC related API requests. Customers using the EC2 Management Console may experience errors Describing Resources, as well as making mutating API requests. Connectivity to existing instances in the AP-SOUTHEAST-2 remains unaffected.</div><div><span class=\"yellowfg\"> 8:49 PM PST</span>&nbsp;We wanted to provide you with more details on the issue causing increased API error rates and latencies in the AP-SOUTHEAST-2 Region. A data store used by a subsystem responsible for the configuration of Virtual Private Cloud (VPC) networks is currently offline and the engineering team are working to restore it. While the investigation into the issue was started immediately, it took us longer to understand the full extent of the issue and determine a path to recovery. We determined that the data store needed to be restored to a point before the issue began. In order to do this restore, we needed to disable writes. Error rates and latencies for the networking-related APIs will continue until the restore has been completed and writes re-enabled. We are working through the recovery process now. With issues like this, it is always difficult to provide an accurate ETA, but we expect to complete the restore process within the next 2 hours and begin to allow API requests to proceed once again. We will continue to keep you updated if that ETA changes. Connectivity to existing instances is not impacted. Also, launch requests that refer to regional objects like subnets that already exist will succeed at this stage, as they do not depend on the affected subsystem. If you know the subnet ID, you can use that to launch instances within the region. We apologize for the impact and continue to work towards full resolution. </div><div><span class=\"yellowfg\">10:10 PM PST</span>&nbsp;We continue to make steady progress towards the restoration of the affected data store and are currently within the 2 hours ETA published above.</div><div><span class=\"yellowfg\">10:55 PM PST</span>&nbsp;We have completed the restoration of the affected data store but are still working towards re-enabling writes. We have seen an improvement in successful launches over the last 20 minutes and expect that to continue as we work towards full recovery. </div><div><span class=\"yellowfg\">11:45 PM PST</span>&nbsp;We can confirm that all error rates and latencies have returned to normal levels. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">Jan 23, 12:30 AM PST</span>&nbsp;Now that we are fully recovered, we wanted to provide a brief summary of the issue. Starting at 4:07 PM PST, customers began to experience increased error rates and latencies for the network-related APIs in the AP-SOUTHEAST-2 Region. Launches of new EC2 instances also experienced increased failure rates as a result of this issue. Connectivity to existing instances was not affected by this event. We immediately began investigating the root cause and identified that the data store used by the subsystem responsible for the Virtual Private Cloud (VPC) regional state was impaired. While the investigation into the issue was started immediately, it took us longer to understand the full extent of the issue and determine a path to recovery. We determined that the data store needed to be restored to a point before the issue began. We began the data store restoration process, which took a few hours and by 10:50 PM PST, we had fully restored the primary node in the affected data store. At this stage, we began to see recovery in instance launches within the AP-SOUTHEAST-2 Region, restoring many customer applications and services to a healthy state. We continued to bring the data store back to a fully operational state and by 11:20 PM PST, all API error rates and latencies had fully recovered. Other AWS services - including AppStream, Elastic Load Balancing, ElastiCache, Relational Database Service, Amazon WorkSpaces and Lambda – were also affected by this event. We apologize for any inconvenience this event may have caused as we know how critical our services are to our customers. We are never satisfied with operational performance of our services that is anything less than perfect, and will do everything we can to learn from this event and drive improvement across our services.</div>",
      "service": "ec2-ap-southeast-2"
    },
    {
      "service_name": "Amazon Relational Database Service (Sydney)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1579742960",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:29 PM PST</span>&nbsp;We are investigating increased API error rates and latencies in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 6:20 PM PST</span>&nbsp;We can confirm increased API error rates and latencies in the AP-SOUTHEAST-2 Region and continue to work towards resolution. Connectivity to existing instances remains unaffected.</div><div><span class=\"yellowfg\"> 7:35 PM PST</span>&nbsp;We are continuing to work towards resolution of increased API error rates and latencies in the AP-SOUTHEAST-2 Region. Connectivity to existing instances remains unaffected.</div><div><span class=\"yellowfg\"> 9:00 PM PST</span>&nbsp;We continue to experience increased API error rates and latencies due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution.</div><div><span class=\"yellowfg\">11:38 PM PST</span>&nbsp;Between 4:41 PM and 11:35 PM PDT we experienced increased API error rates and latencies due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-ap-southeast-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Sydney)",
      "summary": "[RESOLVED] Increased Provisioning Latencies",
      "date": "1579743205",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:33 PM PST</span>&nbsp;We are investigating increased provisioning times and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\"> 6:13 PM PST</span>&nbsp;We can confirm increased provisioning/scaling latencies and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\"> 7:23 PM PST</span>&nbsp;We are continuing to work towards resolution of increased provisioning/scaling latencies and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\"> 8:59 PM PST</span>&nbsp;We continue to experience increased provisioning/scaling latencies and ELB API error rates for load balancers due to the issue affecting EC2 in the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\">Jan 23, 12:22 AM PST</span>&nbsp;Between 4:10 PM and 11:40 PM PST, we experienced increased provisioning/scaling latencies and ELB API error rates for load balancers due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-ap-southeast-2"
    },
    {
      "service_name": "Amazon AppStream 2.0 (Sydney)",
      "summary": "[RESOLVED] Increased Instance Provisioning Error Rates",
      "date": "1579745293",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:08 PM PST</span>&nbsp;We are currently experiencing an issue provisioning new image builder and fleet streaming instances in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 7:20 PM PST</span>&nbsp;We are continuing to investigate an increase in instance provisioning error rates in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 8:32 PM PST</span>&nbsp;We have identified the cause of the increased provisioning error rates in the AP-SOUTHEAST-2 Region and continue working towards resolution.</div><div><span class=\"yellowfg\"> 8:59 PM PST</span>&nbsp;We continue to experience increased instance provisioning error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing streaming sessions and instances will continue to operate.</div><div><span class=\"yellowfg\">Jan 23, 12:41 AM PST</span>&nbsp;We continue to experience increased instance provisioning error rates within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing streaming sessions and instances will continue to operate.</div><div><span class=\"yellowfg\">Jan 23,  1:51 AM PST</span>&nbsp;We are continuing to work towards resolution of increased instance provisioning error rates within the AP-SOUTHEAST-2 Region. Existing streaming sessions and instances will continue to operate.</div><div><span class=\"yellowfg\">Jan 23,  2:38 AM PST</span>&nbsp;We recently experienced increased instance provisioning errors within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "appstream2-ap-southeast-2"
    },
    {
      "service_name": "Amazon ElastiCache (Sydney)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1579748460",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:01 PM PST</span>&nbsp;We are experiencing increased latencies while provisioning new ElastiCache nodes and and elevated API error rates in the AP-SOUTHEAST-2 AWS Region. Existing ElastiCache clusters are not impacted and are continuing to serve traffic. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 9:32 PM PST</span>&nbsp;We continue to experience increased latencies for ElastiCache cluster creation, modification and deletion operations, and elevated API error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing clusters are operating normally.</div><div><span class=\"yellowfg\">11:55 PM PST</span>&nbsp;Between 4:09 PM and 11:44 PM PST, we experienced increased latencies for ElastiCache cluster creation, modification and deletion operations, and elevated API error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticache-ap-southeast-2"
    },
    {
      "service_name": "AWS Lambda (Sydney)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1579749477",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:17 PM PST</span>&nbsp;We can confirm increased API error rates in the AP-SOUTHEAST-2 Region for functions that are configured with VPC settings. Functions that are not configured with VPC settings are unaffected.</div><div><span class=\"yellowfg\"> 9:02 PM PST</span>&nbsp;We continue to experience increased API error rates in the AP-SOUTHEAST-2 Region for functions that are configured with VPC settings due to the issue affecting EC2 in the AP-SOUTHEAST-2 Region. We continue to work towards full resolution.</div><div><span class=\"yellowfg\">11:31 PM PST</span>&nbsp;Between 4:50 PM and 11:00 PM PST, we experienced increased API error rates for functions due to an issue affecting EC2 in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-ap-southeast-2"
    },
    {
      "service_name": "Amazon WorkSpaces (Sydney)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1579750287",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:31 PM PST</span>&nbsp;We are investigating increased Amazon WorkSpaces API error rates and provisioning times for Amazon WorkSpaces in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 8:39 PM PST</span>&nbsp;We have identified the cause of increased Amazon WorkSpaces API error rates and provisioning times for Amazon WorkSpaces in the AP-SOUTHEAST-2 Region and continue working towards resolution.</div><div><span class=\"yellowfg\"> 9:02 PM PST</span>&nbsp;We continue to experience increased API error rates and provisioning times for Amazon WorkSpaces due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. Existing Amazon WorkSpaces sessions will continue to operate.</div><div><span class=\"yellowfg\">11:32 PM PST</span>&nbsp;Between 4:04 PM and 10:55 PM PST, we experienced increased Amazon WorkSpaces API error rates and provisioning times due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is now operating normally.</div>",
      "service": "workspaces-ap-southeast-2"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Route 53 DNS Change Issues ",
      "date": "1579899055",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:50 PM PST</span>&nbsp;We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\"> 1:21 PM PST</span>&nbsp;We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. This will affect provisioning of new resources that rely on Route 53 for DNS, such as EFS and PrivateLink. Queries to existing DNS records are not affected by this issue</div><div><span class=\"yellowfg\"> 1:38 PM PST</span>&nbsp;We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. To help accelerate recovery, the Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway.  Queries to existing DNS records are not affected by this issue. </div><div><span class=\"yellowfg\"> 2:58 PM PST</span>&nbsp;We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. To help accelerate recovery, the Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation and Chime Voice Connector.</div><div><span class=\"yellowfg\"> 3:08 PM PST</span>&nbsp;We have identified root cause resulting in increased propagation times of DNS edits to the Route 53 DNS servers. The Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests in order to help accelerate recovery. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector and Global Accelerator.</div><div><span class=\"yellowfg\"> 3:45 PM PST</span>&nbsp;We have identified root cause resulting in increased propagation times of DNS edits to the Route 53 DNS servers, and are working towards recovery. The Route 53 API is now accepting changes again, though these changes are still experiencing delays propagating as there is a significant backlog of changes to process. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector, Global Accelerator, RDS, SageMaker Ground Truth, Amazon Managed Blockchain and Directory Service.</div><div><span class=\"yellowfg\"> 5:21 PM PST</span>&nbsp;Between 12:07 PM and 5:15 PM PST, customers experienced delays propagating changes submitted to the Route 53 API, as well as increased API error rates from 1:55 PM until 3:20 PM. This also affected provisioning of new resources that rely on Route 53 DNS, such as EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector, Global Accelerator, RDS, SageMaker Ground Truth, Amazon Managed Blockchain, Directory Service and Elastic Inference. The Route 53 API is now operating normally, and all changes that were accepted by the Route 53 API have been propagated. Queries for all existing records were answered normally during this time.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation Delays",
      "date": "1580371190",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:59 PM PST</span>&nbsp;We are investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">Jan 30, 12:15 AM PST</span>&nbsp;We have confirmed that we are seeing increased propagation times for changes to a few CloudFront edge locations. Majority of CloudFront edge locations are consuming configuration changes normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally. </div><div><span class=\"yellowfg\">Jan 30,  1:23 AM PST</span>&nbsp;Between January 29 9:12 PM and January 30 12:48 AM PST we experienced delays in propagation times for changes to CloudFront configurations. During this time end-user requests for content from our edge locations were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rate",
      "date": "1580754533",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:28 AM PST</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;We have identified the root cause of the increased error rates and continue to work toward full resolution.</div><div><span class=\"yellowfg\">11:24 AM PST</span>&nbsp;Between 9:55 and 11:17 AM PST, we experienced increased error rates and latencies in the US-EAST-1 region. Traffic on existing load balancers was unaffected. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Certificate Manager (N. Virginia)",
      "summary": "[RESOLVED]  Certificate Issuance Delays",
      "date": "1580774436",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:00 PM PST</span>&nbsp;We are investigating delays in certificate issuance globally. This does not impact existing certificates. Other dependent services utilizing Certificate Manager may be affected.</div><div><span class=\"yellowfg\"> 4:34 PM PST</span>&nbsp;Between 12:30 PM and 3:50 PM PST, we experienced delays in issuance of new certificates globally. Existing certificates were unaffected. The issue has been resolved and the service is operating normally. </div>",
      "service": "certificatemanager-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Increased Console Errors",
      "date": "1581553041",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:17 PM PST</span>&nbsp;Between 1:10 PM and 3:47 PM PST we experienced periods of increased error rates when accessing the CloudFront Management Console. Customers may have received a 404 Response. Existing distributions and the CloudFront APIs were not impacted by this issue. The issue has been resolved and the CloudFront Management Console is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Auto Scaling (US-West)",
      "summary": "[RESOLVED] Increased error rates and latencies",
      "date": "1581624138",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:02 PM PST</span>&nbsp;We are investigating increased error rates and latencies for Autoscaling API calls in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">12:19 PM PST</span>&nbsp;Between 11:34 AM and 12:05 PM PST we experienced increased error rates and latencies for Autoscaling API calls in the US-GOV-WEST-1 Region. The issue is resolved and the service is operating normally.</div>",
      "service": "autoscaling-us-gov-west-1"
    },
    {
      "service_name": "Amazon CloudWatch (N. Virginia)",
      "summary": "[RESOLVED] Elevated query error rates and delays for CloudWatch Logs Insights in US-EAST-1 Region",
      "date": "1583302711",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:18 PM PST</span>&nbsp;We can confirm increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. Customers running queries using the StartQuery API may not be able to successfully schedule the query execution. Log events continue to be available through the CloudWatch Logs GetLogEvents and FilterLogEvents APIs. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">11:02 PM PST</span>&nbsp;We have identified the cause of the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. Customers running queries using the StartQuery API may not be able to successfully schedule the query execution. Log events continue to be available through the CloudWatch Logs GetLogEvents and FilterLogEvents APIs. We continue to work towards resolution.</div><div><span class=\"yellowfg\">Mar 4, 12:13 AM PST</span>&nbsp;We have resolved the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. The Logs Insights system is currently processing data starting from 8:25 PM PST until now to make it available to query. Until that process completes, customers running Logs Insights queries using the CloudWatch console or the SDK may not be able to see that data from that time period. We continue to work towards full resolution.</div><div><span class=\"yellowfg\">Mar 4,  2:40 AM PST</span>&nbsp;We have resolved the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. The Logs Insights system is currently processing data starting from March 3 8:25 PM PST until now to make it available to query. Until that process completes, customers running Logs Insights queries using the CloudWatch console or the SDK may not be able to see that data from that time period. We have identified the issue and continue to work towards full resolution.</div><div><span class=\"yellowfg\">Mar 4,  6:40 AM PST</span>&nbsp;Between March 3, 8:25 PM and March 4, 6:17 AM PST, some customers experienced increased delays for log events in query results in CloudWatch Logs Insights in the US-EAST-1 Region. We are in the process of backfilling the data. We have resolved the issue and the service is operating normally.</div>",
      "service": "cloudwatch-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates and latencies",
      "date": "1583444770",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:46 PM PST</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. This issue does not affect traffic on running load balancers.</div><div><span class=\"yellowfg\"> 2:05 PM PST</span>&nbsp;We can confirm increased error rates and latencies for ELB APIs in the US-EAST-1 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\"> 2:25 PM PST</span>&nbsp;We have identified the root cause of increased error rates and latencies for ELB APIs in the US-EAST-1 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\"> 2:48 PM PST</span>&nbsp;We have addressed one of the causes of increased error rates and latencies for ELB APIs in the US-EAST-1 Region, and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\"> 3:33 PM PST</span>&nbsp;We have recovered from increased error rates and latencies for ELB APIs in the US-EAST-1 Region, and continue to work towards resolution of increased back-end instance registration times. Traffic remains unaffected on running load balancers.</div><div><span class=\"yellowfg\"> 3:52 PM PST</span>&nbsp;Between 1:25 PM and 3:40 PM PST, we experienced increased API error rates and latencies as well as increased back-end instance registration times in the US-EAST-1 Region. The issue is resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sydney)",
      "summary": "[RESOLVED] EC2 Launch Failures",
      "date": "1583881487",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:04 PM PDT</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;We are still investigating increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region and continue working towards resolution.</div><div><span class=\"yellowfg\"> 5:04 PM PDT</span>&nbsp;Between 2:15 PM and 4:40 PM PDT we experienced increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-2"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased Route 53 Console Errors",
      "date": "1583949127",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:52 AM PDT</span>&nbsp;We have confirmed an issue with the Route 53 console which is causing an increased error rate. All calls to the Route 53 API and DNS servers are being answered normally.</div><div><span class=\"yellowfg\">11:11 AM PDT</span>&nbsp;Between 10:29 AM and 10:37 AM PDT customers experienced an elevated error rate when accessing the Route 53 Console. The issue has been resolved and all console requests are being answered normally. There was no impact to the Route 53 API, DNS, or Health Checking services, which were all operating normally during this time.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (US-West)",
      "summary": "[RESOLVED] Increased Launch Error Rates",
      "date": "1584350375",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:19 AM PDT</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:37 AM PDT</span>&nbsp;We are currently experiencing increased error rates for new instance launches in the US-GOV-WEST-1 Region. Existing instances are not affected.</div><div><span class=\"yellowfg\"> 3:18 AM PDT</span>&nbsp;Between 1:47 AM and 3:02 AM PDT we experienced increased error rates for new instance launches in the US-GOV-WEST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-gov-west-1"
    },
    {
      "service_name": "AWS CodeBuild (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates for Builds",
      "date": "1585097779",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:56 PM PDT</span>&nbsp;We can confirm increased Build error rates in the US-WEST-2 Region. We have identified the root cause and are working toward resolution. </div><div><span class=\"yellowfg\"> 6:26 PM PDT</span>&nbsp;Between 4:05 PM and 5:51 PM PDT we experienced increased build errors due to a missing dependency in the build workflow. The issue has been resolved and the service is operating normally. </div>",
      "service": "codebuild-us-west-2"
    },
    {
      "service_name": "AWS Marketplace",
      "summary": "[RESOLVED] Increased Subscription Error Rates ",
      "date": "1585144373",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:52 AM PDT</span>&nbsp;We are investigating increased AWS Marketplace subscription error rates. </div><div><span class=\"yellowfg\"> 7:21 AM PDT</span>&nbsp;We have identified the cause of the increased AWS Marketplace subscription error rates and continue working towards resolution.</div><div><span class=\"yellowfg\"> 8:41 AM PDT</span>&nbsp;Between 5:05 AM and 8:30 AM PDT we experienced increased AWS Marketplace subscription error rates. The issue has been resolved and the service is operating normally.</div>",
      "service": "marketplace"
    },
    {
      "service_name": "Amazon Simple Email Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Email Receiving Latencies",
      "date": "1585342008",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:46 PM PDT</span>&nbsp;We are currently investigating elevated latencies in our email-sending APIs in the US-EAST-1 Region. This includes the SendEmail/SendRawEmail APIs as well as calls made to the SMTP endpoint.</div><div><span class=\"yellowfg\"> 2:20 PM PDT</span>&nbsp;Between 12:25 PM and 1:52 PM PDT we experienced elevated delivery delays in the US-EAST-1 Region for mail sent to a specific external email provider. This issue impacted the SendEmail/SendRawEmail APIs as well as calls made to the SMTP endpoint. All delayed emails have now been delivered. The issue has been resolved and the service is operating normally. </div>",
      "service": "ses-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] CloudFront High error rates",
      "date": "1585610898",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:28 PM PDT</span>&nbsp;We are investigating elevated error rates and elevated latency in multiple edge locations. </div><div><span class=\"yellowfg\"> 5:08 PM PDT</span>&nbsp;We can confirm elevated error rates and high latency accessing content from multiple Edge Locations, which is also contributing to longer than usual propagation times for changes to CloudFront configurations. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 5:54 PM PDT</span>&nbsp;We are beginning to see recovery for the elevated error rates and high latency accessing content from multiple Edge Locations. Error rates have recovered for all locations except for Europe. Additionally, we continue to work toward recovery for the increased delays in propagating configuration changes to Cloudfront Distributions. </div><div><span class=\"yellowfg\"> 6:21 PM PDT</span>&nbsp;Starting 3:18 PM PDT, we experienced elevated error rates and high latency accessing content from multiple Edge Locations. The elevated error rates and elevated latency accessing content were fully recovered at 5:48 PM PDT. During this time, customers may also have experienced longer than usual change propagation delays for CloudFront configurations and invalidations. The backlog of CloudFront configuration changes and invalidations were fully processed by 6:14 PM PDT. All issues have been fully resolved and the system is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased Launch Errors &amp; API Errors",
      "date": "1586473044",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:57 PM PDT</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 4:21 PM PDT</span>&nbsp;We can confirm increased error rates for new launches and API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 4:54 PM PDT</span>&nbsp;We have identified the root cause resulting in increased error rates for new instance launches and API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. We are seeing signs of recovery and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 5:48 PM PDT</span>&nbsp;Between 3:26 PM and 5:44 PM PDT we experienced increased error rates for new instance launches and periods of API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Simple Queue Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated Error Rates For FIFO Queues",
      "date": "1587119196",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:26 AM PDT</span>&nbsp;We are investigating increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region. All other queues are operating normally.\n</div><div><span class=\"yellowfg\"> 4:10 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region and continue to work towards resolution. All other queues are operating normally and newly created FIFO queues will work without error.</div><div><span class=\"yellowfg\"> 5:05 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region, and we are currently in the process of deploying the fix.</div><div><span class=\"yellowfg\"> 5:17 AM PDT</span>&nbsp;Between 2:01 AM PDT and 5:07 AM PDT we experienced increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sqs-us-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (Tokyo)",
      "summary": "[RESOLVED] Alarm delays",
      "date": "1587379127",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:38 AM PDT</span>&nbsp;現在、AP-NORTHEAST-1 リージョンのいくつかのアラームの処理における増加したレイテンシーについて調査を行なっております。| We are investigating increased latencies for processing some alarms in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 4:23 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンのいくつかのアラーム処理の遅延について引き続き調査しております。問題の解決に向けて対応しております。| We continue to investigate delays in processing some alarms in the AP-NORTHEAST-1 Region. We continue to work toward resolution. </div><div><span class=\"yellowfg\"> 6:18 AM PDT</span>&nbsp;4/20 19:03から21:42(JST)にかけて AP-NORTHEAST-1 リージョンにてお客様がいくつかのアラームの遅延が発生していた可能性がございます。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 3:03 AM and 5:42 AM PDT, customers may have experienced some delayed alarms in the AP-NORTHEAST-1 Region. We have resolved the issue and the service is operating normally. </div>",
      "service": "cloudwatch-ap-northeast-1"
    },
    {
      "service_name": "Amazon Simple Queue Service (Tokyo)",
      "summary": "[RESOLVED] Elevated Error Rates",
      "date": "1587379347",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:42 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレートの上昇について調査を行なっております。 | We are investigating elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region</div><div><span class=\"yellowfg\"> 4:21 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレート上昇を認識しております。| We can confirm elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 5:45 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレート上昇の原因を特定いたしました。現在問題の軽減に向けて対応を進めております。| We have identified the cause of the increased error rates for send and receive operations in the AP-NORTHEAST-1 Region. We are working towards mitigation.</div><div><span class=\"yellowfg\"> 6:28 AM PDT</span>&nbsp;4/20 18:56から22:04(JST)にかけて AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレートの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 2:56 AM and 6:04 AM PDT, we experienced elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sqs-ap-northeast-1"
    },
    {
      "service_name": "AWS Lambda (Tokyo)",
      "summary": "[RESOLVED] Elevated Error Rates",
      "date": "1587379534",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:45 AM PDT</span>&nbsp;現在、 AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおけるエラーレートの上昇について調査を行なっております。 | We are investigating elevated error rates for asynchronous invocations and control plane APIs in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 6:42 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおけるエラーレートの上昇につきまして、原因を特定いたしました。現在では大幅なエラーレートの減少を確認できており、完全な復旧のために継続して対応を行なっております。 | We identified the cause of the increased error rates for asynchronous invocations and write control plane APIs in the AP-NORTHEAST-1 Region. We are now seeing significantly reduced error rates, and continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 7:09 AM PDT</span>&nbsp;4/20 19:03から22:50(JST)にかけて AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおいてエラーレートの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 3:03 AM and 6:50 AM PDT, we we experienced increased error rates for asynchronous invocations and write control plane APIs in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-ap-northeast-1"
    },
    {
      "service_name": "AWS CloudFormation (Tokyo)",
      "summary": "[RESOLVED] Elevated Error Rates",
      "date": "1587380986",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:09 AM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョンのすべてのスタック操作のエラーレート上昇および遅延について調査を行なっております。| We are investigating elevated error rates and latencies for all stack operations in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 6:24 AM PDT</span>&nbsp;/20 19:00から21:40(JST)にかけて AP-NORTHEAST-1 リージョンにおいてエラーレートとレイテンシーの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。|Between 3:00 AM and 5:40 AM PDT we experienced increased error rates and latencies in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. | Between 3:00 AM and 5:40 AM PDT we experienced increased error rates and latencies in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1587462325",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:45 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:16 AM PDT</span>&nbsp;Between 2:10 AM and 2:59 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Delays in Invalidation Change Times",
      "date": "1587509850",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:57 PM PDT</span>&nbsp;We are investigating longer than usual invalidation change times for changes to CloudFront configurations. This issue is not impacting propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 4:28 PM PDT</span>&nbsp;Between 2:26 PM and 4:20 PM PDT, we experienced delays in invalidation times for changes to CloudFront configurations. During this time end-user requests for content and propagation times for changes to CloudFront configurations were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "AWS CloudFormation (Oregon)",
      "summary": "Increased Error Rates",
      "date": "1588279630",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:47 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for AWS CloudFormation stack operations in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 2:38 PM PDT</span>&nbsp;We have identified the root cause for the increased error rates and latencies for AWS CloudFormation stack operations in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 3:10 PM PDT</span>&nbsp;Between 12:02 PM and 2:35 PM PDT we experienced increased error rates and latencies for AWS CloudFormation Stacks operations in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. California)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1589077890",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:31 PM PDT</span>&nbsp;We are investigating elevated API error rates and latencies in the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:13 PM PDT</span>&nbsp;We can confirm elevated API error rates and latencies in the US-WEST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:06 PM PDT</span>&nbsp;We have identified the cause of the elevated API error rates and latencies in the US-WEST-1 Region and continue working towards resolution. Running tasks are not impacted.</div><div><span class=\"yellowfg\"> 9:52 PM PDT</span>&nbsp;Between 6:53 PM and 9:37 PM PDT we experienced elevated API error rates and latencies in the US-WEST-1 Region. The issue has been resolved and the service is operating normally. Running tasks were not impacted.</div>",
      "service": "ecs-us-west-1"
    },
    {
      "service_name": "AWS Batch (N. California)",
      "summary": "[RESOLVED]  Increased Delays in Job State Transition",
      "date": "1589078329",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:38 PM PDT</span>&nbsp;We are investigating delay in job state transitions of AWS Batch Jobs in the US-WEST-1 Region. </div><div><span class=\"yellowfg\"> 8:32 PM PDT</span>&nbsp;We can confirm increased delay in job state transitions of AWS Batch Jobs in the US-WEST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:12 PM PDT</span>&nbsp;We have identified the cause of increased delays in job state transitions of AWS Batch Jobs in the US-WEST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:55 PM PDT</span>&nbsp;Between 6:55 PM and 9:38PM PDT we experienced delayed job state transitions of AWS Batch Jobs in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "batch-us-west-1"
    },
    {
      "service_name": "Amazon OpenSearch Service (N. Virginia)",
      "summary": "Increased indexing and query error rates for some VPC Domains",
      "date": "1590072270",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:44 AM PDT</span>&nbsp;We are experiencing elevated error rates for indexing and query operations affecting a small number of VPC domains in the US-EAST-1 Region. We have identified the issue and working on recovery. </div><div><span class=\"yellowfg\"> 8:38 AM PDT</span>&nbsp;Between 2:55 AM and 8:30 AM PDT, we experienced elevated error rates for indexing and query operations affecting a small number of VPC domains in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticsearch-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Registry (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1590187729",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:48 PM PDT</span>&nbsp;Between 2:48 PM and 3:10 PM PDT Amazon ECR experienced certificate errors while using Docker clients in the US-WEST-2 Region. We have resolved the issue and the service is operating normally.</div>",
      "service": "ecr-us-west-2"
    },
    {
      "service_name": "Amazon Connect (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates ",
      "date": "1591222848",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:20 PM PDT</span>&nbsp;We are investigating intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 3:51 PM PDT</span>&nbsp;We have identified the root cause of the issue causing intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region and continue working towards resolution.</div><div><span class=\"yellowfg\"> 4:40 PM PDT</span>&nbsp;We are beginning to see recovery for intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region. We continue to work toward full recovery.</div><div><span class=\"yellowfg\"> 5:53 PM PDT</span>&nbsp;Between 2:04 PM and 5:10 PM PDT some Amazon Connect customers experienced issues accessing the Amazon Connect web application in the US-WEST-2 Region. For a portion of that time, some users may have seen error messages when accessing the Amazon Connect web application. The issue has been resolved and the service is operating normally.</div>",
      "service": "connect-us-west-2"
    },
    {
      "service_name": "Amazon DynamoDB (Sao Paulo)",
      "summary": "[RESOLVED] Increased 400 error rates ",
      "date": "1591406183",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:16 PM PDT</span>&nbsp;We are investigating increased error rates for API requests talking to DynamoDB streams in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:34 PM PDT</span>&nbsp;We have identified the root cause of increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. Restarting applications talking to streams will resolve the issue while we continue to work towards resolution.</div><div><span class=\"yellowfg\"> 7:43 PM PDT</span>&nbsp;We are beginning to see recovery for the increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. Restarting applications talking to streams will resolve the issue while we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 9:59 PM PDT</span>&nbsp;Between 3:50 PM and 9:53 PM PDT customers experienced increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "dynamodb-sa-east-1"
    },
    {
      "service_name": "Amazon Simple Notification Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates for SMS Delivery",
      "date": "1591822333",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:52 PM PDT</span>&nbsp;We are investigating elevated error rates and latencies when delivering SMS messages in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:33 PM PDT</span>&nbsp;We have identified the root cause of increased error rates and latencies when delivering SMS messages in the US-EAST-1 Region and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:47 PM PDT</span>&nbsp;Between 9:10 AM and 2:04 PM PDT, we experienced elevated error rates and latencies when delivering SMS messages in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased API Error Rates ",
      "date": "1591943413",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:30 PM PDT</span>&nbsp;We are investigating increased error rates and latencies on AWS IAM administrative APIs with potential impact in multiple regions. IAM role creation is impacted. Other AWS services whose features require these actions may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=\"yellowfg\">Jun 12, 12:03 AM PDT</span>&nbsp;We continue to investigate increased error rates and latencies on AWS IAM administrative APIs with potential impact in multiple regions. IAM role creation is impacted. Other AWS services like AWS CloudFormation whose features require these actions may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=\"yellowfg\">Jun 12,  2:12 AM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRole APIs and are working towards resolution. Other AWS services such as AWS CloudFormation whose features require these actions may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=\"yellowfg\">Jun 12,  3:30 AM PDT</span>&nbsp;We wanted to provide you with more details on the issue causing increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRole APIs. While we have identified the root cause and are working towards resolution, with an issue like this, it is always difficult to provide an accurate ETA, but we expect to restore access to the CreateRole and CreateServiceLinkedRole APIs within the next several hours. We are working through the recovery process now and will continue to keep you updated if this ETA changes. IAM user authentications and authorizations are not impacted. Other AWS services like AWS CloudFormation whose features require these actions may also be impacted.</div><div><span class=\"yellowfg\">Jun 12,  4:27 AM PDT</span>&nbsp;We are beginning to see improvements as we continue to work towards recovery for the AWS IAM CreateRole and CreateServiceLinkedRole API Error Rates. Full resolution is estimated to take a few hours. Other AWS services such as AWS CloudFormation whose features require these actions will continue to be impacted. User authentications and authorizations are not impacted.</div><div><span class=\"yellowfg\">Jun 12,  5:20 AM PDT</span>&nbsp;We continue to see significant recovery for impacted customers for the increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRoles APIs. We expect the residual error rates and latency to subside further over the next 2-3 hours. Other AWS services such as AWS CloudFormation whose features require these actions will see a similar overall reduction in errors during that time. User authentications and authorizations remain unimpacted.</div><div><span class=\"yellowfg\">Jun 12,  6:43 AM PDT</span>&nbsp;Between June 11 9:56 PM PDT and June 12 6:40 AM PDT, AWS IAM experienced increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRoles APIs. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">Jun 12,  7:24 PM PDT</span>&nbsp;Although this issue has been resolved and the service is operating normally, if you have any questions or any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at <a href=\"https://console.aws.amazon.com/support/\">https://console.aws.amazon.com/support/</a>.</div>",
      "service": "iam"
    },
    {
      "service_name": "AWS Organizations",
      "summary": "[RESOLVED]  Increased API Error Rates",
      "date": "1591948670",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:57 AM PDT</span>&nbsp;We are investigating increased error rates for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust may also be impacted when activating new organizations or accounts.</div><div><span class=\"yellowfg\"> 2:13 AM PDT</span>&nbsp;We have identified the root cause of the increased error rates for account creation and invitations for Organizations with all features enabled and are working towards resolution. Other AWS services using Organizations service trust may also be impacted when activating new organizations or accounts.</div><div><span class=\"yellowfg\"> 5:38 AM PDT</span>&nbsp;We confirm significant recovery for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust are also recovering when activating new organizations or accounts. In line with IAM, we expect the residual error rates and latency to subside further over the next 2-3 hours. </div><div><span class=\"yellowfg\"> 6:27 AM PDT</span>&nbsp;Between June 11 9:55 PM PDT and June 12 5:54 AM PDT, we experienced increased error rates for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust may have also been impacted when activating new organizations or accounts. The issue has been resolved and the service is operating normally. If you have any questions or operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at <a href=\"https://aws.amazon.com/support\">https://aws.amazon.com/support</a>.</div>",
      "service": "organizations"
    },
    {
      "service_name": "Amazon CloudWatch (Ireland)",
      "summary": "Elevated API faults and latencies in EU-WEST-1.",
      "date": "1592911198",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:19 AM PDT</span>&nbsp;We are investigating increased faults and latencies for CloudWatch APIs and metrics in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics.</div><div><span class=\"yellowfg\"> 4:55 AM PDT</span>&nbsp;Between 3:13 AM and 4:37 AM PDT, some customers experienced elevated faults when calling CloudWatch APIs in the EU-WEST-1 Region. Some metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudwatch-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Oregon)",
      "summary": "[RESOLVED] Increased Network Provisioning Latencies",
      "date": "1593207544",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;We are experiencing delayed network provisioning times for newly launched instances and newly mapped Elastic IP addresses in a single Availability Zone in the US-WEST-2 Region. Existing instances are not affected by this issue. </div><div><span class=\"yellowfg\"> 3:04 PM PDT</span>&nbsp;We have resolved the issue affecting network provisioning times for newly launched instances and newly mapped Elastic IP addresses in a single Availability Zone in the US-WEST-2 Region. Existing instances were not affected by this issue. The issue is resolved and the service is operating normally.</div>",
      "service": "ec2-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1594241886",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:58 PM PDT</span>&nbsp;We are investigating an increased API error rate for the DescribeInstances and DescribeImages APIs in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:11 PM PDT</span>&nbsp;Between 1:22 PM and 2:06 PM PDT we experienced increased error rates and latencies for the DescribeInstances and DescribeImages APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (London)",
      "summary": "[RESOLVED] Elevated latencies and Faults in EU-WEST-2",
      "date": "1594383041",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:10 AM PDT</span>&nbsp;We are investigating increased faults and latencies for CloudWatch APIs and metrics in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 5:24 AM PDT</span>&nbsp;We can confirm elevated API faults and some delayed metrics in EU-WEST-2 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 6:04 AM PDT</span>&nbsp;We have identified the root cause for elevated API faults and latencies for the GetMetricData and GetMetricWidgetImage APIs in the EU-WEST-2 Region. We are actively working to resolve the issue and are starting to see recovery.</div><div><span class=\"yellowfg\"> 6:32 AM PDT</span>&nbsp;We have identified the root cause for elevated API faults and latencies for the GetMetricData and GetMetricWidgetImage APIs in the EU-WEST-2 Region. We can confirm significant recovery and are working actively to fully resolve the issue.</div><div><span class=\"yellowfg\"> 8:02 AM PDT</span>&nbsp;Between 4:07 AM and 7:28 AM PDT, customers experienced elevated API fault rates and latencies in the EU-WEST-2 Region. Some metrics were also delayed initially. We have resolved the issue and the service is operating normally.</div>",
      "service": "cloudwatch-eu-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ireland)",
      "summary": "[RESOLVED] Network connectivity issues",
      "date": "1594732662",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:17 AM PDT</span>&nbsp;We are investigating network connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region.\n\n</div><div><span class=\"yellowfg\"> 6:37 AM PDT</span>&nbsp;Network connectivity has been restored for the vast majority of the affected instances in a single Availability Zone in the EU-WEST-1 Region. Some EBS volumes within the affected Availability Zone are also experiencing degraded performance. We continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 7:35 AM PDT</span>&nbsp;Starting at 5:35 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in a single Availability Zone in the EU-WEST-1 Region. By 6:00 AM PDT, power and networking connectivity had been restored for affected instances and by 6:31 AM PDT, degraded performance for affected EBS volumes had been resolved. By 7:08 AM PDT, the vast majority of affected instances had fully recovered. The small number of remaining instances are hosted on hardware which was adversely affected by the loss of power. While we will continue to work to recover all affected instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (US-East)",
      "summary": "[RESOLVED] Increased error rates ",
      "date": "1594851373",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:16 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-GOV-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:47 PM PDT</span>&nbsp;Between 2:39 PM and 3:38 PM PDT we experienced increased API error rates and latencies in the US-GOV-EAST-1 Region. The issue has been resolved and the service is operating normally.\n</div>",
      "service": "ec2-us-gov-east-1"
    },
    {
      "service_name": "Amazon Simple Workflow Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated API and Workflow Execution Latencies ",
      "date": "1595269046",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:17 AM PDT</span>&nbsp;Between 5:50 AM and 10:50 AM PDT, we experienced elevated API and workflow execution latencies in the US-EAST-1 Region. Some AWS services were also impacted during this period. The issue has been resolved and the services are operating normally.</div>",
      "service": "swf-us-east-1"
    },
    {
      "service_name": "AWS CloudFormation (N. Virginia)",
      "summary": "[RESOLVED] Elevated Stack Latencies",
      "date": "1595269919",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:32 AM PDT</span>&nbsp;Between 5:50 AM and 10:50 AM PDT, AWS CloudFormation experienced increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1595327154",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:25 AM PDT</span>&nbsp;Between 12:02 AM and 2:35 AM PDT AWS customers experienced increased error rates while calling the IAM assume role, get session token and other APIs with the long term credentials. As of 2:35 AM PDT, we are fully recovered and the issue is resolved now. Other AWS services such as AWS CloudFormation whose features require these actions experienced similar impact.</div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1595527657",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:07 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:23 PM PDT</span>&nbsp;We're seeing some recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region, but some API requests may see a \"request limit exceeded\" responses as we work towards full recovery. Some Instance status checks for newly launched EC2 instances in a single Availability Zone may return insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region, but some API requests may still see \"request limit exceeded\" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=\"yellowfg\"> 2:03 PM PDT</span>&nbsp;We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region and a smaller number of API requests are now returning \"request limit exceeded\" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event. </div><div><span class=\"yellowfg\"> 2:52 PM PDT</span>&nbsp;We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region. The API DescribeVolumes may return \"request limit exceeded\" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=\"yellowfg\"> 3:56 PM PDT</span>&nbsp;We are seeing recovery for increased EC2 API error rates and latencies with the exception of small number of DescribeVolumes calls which may return \"request limit exceeded\" responses as we continue to work on this issue. Instance status checks for EC2 instances have recovered. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=\"yellowfg\"> 5:02 PM PDT</span>&nbsp;We have resolved the issue causing periods of increased EC2 API error rates and latencies as well as insufficient-data results for EC2 instance and system status checks in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Route 53 Resolver (N. Virginia)",
      "summary": "[RESOLVED] DNS Propagation Delays for EC2 Instance Names ",
      "date": "1595861956",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:59 AM PDT</span>&nbsp;We are investigating VPC DNS propagation delays in the US-EAST-1 Region. This issue only affects resolution of newly launched EC2 instances that rely on public to private IP DNS mapping, such as newly launched Relational Database Service (RDS) instances. It will also affect configuration changes to VPC peering associations. We have identified root cause and are actively working towards identifying a mitigation.</div><div><span class=\"yellowfg\"> 8:24 AM PDT</span>&nbsp;We have identified a mitigation path and are working towards resolution. This issue only affects resolution of newly launched EC2 instances that rely on public to private IP DNS mapping, such as newly launched Relational Database Service (RDS) instances from within a VPC. It will also affect configuration changes to VPC peering associations.</div><div><span class=\"yellowfg\"> 9:26 AM PDT</span>&nbsp;We have confirmed our mitigation is correct, and are actively deploying it. Recovery is in progress now.</div><div><span class=\"yellowfg\">10:43 AM PDT</span>&nbsp;We are continuing to deploy the mitigation to this issue. Impact is limited to workloads that require resolving public DNS names to private IP addresses for instances launched in a newly created VPC, as well as reverse DNS lookups from within a newly created VPC. Resolution of all instances in existing VPCs continue to work normally.</div><div><span class=\"yellowfg\">12:04 PM PDT</span>&nbsp;We are continuing to deploy the mitigation to this issue. Customers may see partial recovery as we work towards full resolution.</div><div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;Mitigation efforts continue, and customers should see increased recovery as we work towards full resolution.</div><div><span class=\"yellowfg\"> 1:41 PM PDT</span>&nbsp;Between 6:32 AM and 1:35 PM PDT, customers experienced issues resolving newly created public EC2 instance names to private IPs within newly created VPCs in the US-EAST-1 Region. Creation of new Route 53 Resolver Endpoints was also delayed during this time. Newly created instances within existing VPCs resolved normally, and all other DNS functionality worked normally in all VPCs during this time. The issue has been resolved and all DNS queries are being answered normally.</div>",
      "service": "route53resolver-us-east-1"
    },
    {
      "service_name": "Amazon Route 53 Resolver (N. Virginia)",
      "summary": "[RESOLVED] Single AZ Intermittent DNS Lookup",
      "date": "1595927486",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:11 AM PDT</span>&nbsp;We are investigating an increase in DNS lookup failures from EC2 instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:57 AM PDT</span>&nbsp;We are continuing to investigate increased DNS resolution errors from EC2 instances in a single Availability Zone of the US-EAST-1 Region. This could also impact functionality of other services that use EC2, such as App Mesh, Cloud9, ElasticSearch, EMR, Managed Blockchain, SageMaker, Transfer for SFTP, WorkMail, and Glue.</div><div><span class=\"yellowfg\"> 3:44 AM PDT</span>&nbsp;We are implementing a mitigation to the increased DNS resolution errors from EC2 instances in a single Availability Zone in the US-EAST-1 Region, and are starting to see recovery. This issue could also impact functionality of other services that use EC2, such as App Mesh, Cloud9, ElasticSearch, EMR, Managed Blockchain, SageMaker, Transfer for SFTP, WorkMail, RDS, and Glue.</div><div><span class=\"yellowfg\"> 4:19 AM PDT</span>&nbsp;DNS resolution failures in a single Availability Zone in the US-EAST-1 Region have largely been mitigated, and we are continuing to work towards full mitigation. </div><div><span class=\"yellowfg\"> 4:37 AM PDT</span>&nbsp;Between 1:22 AM and 4:13 AM PDT, customers experienced an increase in DNS resolution errors from EC2 instances in a single Availability Zone of the US-EAST-1 Region. This could have also impacted functionality of other services that use EC2, such as RDS, SageMaker, EMR, WorkMail, MSK, AWS IoT Analytics Service, Amazon ElasticSearch, Cloud9, AppMesh, Amazon Managed Blockchain, Glue, and AWS Transfer. The issue has been resolved and all DNS queries are being answered normally.</div>",
      "service": "route53resolver-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED]  Increased API Error Rates ",
      "date": "1596028900",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:21 AM PDT</span>&nbsp;We have identified the cause of the increased API error rates in a single Availability Zone in the US-EAST-1 Region and continue working towards resolution. Customers experiencing errors launching new EC2 instances may attempt to launch their EC2 instances in another Availability Zone. Existing running instances are unaffected.</div><div><span class=\"yellowfg\"> 8:23 AM PDT</span>&nbsp;We want to provide more information on this issue and progress toward resolution. At 5:18 AM PDT, we began experiencing increased API errors that originated from one of our EC2 sub-systems that is responsible for managing EC2 instances. This resulted in increased error rates for some EC2 APIs and affected new instance launches in a Single Availability Zone. The root cause of the error rates affecting the sub-system has been identified and engineers are currently working on resolving the issue. Existing instances remain unaffected by this issue.</div><div><span class=\"yellowfg\">10:19 AM PDT</span>&nbsp;We have deployed a fix to the impacted EC2 sub-system causing increased API error rates and new instance launch failures in a Single Availability zone in the US-EAST-1 Region and are beginning to see recovery. We continue to work towards full resolution. Existing instances remain unaffected by this issue.\n</div><div><span class=\"yellowfg\">10:52 AM PDT</span>&nbsp;Between 5:18 AM and 10:25 AM PDT we experienced increased error rates for some EC2 APIs and new instance launches in a Single Availability Zone in the US-EAST-1 region. Existing instances were unaffected. We are working to address API errors affecting a small number of EBS volumes as a result of this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. California)",
      "summary": " [RESOLVED] Instance Connectivity and Instance Status Check Metrics",
      "date": "1597885789",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:09 PM PDT</span>&nbsp;Between 4:38 PM and 5:35 PM PDT a small number of instances were unavailable due to power loss in a single Availability Zone (usw1-az1)  in the US-WEST-1 Region. Those customers received notification on their Personal Health Dashboard. During this time, some customers also experienced insufficient-data in the results from EC2 instance status checks for instances that were not impacted by the power event. During this time, CloudWatch alarms may have transitioned into \"INSUFFICIENT_DATA\" state if set on the delayed metrics. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-us-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] Instance Connectivity",
      "date": "1598350861",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:21 AM PDT</span>&nbsp;We are investigating instance connectivity issues in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 3:52 AM PDT</span>&nbsp;We are experiencing instance connectivity issues in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. As a result we are experiencing elevated API latencies for multiple APIs, as well as elevated API failure rates on the CreateSnapshot API only. We are also experiencing elevated instance launch failure rates in the affected Availability Zone.</div><div><span class=\"yellowfg\"> 4:46 AM PDT</span>&nbsp;We have restored connectivity to some of the instances in the affected Availability Zone (euw2-az2), and API latencies and launch failure rates are largely recovered. Access to some EBS volumes in the affected Availability Zone was also affected. We are working to restore connectivity to the remaining affected instances and volumes.</div><div><span class=\"yellowfg\"> 5:50 AM PDT</span>&nbsp;We have restored connectivity to a further subset of the instances and volumes in the affected Availability Zone (euw2-az2). We are continuing to work to restore connectivity to the remaining affected instances and volumes.</div><div><span class=\"yellowfg\"> 7:01 AM PDT</span>&nbsp;We wanted to provide some more information for the event affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Starting at 2:05 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone. As a result of this we also experienced elevated API latencies for some of our APIs, elevated API failure rates on the CreateSnapshot API, and elevated instance launch failure rates in the affected Availability Zone. By 4:50 AM PDT, power and networking connectivity had been restored to the majority of affected instances, and degraded performance for the majority of affected EBS volumes had been resolved. While we will continue to work to recover the remaining instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div><div><span class=\"yellowfg\">12:13 PM PDT</span>&nbsp;Starting at 2:05 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone (euw2-az2) . By 4:50 AM PDT, power and networking connectivity had been restored to the majority of affected instances, and degraded performance for the majority of affected EBS volumes had been resolved. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "Amazon Relational Database Service (London)",
      "summary": "[RESOLVED] Instance Impairments",
      "date": "1598351776",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:36 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 4:19 AM PDT</span>&nbsp;We can confirm connectivity issues affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 5:24 AM PDT</span>&nbsp;We have restored connectivity to some of the instances in the affected Availability Zone (euw2-az2) in the EU-WEST-2 Region, and continue to work to restore connectivity to the remaining affected instances.</div><div><span class=\"yellowfg\"> 6:26 AM PDT</span>&nbsp;We have restored connectivity to a further subset of the instances in the affected Availability Zone (euw2-az2) in the EU-WEST-2 Region, and continue to work to restore connectivity to the remaining affected instances.</div><div><span class=\"yellowfg\"> 7:55 AM PDT</span>&nbsp;We wanted to provide additional information for the event affecting some RDS database instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Starting at 2:05 AM PDT we experienced power and network connectivity issues for some database instances within the affected Availability Zone. The affected Multi-AZ databases failed over to another Availability Zone as expected, and only Single-AZ databases remained affected. By 5:30 AM PDT, power and networking connectivity had been restored to the majority of affected Single-AZ database instances. We are continuing to work to recover the remaining instances.</div><div><span class=\"yellowfg\">12:29 PM PDT</span>&nbsp;We have recovered the vast majority of RDS instances affected by the power event within a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Instances in other Availability Zones remain unaffected by this event.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;Starting at 2:05 AM PDT we experienced power and network connectivity issues for some database instances within the affected Availability Zone. The affected Multi-AZ databases failed over to another Availability Zone as expected, and only Single-AZ databases remained affected. By 5:30 AM PDT, power and networking connectivity had been restored to the majority of affected Single-AZ database instances. Since the beginning of the impact, we have been working to recover the remaining database instances. A small number of remaining database instances are hosted on hardware which was adversely affected by the loss of power. For these database instances, we have provided additional recovery guidance via the Personal Health Dashboard. The issue has been resolved and the service is operating normally. </div>",
      "service": "rds-eu-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Oregon)",
      "summary": "[RESOLVED] Increased Provisioning and Registration Times",
      "date": "1598395861",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:51 PM PDT</span>&nbsp;Between 11:46 AM and 3:21 PM PDT, we experienced increased provisioning and registration times for load balancers in the US-WEST-2 Region. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-west-2"
    },
    {
      "service_name": "AWS Batch (N. Virginia)",
      "summary": "[RESOLVED] Increased Job Processing Delays",
      "date": "1598918190",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:56 PM PDT</span>&nbsp;We are investigating increased delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:27 PM PDT</span>&nbsp;We can confirm intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:58 PM PDT</span>&nbsp;We continue to investigate the root cause of intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and work toward resolution.</div><div><span class=\"yellowfg\"> 6:41 PM PDT</span>&nbsp;We continue to investigate intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and work toward resolution.</div><div><span class=\"yellowfg\"> 9:08 PM PDT</span>&nbsp;We recently experienced intermittent delayed job state transitions of AWS Batch Jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "batch-us-east-1"
    },
    {
      "service_name": "Amazon SageMaker (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates and Latencies for Multiple API operations",
      "date": "1598920406",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:33 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.</div><div><span class=\"yellowfg\"> 6:04 PM PDT</span>&nbsp;We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.</div><div><span class=\"yellowfg\"> 6:47 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region and work towards resolution. Previously created jobs and endpoints are unaffected.</div><div><span class=\"yellowfg\"> 9:04 PM PDT</span>&nbsp;Between 1:04 PM PDT and 8:40 PM PDT we experienced increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "sagemaker-us-east-1"
    },
    {
      "service_name": "AWS DeepRacer (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates for Training Jobs",
      "date": "1598923959",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:32 PM PDT</span>&nbsp;We are investigating increased error rates for training jobs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:06 PM PDT</span>&nbsp;We have confirmed intermittent issues with training jobs in the US-EAST-1 Region which result in them getting stuck in an “Initializing” state. We continue to work toward resolution.</div><div><span class=\"yellowfg\"> 9:10 PM PDT</span>&nbsp;Between 2:05 PM and 9:00 PM PDT we experienced increased error rates for training jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "deepracer-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Increased Propagation Time for Invalidations",
      "date": "1599068507",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:41 AM PDT</span>&nbsp;We are experiencing longer than usual propagation time for invalidations to a few edge locations in the CloudFront network. Changes for all other configurations are propagating normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">11:42 AM PDT</span>&nbsp;Between 9:30 AM and 11:38 AM PDT we experienced longer than usual propagation time for invalidations to a few edge locations in the CloudFront network. During this event changes for all other configurations propagated normally. Additionally, end-user requests for content from our edge locations were not affected by this issue and were being served normally. The issue is resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased Error Rates and Latencies",
      "date": "1599168415",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:26 PM PDT</span>&nbsp;We are investigating increased authentication error rates and latencies affecting IAM globally. Authenticated requests to other AWS services are also impacted. </div><div><span class=\"yellowfg\"> 2:35 PM PDT</span>&nbsp;We are seeing significant recovery for IAM authentication error rates and latencies and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 3:03 PM PDT</span>&nbsp;Between 2:02 PM and 2:23 PM PDT we experienced increased error rates and latencies for IAM operations in all AWS Regions. Attempts to create, describe, modify or delete IAM accounts and roles may have experienced a failure during this time. Authentication using IAM accounts and roles was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "AWS Storage Gateway (N. California)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026383",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:32 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 6:55 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-WEST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",
      "service": "storagegateway-us-west-1"
    },
    {
      "service_name": "AWS Storage Gateway (Frankfurt)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026390",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:33 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 7:05 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the EU-CENTRAL-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",
      "service": "storagegateway-eu-central-1"
    },
    {
      "service_name": "AWS Storage Gateway (Sao Paulo)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026396",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:34 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 7:06 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the SA-EAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",
      "service": "storagegateway-sa-east-1"
    },
    {
      "service_name": "AWS Storage Gateway (Singapore)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026398",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 5:40 PM PDT</span>&nbsp;</div><div><span class=\"yellowfg\"> 6:53 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-SOUTHEAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",
      "service": "storagegateway-ap-southeast-1"
    },
    {
      "service_name": "AWS Storage Gateway (Ireland)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026405",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 6:54 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the EU-WEST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",
      "service": "storagegateway-eu-west-1"
    },
    {
      "service_name": "AWS Storage Gateway (N. Virginia)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026407",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 6:32 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-EAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",
      "service": "storagegateway-us-east-1"
    },
    {
      "service_name": "AWS Storage Gateway (Oregon)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026408",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:29 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 6:30 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-WEST-2 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",
      "service": "storagegateway-us-west-2"
    },
    {
      "service_name": "AWS Storage Gateway (Sydney)",
      "summary": "[RESOLVED] Storage Gateway VMs Offline",
      "date": "1600026439",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:47 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:30 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 6:31 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-SOUTHEAST-2 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",
      "service": "storagegateway-ap-southeast-2"
    },
    {
      "service_name": "AWS Storage Gateway (Tokyo)",
      "summary": "[RESOLVED] Storage Gateway VM オフライン| Storage Gateway VMs Offline",
      "date": "1600026628",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:50 PM PDT</span>&nbsp;9/13 23:39 (JST) から、Storage Gateway VM がオフラインとして表示され、キャッシュ読み取りや、サービスへのアップロードを実行できないという問題が発生しています。ゲートウェイは引き続き書き込みを受け付けますが、問題が解消するまで、サービスへのアップロードを続行できません。根本原因は特定され、現在、解決に向けて取り組んでいます。\n--\nBeginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 2:59 PM PDT</span>&nbsp;Storage Gateway に影響する問題について、引き続き解決に向けて取り組んでいます。Gateway は引き続き書き込みを受け付けますが、この問題が解決するまでサービスへのアップロードを行うことができません。現時点で入手可能な情報に基づいて、完全な問題の解決には 2 時間かかると推定されます。現在、復旧プロセスを行っており、このタイムラインが変更された場合でも、引き続き最新の状況ををアップデートし続けます。\n--\nWe continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=\"yellowfg\"> 5:36 PM PDT</span>&nbsp;Storage Gateway に影響する問題について、一部の AWS リージョンで復旧を開始しています。AWS では影響を受けているすべての AWS リージョンの復旧に向けて引き続き取り組んでいます。復旧した場合、各 AWS リージョンのメッセージを更新します。-- We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=\"yellowfg\"> 6:36 PM PDT</span>&nbsp;日本時間 9/13 23:39 から、AP-NORTHEAST-1 リージョンの Storage Gateway に影響する問題が発生しました。イベント中、ゲートウェイはキャッシュからの読み取りを実行できず、SGW コンソールにオフラインとして表示されました。ゲートウェイは引き続き書き込みを受け付けましたが、AWS サービスにアップロードができませんでした。現在、問題は解決され、サービスは正常に動作しています。-- Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-NORTHEAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",
      "service": "storagegateway-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1600266641",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:30 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.\n\n</div><div><span class=\"yellowfg\"> 7:45 AM PDT</span>&nbsp;Between 6:55 AM and 7:29 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased Error Rates &amp; Latencies",
      "date": "1600294646",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;We are investigating increased authentication error rates and latencies affecting IAM. IAM related requests to other AWS services may also be impacted.</div><div><span class=\"yellowfg\"> 3:22 PM PDT</span>&nbsp;We can confirm error rates and latencies affecting IAM. IAM related requests to other AWS Services may also be impacted. </div><div><span class=\"yellowfg\"> 3:41 PM PDT</span>&nbsp;We can confirm that we are experiencing increased error rates and latencies for IAM operations in all AWS Regions. Attempts to create, describe, modify or delete IAM accounts and roles may be experiencing elevated failures. Authentication using IAM accounts and roles are not affected. We have identified the root cause and are working toward resolution.</div><div><span class=\"yellowfg\"> 4:12 PM PDT</span>&nbsp;We continue to work towards resolution of the increased error rates and latencies for IAM operations in all AWS Regions. The issue continues to affect create, describe, modify or delete of IAM accounts and roles. Other AWS Services that perform these IAM mutating operations may be impacted during this issue. Authentication using IAM accounts and roles are not affected.</div><div><span class=\"yellowfg\"> 4:20 PM PDT</span>&nbsp;We are seeing recovery for the increased error rates and latencies affecting IAM operations in all AWS Regions. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 4:30 PM PDT</span>&nbsp;We continue to observe significant recovery for the increased error rates and latencies affecting IAM operations in all AWS Regions. As we work toward full recovery, mutating IAM operations may take longer than normal to propagate. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 5:05 PM PDT</span>&nbsp;We continue to work toward full resolution. We are currently investigating increased latency which may result in intermittent timeouts for Read/Describe IAM Operations for some customers.</div><div><span class=\"yellowfg\"> 5:42 PM PDT</span>&nbsp;Between 2:48 PM and 5:28 PM PDT we experienced periods of increased error rates and latencies for IAM operations impacting all AWS Regions. This issue affected mutating operations of IAM Users and Roles as well as Assume Role. Other AWS Services that performed these IAM operations were also impacted. Authentication using existing IAM users and roles was not affected. The majority of the impact was mitigated at 4:12 PM, and the issue was fully resolved at 5:28 PM. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1600360315",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:31 AM PDT</span>&nbsp;We are investigating increased API error rates for the SubscribeToShard API in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 9:55 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API error rates for the SubscribeToShard API in the US-WEST-2 Region and we are currently mitigating the impact.</div><div><span class=\"yellowfg\">10:07 AM PDT</span>&nbsp;Between 06:20 AM and 09:56 AM PDT we experienced an increased error rate for the SubscribeToShard API, affecting some customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sao Paulo)",
      "summary": "[RESOLVED] Network connectivity",
      "date": "1600380261",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:04 PM PDT</span>&nbsp;Between 1:37 PM and 2:35 PM PDT, we experienced intermittent connectivity issues from and to instances in a single Availability Zone (sae1-az1) in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-sa-east-1"
    },
    {
      "service_name": "Auto Scaling (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1600411909",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:51 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-WEST-2 Region</div><div><span class=\"yellowfg\">Sep 18, 12:26 AM PDT</span>&nbsp;We continue to investigate increased API error rates and latencies in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">Sep 18,  1:17 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies in the US-WEST-2 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\">Sep 18,  3:24 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API error rates and latencies in the US-WEST-2 Region and continue working towards resolution.</div><div><span class=\"yellowfg\">Sep 18,  3:51 AM PDT</span>&nbsp;Between September 17 8:52 PM and September 18 3:24 AM PDT, we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "autoscaling-us-west-2"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] 日本のエッジロケーションの一部でエラーが上昇しておりました。| Elevated Errors from one of our edge locations in Japan ",
      "date": "1601153265",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:47 PM PDT</span>&nbsp;日本時間 9/26 PM 5:55 から PM 6:45 の間、CloudFrontをご利用中の一部のお客様で、日本のエッジロケーションで断続的にエラーが発生しておりました。現在、問題は解決され、サービスは正常に動作しています。| Between 5:55 PM and 6:45 PM JST, some CloudFront customers may have experienced intermittent errors from one of our edge locations in Japan. The issue has been resolved and service is operating normally. </div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Simple Notification Service (N. Virginia)",
      "summary": "[RESOLVED] Increased SNS Message Delivery Latency",
      "date": "1601479698",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:28 AM PDT</span>&nbsp;We are investigating increased delivery times for SNS messages in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:34 AM PDT</span>&nbsp;We have identified the cause of increased delivery times for SNS messages and are working towards recovery.</div><div><span class=\"yellowfg\">10:17 AM PDT</span>&nbsp;We have observed significant recovery for SNS message delivery times in the US-EAST-1 Region. All notifications continue to be delivered as we work towards full recovery.</div><div><span class=\"yellowfg\">11:14 AM PDT</span>&nbsp;We continue to observe significant recovery for SNS message delivery times in the US-EAST-1 Region. All notifications continue to be delivered as we to work towards full recovery.</div><div><span class=\"yellowfg\">11:42 AM PDT</span>&nbsp;Recently, we experienced increased SNS message delivery times in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. We are continuing to process pending message deliveries for a small number of customers.</div>",
      "service": "sns-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] CloudFront DNS Errors",
      "date": "1601574323",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:45 AM PDT</span>&nbsp;現在日本で CloudFront ディストリビューションの名前解決において断続的な DNS エラーが発生している事象について調査しております。\n| We are actively investigating intermittent DNS errors resolving some CloudFront distributions in Japan.\n</div><div><span class=\"yellowfg\">11:37 AM PDT</span>&nbsp;日本時間 10月2日 00:55 から 02:25 にかけて、日本の一部お客様において CloudFront ディストリビューションの DNS 名前解決が失敗する事象が発生しておりました。この問題は解決され、サービスは正常に稼働しています。\n| Between 8:55 AM and 10:25 AM PDT, some customers in Japan had a small percentage of DNS lookups fail when resolving CloudFront distributions. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased IAM Error Rates and Latencies",
      "date": "1601585449",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:50 PM PDT</span>&nbsp;We are investigating increased error rates and latencies affecting IAM. IAM related requests to other AWS services may also be impacted. </div><div><span class=\"yellowfg\"> 2:37 PM PDT</span>&nbsp;Between 1:10 PM and 1:50 PM PDT, we experienced increased error rates for mutating (create, update, delete) IAM APIs. IAM related requests to other AWS services may have been impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Network connectivity issues",
      "date": "1602233331",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:48 AM PDT</span>&nbsp;We are investigating networking connectivity issues for a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. We have identified root cause and are working towards resolution. Network connectivity for existing instances is not affected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.</div><div><span class=\"yellowfg\"> 2:47 AM PDT</span>&nbsp;We continue to work toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.</div><div><span class=\"yellowfg\"> 4:53 AM PDT</span>&nbsp;We are still working toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, launching a replacement instance may resolve the issue.</div><div><span class=\"yellowfg\"> 6:35 AM PDT</span>&nbsp;We are still working towards recovery for the ongoing networking connectivity issues. These affect a small subset of EC2 instances launched after October 08, 2020, at 9:37 PM PDT within a single Availability Zone (use1-az2) in the US-EAST-1 Region. For instances that are affected, customers can launch replacement instances in another Availability Zone.</div><div><span class=\"yellowfg\"> 9:57 AM PDT</span>&nbsp;We wanted to provide you with some more details on the issue affecting network connectivity for a subset of EC2 instances in a single Availability Zone (use1-az2) in the US-EAST-1 Region. The issue is affecting the subsystem responsible for updating VPC network configuration and mappings when new instances are launched or Elastic Network Interfaces (ENI) are attached to instances, within the affected Availability Zone. This subsystem makes use of a cell-based architecture, which subdivides the Availability Zone into smaller cells, with each cell being responsible for the VPC network configuration and mappings for a subset of instances within the Availability Zone.\n\nAt 9:37 PM PDT on October 8th, a single cell within this subsystem began experiencing elevated failures in updating VPC network configuration and mappings for instances managed by the affected cell. These elevated failures cause network configuration and mappings to be delayed or to fail for new instance launches and attachments of ENIs within the affected cell. The issue can also cause connectivity issues between an affected instance in the affected Availability Zone and newly launched instances within other Availability Zones in the US-EAST-1 Region, since updated VPC network configuration and mappings are not able to be updated within the affected Availability Zone.\n\nWe have identified the root cause and have been working to resolve the issue and restore the updating of VPC network configuration and mappings within the affected cell. For instances that are affected by this issue, relaunching the instance within the affected Availability Zone may mitigate the issue. If possible, relaunching the instance in other Availability Zones will mitigate the issue.\n\nWe will continue to provide updates as we work towards full resolution.</div><div><span class=\"yellowfg\">11:11 AM PDT</span>&nbsp;We have taken steps to address the issue affecting network connectivity for some instances in a single Availability Zone (use1-az2) in the US-EAST-1 Region. As of 10:20 AM PDT, we started to see recovery for affected instances and continue to work toward full resolution of the issue.\n</div><div><span class=\"yellowfg\">11:54 AM PDT</span>&nbsp;Starting at 9:37 PM PDT on October 8th, we experienced increased network connectivity issues for a subset of instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. This was caused by a single cell within the subsystem responsible for the updating VPC network configuration and mappings experiencing elevated failures. These elevated failures caused network configuration and mappings to be delayed or to fail for new instance launches and attachments of ENIs within the affected cell. The issue has also caused connectivity issues between an affected instance in the affected Availability Zone(use1-az2) and newly launched instances within other Availability Zones in the US-EAST-1 Region, since updated VPC network configuration and mappings were not able to be updated within the affected Availability Zone(use1-az2). The root cause of the issue was addressed and at 10:20 AM PDT on October 9th, we began to see recovery for the affected instances. By 11:10 AM PDT, all affected instances had fully recovered. The issue has been resolved and the service is operating normally</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Lambda (N. Virginia)",
      "summary": "[RESOLVED] Increased Invoke Error Rate",
      "date": "1602234228",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:03 AM PDT</span>&nbsp;We have identified an increase in invoke error rates in the US-EAST-1 Region and are working towards resolution.</div><div><span class=\"yellowfg\"> 3:11 AM PDT</span>&nbsp;Between October 8 10:35 PM and October 9 2:25 AM PDT we experienced increased Lambda invoke error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Sao Paulo)",
      "summary": "[RESOLVED] Internet connectivity issues ",
      "date": "1602367502",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:05 PM PDT</span>&nbsp;We are investigating internet connectivity issues in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:11 PM PDT</span>&nbsp;Between 2:26 PM and 3:00 PM PDT, customers experienced issues connecting to the SA-EAST-1 Region from the internet and other AWS Regions. The issue has been resolved and all services are operating normally.</div>",
      "service": "internetconnectivity-sa-east-1"
    },
    {
      "service_name": "AWS Marketplace",
      "summary": "[RESOLVED] Increased AWS Marketplace Subscription Error Rates and Latencies",
      "date": "1602664203",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:30 AM PDT</span>&nbsp;We can confirm increased AWS Marketplace subscription error rates and latencies. We have identified the cause of the impact and continue working towards resolution.</div><div><span class=\"yellowfg\"> 2:27 AM PDT</span>&nbsp;We can confirm increased AWS Marketplace subscription error rates and latencies. We are seeing partial recovery and are working towards full resolution. </div><div><span class=\"yellowfg\"> 2:54 AM PDT</span>&nbsp;Between October 13 8:45 PM and October 14 2:50 AM PDT, we experienced increased AWS Marketplace subscription error rates and latencies. The issue has been resolved and the service is operating normally.</div>",
      "service": "marketplace"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sydney)",
      "summary": "EC2 Instance Meta Data Service Errors ",
      "date": "1603333734",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:28 PM PDT</span>&nbsp;We are investigating increased errors for the EC2 Instance Meta Data Service in the AP-SOUTHEAST-2 Region. Applications within the instance attempting to query the Instance Meta Data Service may experience increased errors or timeouts. Since the Instance Meta Data Service may be used during the operating system boot process, some instances may be experiencing launch failures. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 8:46 PM PDT</span>&nbsp;We continue to investigate increased errors for the EC2 Instance Meta Data Service in the AP-SOUTHEAST-2 Region. We are seeing the most impact for newly launched instances. Applications within the instance attempting to query the Instance Meta Data Service may experience increased errors or timeouts. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 9:58 PM PDT</span>&nbsp;We have identified the root cause and resolved the issue causing increased errors and timeouts for the EC2 Instance Meta Data Service for newly launched instances in the AP-SOUTHEAST-2 Region. From 9:26 PM PDT, newly launched instances are no longer experiencing increased errors and timeouts for the EC2 Instance Meta Data Service. Instances that were affected by this issue will self-recover over the coming hours, but for immediate recovery, we recommend relaunching, or stopping and starting, the affected instances to resolve the issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Tokyo)",
      "summary": "[RESOLVED] ネットワーク接続性の問題 | Network Connectivity Issues ",
      "date": "1603336222",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:10 PM PDT</span>&nbsp;AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のEC2インスタンスおよびEBSボリュームのパフォーマンス低下に関するネットワーク接続性の問題を調査しております。 | We are investigating network connectivity issues for some EC2 instances and degraded volume performance for some EBS volumes in a single Available Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 8:41 PM PDT</span>&nbsp;一部のEC2インスタンスに影響を与えていたネットワーク接続性の問題は解消しました。AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のEBSボリュームのパフォーマンス低下の問題については、引き続き調査を行っております | The networking connectivity issues affecting some EC2 instances have been resolved but we continue to investigate degraded performance for some EBS volumes within the affected Availability Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region. </div><div><span class=\"yellowfg\"> 9:15 PM PDT</span>&nbsp;日本時間11:42から11:53の間、AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のインスタンスにおいて、ネットワーク接続性の問題がありました。また、日本時間11:42から13:09の間、同アベイラビリティゾーンにおける一部のEBSボリュームにおいて、ボリュームパフォーマンスの低下がありました。問題は解消し、現在正常に稼働しております | Between 7:42 PM PDT and 7:53 PM PDT we experienced network connectivity issues for some instances in a single Availability Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region. Between 7:42 PM and 9:09 PM PDT, we also had degraded volume performance for some EBS volumes in that Availability Zone. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1603669101",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:38 PM PDT</span>&nbsp;저희는 AP-NORTHEAST-2 리전의 API 오류율와 지연속도 증가를 조사 중에 있습니다. | We are investigating increased API error rates and latencies in the AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 5:27 PM PDT</span>&nbsp;AP-NORTHEAST-2 리전의 EC2 API에 3:46 PM 부터 5:08 PM PDT 까지 지연속도의 증가가 있었습니다. 해당 문제는 현재 해결되었으며 정상 동작 중입니다. | Between 3:46 PM and 5:08 PM PDT we experienced slightly increased latencies for the EC2 APIs in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "Amazon Route 53 Resolver (N. Virginia)",
      "summary": "[RESOLVED] Increased DNS query timeouts",
      "date": "1604345391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:29 AM PST</span>&nbsp;We are investigating intermittent increased DNS query timeouts in a single Availability Zone (use1-az6) of the US-EAST-1 Region. We are already seeing recovery but are working to confirm.</div><div><span class=\"yellowfg\">11:42 AM PST</span>&nbsp;Between 9:51 AM and 10:02 AM, and between 10:41 AM and 11:10 AM PST, customers experienced increased DNS query timeouts in a single Availability Zone (use1-az6) of the US-EAST-1 Region. The issue has been resolved and all DNS queries are being answered normally.</div>",
      "service": "route53resolver-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation Delays",
      "date": "1604706910",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:55 PM PST</span>&nbsp;We are investigating longer than usual propagation times for changes to CloudFront configurations to few of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally. </div><div><span class=\"yellowfg\"> 4:25 PM PST</span>&nbsp;We have identified the root cause of the longer than usual propagation times for changes to CloudFront configurations. We continue to work toward resolution. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 4:42 PM PST</span>&nbsp;Between 2:21 PM and 4:23 PM PST customers may have experienced longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] 네트워크 연결 | Network Connectivity",
      "date": "1605071272",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:07 PM PST</span>&nbsp;2020년 11월 11 일 오전 4:59에서 오전 11:25 (대한민국 시간 기준) 사이에 AP-NORTHEAST-2 Region 내 한 개의 Availability Zone에서 소수 instance의 연결 (connectivity) 이슈가 있었습니다. 해당 이슈는 해결 되었고 서비스는 정상적으로 제공되고 있습니다. 본 서비스의 이용을 복구하기 위하여 이용자가 추가적으로 조치할 사항은 없습니다. 만약 질문이 있으시거나 서비스 관련 운영상의 이슈가 있을경우, AWS Support Center상 다음 링크(https://console.aws.amazon.com/support ) 를 통하여 AWS Support Department에 연락하여 주시길 바랍니다. | Between 11:59 AM and 6:25 PM PST on November 10, 2020, a small number of instances experienced connectivity issues in a single Availability Zone in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.  Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "Amazon Athena (Mumbai)",
      "summary": "[RESOLVED] Increased Web Console Error Rate ",
      "date": "1605156361",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:46 PM PST</span>&nbsp;We are investigating increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region. The APIs and Command Line Interface (CLI) are not impacted by this issue.</div><div><span class=\"yellowfg\"> 9:44 PM PST</span>&nbsp;We can confirm increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region and continue working towards resolution. The APIs and Command Line Interface (CLI) are not impacted by this issue.</div><div><span class=\"yellowfg\">10:31 PM PST</span>&nbsp;We have identified the root cause of the issue causing increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region and continue working towards resolution. The APIs and Command Line Interface (CLI) are not impacted by this issue.</div><div><span class=\"yellowfg\">11:38 PM PST</span>&nbsp;Between 2:45 PM and 11:20 PM PST, we experienced intermittent increased error rates for requests to the Amazon Athena Management Console in the AP-SOUTH-1 Region. The APIs and Command Line Interface (CLI) were not impacted by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "athena-ap-south-1"
    },
    {
      "service_name": "AWS QuickSight (N. Virginia)",
      "summary": "[RESOLVED] Increased Website Latencies and Error Rates ",
      "date": "1605629180",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:06 AM PST</span>&nbsp;We are investigating increased website latencies and error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:49 AM PST</span>&nbsp;We have identified the root cause of the issue and are working towards resolution. We have started to see improvements in error rates and latencies in US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:23 AM PST</span>&nbsp;Between 7:01 AM and 9:14 AM PST, we experienced increased website latencies and error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "quicksight-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Sao Paulo)",
      "summary": "[RESOLVED] Network Connectivity ",
      "date": "1605835318",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:21 PM PST</span>&nbsp;We are investigating an issue which is affecting connectivity from the Internet and other AWS Regions for some customers using services in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:44 PM PST</span>&nbsp;Between 4:47 PM and 5:19 PM PST we experienced an issue which affected Internet connectivity for some customers using AWS services in the SA-EAST-1 Region. Connectivity between instances within the Region was not affected. The issue has been resolved and connectivity has been restored. </div>",
      "service": "internetconnectivity-sa-east-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (N. Virginia)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1605843388",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:36 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:12 PM PST</span>&nbsp;We are still investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:23 PM PST</span>&nbsp;Between 6:36 PM and 9:05 PM PST, Kinesis Data Streams experienced increased API error rates in the US-EAST-1 region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-us-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Errors and Delayed metrics",
      "date": "1605843713",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:41 PM PST</span>&nbsp;We are investigating increased delays for CloudWatch metrics in the US-EAST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics. </div><div><span class=\"yellowfg\"> 8:31 PM PST</span>&nbsp;We continue to investigate elevated API error rates and increased delays for CloudWatch metrics in the US-EAST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics.</div><div><span class=\"yellowfg\"> 9:32 PM PST</span>&nbsp;Between 7:16 PM and 9:08 PM PST, some customers experienced elevated error rates when calling CloudWatch APIs in the US-EAST-1 Region. Some metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics are in the process of backfilling in CloudWatch console graphs and for API retrieval. The service is operating normally.</div>",
      "service": "cloudwatch-us-east-1"
    },
    {
      "service_name": "Amazon AppStream 2.0 (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1605844702",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:58 PM PST</span>&nbsp;We are investigating increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:51 PM PST</span>&nbsp;We are continuing to investigate increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:26 PM PST</span>&nbsp;Between 6:46 PM and 8:59 PM PST, AppStream 2.0 experienced increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "appstream2-us-east-1"
    },
    {
      "service_name": "AWS WAF",
      "summary": "[RESOLVED] Elevated WAF Error Rates",
      "date": "1606064823",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:07 AM PST</span>&nbsp;We are investigating increased error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Customers who use AWS WAF with ALB in EU-WEST-1 may see increased ALB error rates.</div><div><span class=\"yellowfg\"> 9:15 AM PST</span>&nbsp;We are investigating increased error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Customers who use AWS WAF with ALB in EU-WEST-1 may see increased ALB error rates. Impacted customers can optionally set ALB to fail open instead of the default of failing closed: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#load-balancer-waf</div><div><span class=\"yellowfg\">10:22 AM PST</span>&nbsp;We are starting to see recovery of error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Impacted customers can optionally set ALB to fail open instead of the default of failing closed:  https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#load-balancer-waf</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;Between 7:54 AM and 9:43 AM PST, some customers may have experienced elevated error rates for request inspection by AWS WAF in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "awswaf"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (N. Virginia)",
      "summary": "[RESOLVED] Additional Information",
      "date": "1606310100",
      "status": "0",
      "details": "",
      "description": "<div><span class=\"yellowfg\">Nov 28, 12:05 AM PST</span>&nbsp;We’d like to share more information about the Kinesis event on Wednesday November 25th. Additional details are available <a href='https://aws.amazon.com/message/11201/'>here</a>. Should you have any questions, please contact <a href='https://aws.amazon.com/support'>AWS Support</a>.</div>",
      "service": "kinesis-us-east-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1606314960",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:36 AM PST</span>&nbsp;We are investigating increased error rates for Kinesis Data Streams APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PST</span>&nbsp;We are continuing to investigate increased Kinesis Data Streams API errors, and are working on identifying root cause.</div><div><span class=\"yellowfg\"> 8:12 AM PST</span>&nbsp;Kinesis Data Streams customers are still experiencing increased API errors. This is also impacting other services, including ACM, Amplify Console, API Gateway, AppStream2, AppSync, Athena, Cloudformation, Cloudtrail, CloudWatch, Cognito, Connect, DynamoDB, EventBridge, IoT Services, Lambda, LEX, Managed Blockchain, Resource Groups, SageMaker, Support Console, and Workspaces. We are continuing to work on identifying root cause.</div><div><span class=\"yellowfg\"> 8:52 AM PST</span>&nbsp;The Kinesis Data Streams API is severely impaired. This is also impacting other services, including ACM, Amplify Console, API Gateway, AppStream2, AppSync, Athena, CloudFormation, CloudTrail, CloudWatch, Cognito, Connect, DynamoDB, EventBridge, IoT Services, Lambda, LEX, Managed Blockchain, Resource Groups, SageMaker, Support Console, and Workspaces. We are actively working towards resolution.</div><div><span class=\"yellowfg\"> 9:32 AM PST</span>&nbsp;The Kinesis Data Streams API is currently impaired in the US-EAST-1 Region. As a result customers are not able to write or read data published to Kinesis streams.\n\nCloudWatch metrics and events are also affected, with elevated PutMetricData API error rates and some delayed metrics.\n\nWhile EC2 instances and connectivity remain healthy, some instances are experiencing delayed instance health metrics, but remain in a healthy state.\n\nAutoScaling is also experiencing delays in scaling times due to CloudWatch metric delays.\n\nThe issue is also affecting other services, including ACM, Amplify Console, API Gateway, AppMesh, AppStream2, AppSync, Athena, Batch, CloudFormation, CloudTrail, Cognito, Connect, DynamoDB, EventBridge, Glue, IoT Services, Lambda, LEX, Managed Blockchain, Marketplace, Personalize, RDS, Resource Groups, SageMaker, Support Console, Well Architected, and Workspaces. For further details on each of these services, please see the Personal Health Dashboard.\n\nOther services, like S3, remain unaffected by this event.\n\nThis issue has also affected our ability to post updates to the Service Health Dashboard.\n\nWe are continuing to work towards resolution.</div><div><span class=\"yellowfg\">11:23 AM PST</span>&nbsp;We continue to work towards recovery of the issue affecting the Kinesis Data Streams API in the US-EAST-1 Region. For Kinesis Data Streams, the issue is affecting the subsystem that is responsible for handling incoming requests. The team has identified the root cause and we continue to make progress in addressing the root cause. We are seeing some improvement in error rates, but continue to work towards full resolution.\n\nThe issue also affects other services, or parts of these services, that utilize Kinesis Data Streams within their workflows. While features of multiple services are impacted, some services have seen broader impact and service-specific impact details are included within Recent Events on the Service Health Dashboard.</div><div><span class=\"yellowfg\"> 1:59 PM PST</span>&nbsp;Kinesis Data Streams API requests are still significantly impaired. We have identified a mitigation for this issue, and are actively working towards resolution.</div><div><span class=\"yellowfg\"> 2:49 PM PST</span>&nbsp;Kinesis Data Streams API requests are still impaired but are starting to see recovery. We continue to actively work towards resolution.</div><div><span class=\"yellowfg\"> 4:42 PM PST</span>&nbsp;Kinesis Data Streams API operations are seeing gradual recovery but customers may continue to experience increased latencies and failure rates. We continue to actively work towards resolution.</div><div><span class=\"yellowfg\"> 6:32 PM PST</span>&nbsp;We have now fully mitigated the impact to the subsystem within Kinesis that is responsible for the processing of incoming requests and are no longer seeing increased error rates or latencies. However, we are not yet taking the full traffic load and are working to relax request throttles on the service. Over the next few hours we expect to relax these throttles to previous levels. We expect customers to begin seeing recovery as these throttles are relaxed over this timeframe.</div><div><span class=\"yellowfg\"> 8:53 PM PST</span>&nbsp;We are continuing to relax the request throttles for Kinesis Data Streams and are gradually increasing the traffic into the service. We have not yet enabled requests to Kinesis Data Streams from VPC Endpoints. The Kinesis Data Streams subsystem continues to operate normally, and we expect incremental recovery over the next few hours.</div><div><span class=\"yellowfg\"> 9:26 PM PST</span>&nbsp;We have now enabled a subset of requests to Kinesis Data Streams using VPC Endpoints.</div><div><span class=\"yellowfg\">10:06 PM PST</span>&nbsp;We have now enabled all requests to Kinesis Data Streams through Internet-facing endpoints. We are continuing to work to re-enable all requests to Kinesis Data Streams using VPC Endpoints.</div><div><span class=\"yellowfg\">11:00 PM PST</span>&nbsp;We have now enabled all requests to Kinesis Data Streams through both Internet-facing endpoints and VPC Endpoints.</div><div><span class=\"yellowfg\">Nov 26, 12:03 AM PST</span>&nbsp;Between 5:15 AM and 11:10 PM PST customers experienced a significant impairment to their Amazon Kinesis Data Streams API operations. We have identified the root cause and have completed immediate actions to prevent recurrence. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-us-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (N. Virginia)",
      "summary": "[RESOLVED] API Error Rates and Metric Delays",
      "date": "1606316400",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:38 AM PST</span>&nbsp;We are investigating increased error rates for CloudWatch PutMetricData API in US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:40 AM PST</span>&nbsp;We can confirm elevated PutMetricData API error rates and some delayed metrics in US-EAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 1:49 PM PST</span>&nbsp;We are still seeing elevated PutMetricData API error rates and some delayed metrics in US-EAST-1 Region. We continue to work toward full recovery.</div><div><span class=\"yellowfg\"> 3:42 PM PST</span>&nbsp;We continue to experience increased error rates impacting the PutMetricData API error rates, and delayed Cloudwatch metrics. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Once we have observed recovery, metrics will backfill if they were queued during the impact</div><div><span class=\"yellowfg\"> 5:31 PM PST</span>&nbsp;We continue experience API error rates and delayed Cloudwatch metrics. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Once we have observed recovery, metrics will backfill if they were queued during the impact.</div><div><span class=\"yellowfg\"> 8:18 PM PST</span>&nbsp;We continue to experience API error rates and delayed CloudWatch metrics. We expect to see recovery once the on-going Kinesis issue is fully resolved. Metrics will start to backfill once the Kinesis issue is resolved.</div><div><span class=\"yellowfg\">10:44 PM PST</span>&nbsp;We are beginning to see recovery in API error rates and delayed metrics as Kinesis recovers.</div><div><span class=\"yellowfg\">10:56 PM PST</span>&nbsp;Between 5:15 AM and 10:31 PM PST, some customers experienced elevated error rates when calling CloudWatch APIs in the US-EAST-1 Region. Metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into the INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics are in the process of backfilling in CloudWatch console graphs and for API retrieval. The service is operating normally.</div>",
      "service": "cloudwatch-us-east-1"
    },
    {
      "service_name": "Amazon Cognito (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1606316820",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:11 AM PST</span>&nbsp;We are investigating an increased API failure rate for Cognito User Pools and Identity Pools operations in the us-east-1 Region.</div><div><span class=\"yellowfg\"> 8:31 AM PST</span>&nbsp;We are continuing to experience increased API failure rates for Cognito User Pools and Identity Pools operations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:14 AM PST</span>&nbsp;We are continuing to experience increased API failure rates for Cognito User Pools and Identity Pools operations in the US-EAST-1 Region. Cognito customers are experiencing errors when authenticating end users or creating new accounts for their Cognito User pools and for obtaining temporary AWS credentials using Cognito Identity pools.</div><div><span class=\"yellowfg\">11:54 AM PST</span>&nbsp;We have identified the root cause and continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:48 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region and we see improvement in the error rates experienced by customers.</div><div><span class=\"yellowfg\"> 1:30 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region and we see further improvement in the error rates experienced by customers.</div><div><span class=\"yellowfg\"> 2:23 PM PST</span>&nbsp;We are in the final stages of recovery for Cognito User Pools and customers should be seeing significant improvement. We expect to be fully recovered within 30 minutes.</div><div><span class=\"yellowfg\"> 2:43 PM PST</span>&nbsp;Between 5:15 AM and 2:28 PM PST customers experienced increased API failure rates for Cognito User Pools and Identity Pools in the US-EAST-1 Region. This was due to an issue with Kinesis Data Streams. We have implemented a mitigation to this issue. Cognito is now operating normally.</div>",
      "service": "cognito-us-east-1"
    },
    {
      "service_name": "Amazon EventBridge (N. Virginia)",
      "summary": "[RESOLVED] Increased API error rates and event delivery latencies",
      "date": "1606319040",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:45 AM PST</span>&nbsp;We are investigating increased API error rates for EventBridge in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:37 AM PST</span>&nbsp;We are continuing to investigate increased API error rates for EventBridge in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:06 AM PST</span>&nbsp;We can confirm significant API faults and latencies for EventBridge in the US-EAST-1 Region, which is related to an ongoing issue with Kinesis Data Streams. We are continuing to work towards resolution.</div><div><span class=\"yellowfg\">10:44 AM PST</span>&nbsp;We can confirm significant API faults and latencies for EventBridge in the US-EAST-1 Region, which is related to an ongoing issue with Kinesis Data Streams. Currently, customer calls to the PutEvents API are failing with 5XX unavailable errors. In addition, rules subscribing to events from AWS services are delayed. We are continuing to work towards resolution.</div><div><span class=\"yellowfg\"> 1:36 PM PST</span>&nbsp;We have identified the root cause of elevated API errors and latencies in US-EAST-1. While we are seeing partial recovery, we continue to see elevated error rates and latency on API calls to PutEvents. Additionally, Rules subscribing to Events from other AWS Services continue to be delayed as we work toward full resolution.</div><div><span class=\"yellowfg\"> 3:24 PM PST</span>&nbsp;We have identified the root cause of elevated API errors and latencies in the US-EAST-1 Region and have begun implementing mitigations resulting in partial recovery. We continue to see elevated error rates and latency on API calls to PutEvents. Additionally, for Rules subscribing to Events from other AWS Services, latency is recovering but still elevated. We are continuing to work towards full resolution.</div><div><span class=\"yellowfg\"> 4:04 PM PST</span>&nbsp;We have identified the root cause of elevated API errors and latencies in the US-EAST-1 Region and have implemented mitigations resulting in significant improvement in error rates and latency on the PutEvents API. We continue to see delays in Rules subscribing to Events from other AWS Services and are working towards full resolution.</div><div><span class=\"yellowfg\"> 6:04 PM PST</span>&nbsp;Newly published events are no longer experiencing elevated delivery latencies, however, we continue to see delays in Rules subscribing to events from other AWS Services that were delivered earlier in the day and are working towards full resolution.</div><div><span class=\"yellowfg\"> 7:33 PM PST</span>&nbsp;Newly published events are no longer experiencing elevated delivery latencies. We continue to see delays in Rules subscribing to scheduled events that were delivered earlier in the day and we are working through backlog of these events over the next few hours.</div><div><span class=\"yellowfg\"> 9:52 PM PST</span>&nbsp;Between 5:15 AM and 9:30 PM PST, we experienced elevated API errors and event delivery latency in the US-EAST-1 Region related to an issue with Kinesis Data Streams. While the issue has been resolved and the service is operating normally, we are continuing to work through the backlog of AWS CloudTrail events which will be delivered over the next several hours.</div>",
      "service": "events-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation and Invalidations Reporting Delay",
      "date": "1606320420",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:54 AM PST</span>&nbsp;We are investigating longer than usual reporting update delays for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. Also, end-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">11:17 AM PST</span>&nbsp;We are investigating longer than usual reporting update delays for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. During this time, CloudFront Access Logs, Metrics and Reporting may also be affected. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 1:25 PM PST</span>&nbsp;We are working towards recovery for delays in reporting updates for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. CloudFront Access Logs, Metrics and Reporting may continue to be affected. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 2:50 PM PST</span>&nbsp;Change propagation of CloudFront configurations and invalidations have recovered and are operating normally. However, CloudFront Access Logs, Metrics and Reporting continue to be affected. End-user requests for content from our Edge Locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 4:16 PM PST</span>&nbsp;We are still observing partial recovery on Access Logs, Metrics and Reports but intermittent gaps and delays exist. We continue to work toward full resolution. End-user requests for content from our Edge Locations were not affected by this issue and continue to operate normally.</div><div><span class=\"yellowfg\"> 6:06 PM PST</span>&nbsp;We continue to work toward full resolution, and expect full recovery once the on-going Kinesis issue is resolved. Upon further recovery, we expect Access Logs, Metrics and Reports to fully recover and start backfilling over time for those queued during the impact.</div><div><span class=\"yellowfg\"> 9:44 PM PST</span>&nbsp;CloudFront Access Logs, Metrics, and Reporting continues to be affected by the Kinesis event but we are observing improving recovery. CloudFront edge locations are serving traffic as expected. Change propagation and cache invalidation times are operating within normal time windows.</div><div><span class=\"yellowfg\">11:51 PM PST</span>&nbsp;Between 5:41 AM and 2:40 PM PST, we experienced longer than usual reporting delays for Invalidations and CloudFront configurations to edge locations. Customer changes were propagating normally across our edge locations during this time but the associated reporting was not getting updated correctly. Between 5:41 AM and 11:26 PM PST, CloudFront Real-time Metrics were not available. CloudFront’s Real-time Metrics are now available in CloudWatch. The backlog of CloudFront Access Logs and Reports will be backfilled over the next few hours. During this time, all end-user requests for content from our edge locations were not affected by this issue and were being served normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Kubernetes Service (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Launches",
      "date": "1606324920",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:22 AM PST</span>&nbsp;We are investigating increased API error rates for cluster and node group operations in the US-EAST-1 region. We are also investigating increased Fargate pod launch failures. Existing EKS clusters and managed node groups are operating normally.</div><div><span class=\"yellowfg\">11:03 AM PST</span>&nbsp;Customer’s applications that are backed by pods already running are not impacted. We are continuing to experience API error rates in the US-EAST-1 region. EKS customers are experiencing errors when creating, upgrading and deleting EKS clusters and managed node groups. Existing managed node groups may experience errors scaling up or down. Customers will experience errors launching new Fargate pods.</div><div><span class=\"yellowfg\"> 1:55 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. Customer applications that are backed by pods already running are not impacted. Also applications and pods can be started and run on EC2 instances that are already part of the cluster. EKS customers are experiencing errors when creating, upgrading and deleting EKS clusters and managed node groups. Customers will experience errors launching new Fargate pods, running Fargate pods are not impacted.</div><div><span class=\"yellowfg\"> 3:10 PM PST</span>&nbsp;We continue working towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. EKS Fargate pod launches are now seeing recovery. Customer applications that are backed by pods already running are not impacted. Applications and pods can be started and run on EC2 instances that are already part of the cluster. Customers can also create Managed Node groups for existing clusters. EKS customers will still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=\"yellowfg\"> 4:29 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. EKS Fargate pod launches have now recovered and is operating normally. Customer applications that are backed by pods already running are not impacted. Applications and pods can be started and run on EC2 instances that are already part of the cluster. Customers can also create Managed Node groups for existing clusters. EKS customers will still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=\"yellowfg\"> 6:11 PM PST</span>&nbsp;We continue to observe partial recovery for EKS cluster and node group API operations in the US-EAST-1 Region and are working toward full resolution. Customers may still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=\"yellowfg\"> 9:48 PM PST</span>&nbsp;We continue to observe partial recovery for EKS cluster and node group API operations in the US-EAST-1 Region and are working towards full resolution. We expect to see complete recovery once the on-going Kinesis issue is fully resolved. Until then, customers may still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=\"yellowfg\">11:04 PM PST</span>&nbsp;Between 5:15 AM and 10:20 PM PST, we experienced elevated API errors for cluster, node group operations and Fargate pod launches US-EAST-1 Region. Existing clusters and node groups were unaffected during the event. The issue has been resolved and the service is operating normally.</div>",
      "service": "eks-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Launches",
      "date": "1606328375",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:22 AM PST</span>&nbsp;We are investigating increased API error rates and delays delivering task events and metrics in the US-EAST-1 region. We are also investigating increased task launch error rates for the Fargate launch type. Running tasks are not impacted.</div><div><span class=\"yellowfg\">11:07 AM PST</span>&nbsp;Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are continuing to experience API error rates and delays delivering task events and metrics in the US-EAST-1 region. ECS clusters are also not able to scale up or down due to task launch errors. Customers are missing metrics and events from their running tasks as ECS Insights is not able to propagate information. Task Set and Capacity Providers are also impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.</div><div><span class=\"yellowfg\"> 1:57 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. Customers are experiencing API error rates and delays delivering task events and metrics in the US-EAST-1 region. ECS clusters are also not able to scale up or down due to task launch errors. Customers are missing metrics and events from their running tasks as ECS Insights is not able to propagate information. Task Set and Capacity Providers are also impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.</div><div><span class=\"yellowfg\"> 2:13 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are seeing increased delivery rates for task events and metrics. ECS clusters are not able to scale up or down due to task launch errors with Task Set and Capacity Providers impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.</div><div><span class=\"yellowfg\"> 3:04 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. ECS on Fargate task launches are seeing recovery. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are seeing increased delivery rates for task events and metrics. ECS clusters using Capacity Providers are still seeing impact.</div><div><span class=\"yellowfg\"> 4:24 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery. Delivery of task events and metrics is starting to catch up, and API error rates are declining.</div><div><span class=\"yellowfg\"> 5:36 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery with a small number of task launches failing. Task event delivery is fully recovered. There continues to be higher than normal latencies for metrics due to continued CloudWatch impact.</div><div><span class=\"yellowfg\"> 7:16 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery with a very small number of task launches failing. Task event delivery is fully recovered. We are investigating slow deprovisioning of tasks. Capacity Providers and metrics delivery latency are both impacted until CloudWatch recovers.</div><div><span class=\"yellowfg\"> 9:06 PM PST</span>&nbsp;ECS is investigating slow deprovisioning of tasks causing tasks to remain in a deactivating or deprovisioning for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal. CloudWatch Container Insights is now correctly showing recent metrics from ECS. CloudWatch metrics for ECS and Capacity Providers are continuing to see impact while we wait for Kinesis and CloudWatch recovery.</div><div><span class=\"yellowfg\">10:49 PM PST</span>&nbsp;We are starting to see recovery for CloudWatch metrics for ECS and Capacity Providers. CloudWatch Container Insights is now showing recent metrics from ECS. ECS continues working to resolve slow tasks deprovisioning which causes tasks to remain in a deactivating or deprovisioning states for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.</div><div><span class=\"yellowfg\">Nov 26, 12:11 AM PST</span>&nbsp;CloudWatch metrics for ECS and Capacity Providers have recovered. We continue working to resolve slow task deprovisioning which is causing tasks to remain in a deactivating or deprovisioning state for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.</div><div><span class=\"yellowfg\">Nov 26, 12:29 AM PST</span>&nbsp;CloudWatch metrics for ECS and Capacity Providers have recovered. We expect to see recovery on slow task deprovisioning. which is causing tasks to remain in a deactivating or deprovisioning state for extended periods of time, once CloudMap is fully recovered. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.</div><div><span class=\"yellowfg\">Nov 26,  1:17 AM PST</span>&nbsp;Between November 25 5:15 AM and November 26 1:08 AM PST, we experienced elevated API and task launch error rates, delayed metrics impacting Capacity Provider scaling, CloudWatch Container Insights, and CloudWatch metrics for ECS. Running tasks were not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "AWS IoT Core (N. Virginia)",
      "summary": "[RESOLVED] Increased latency and error rates\n",
      "date": "1606328375",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:49 AM PST</span>&nbsp;We continue to experience increased latency and API failure rates for Connect, Subscribe, Messaging, and Shadow operations in the US-EAST-1 region. This is also impacting AWS IoT Device Management and AWS IoT Device Defender. We continue to work towards resolution.</div><div><span class=\"yellowfg\">11:44 AM PST</span>&nbsp;We are continuing to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. Messaging on existing connections and Shadow operations are now operating normally. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 3:11 PM PST</span>&nbsp;We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. Messaging on existing connections and Shadow operations continue to operate normally. We have identified the root cause and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 4:51 PM PST</span>&nbsp;We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Messaging on existing connections and Shadow operations continue to operate normally.</div><div><span class=\"yellowfg\"> 7:19 PM PST</span>&nbsp;We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. We expect to see recovery once the on-going Kinesis issue is fully resolved. Messaging on existing connections and Shadow operations continue to operate normally.</div><div><span class=\"yellowfg\"> 9:21 PM PST</span>&nbsp;We are seeing recovery for Connect and Subscribe operations in the US-EAST-1 region. We continue to work towards full resolution. Messaging on existing connections and Shadow operations continue to operate normally.</div><div><span class=\"yellowfg\">10:02 PM PST</span>&nbsp;Between 5:15 AM and 9:52 PM PST we experienced increased error rates and latency for Connect, Subscribe, Publish and Shadow operations. The issue has been resolved and the service is operating normally.</div>",
      "service": "awsiot-us-east-1"
    },
    {
      "service_name": "AWS IoT SiteWise (N. Virginia)",
      "summary": "[RESOLVED] Increased Error rates and Latency",
      "date": "1606331100",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:07 AM PST</span>&nbsp;We are continuing to experience elevated error rate on data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work towards resolution.</div><div><span class=\"yellowfg\">12:24 PM PST</span>&nbsp;We are continuing to experience elevated error rates on data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 3:06 PM PST</span>&nbsp;We are beginning to see recovery for data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 4:50 PM PST</span>&nbsp;We are beginning to see recovery for data ingestion. Access to existing data, transforms, and metrics is unaffected. We are continuing to experience increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 7:23 PM PST</span>&nbsp;We have seen recovery for data ingestion. Access to existing data, transforms, and metrics is unaffected. We are continuing to experience increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.</div><div><span class=\"yellowfg\">10:23 PM PST</span>&nbsp;We have seen recovery for data ingestion and the generation of auto-computed aggregates. Access to existing data, transforms, and metrics is unaffected. We are beginning to see recovery in the execution of computations for IoT SiteWise transforms and metrics but are still experiencing computation latency in the US-EAST-1 Region. We continue to work toward full resolution. IoT Events and IoT Analytics have recovered and those services are operating normally.</div><div><span class=\"yellowfg\">Nov 26, 12:27 AM PST</span>&nbsp;We are beginning to see recovery in the execution of computations for IoT SiteWise transforms and metrics but are still experiencing computation latency in the US-EAST-1 Region. All other functions of IoT SiteWise including data ingestion and the generation of auto-computed aggregates have recovered and are operating normally. We continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 26,  3:55 AM PST</span>&nbsp;Between November 25 5:15 AM PST and November 26 3:49 AM PST, we experienced elevated error rates and increased latency on data ingestion, computation of aggregates, transforms, and metrics in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. We are still executing computations for transforms and metrics on data that may have arrived during the impact window. These will appear in customers' accounts as we process them over the next few hours.</div>",
      "service": "iotsitewise-us-east-1"
    },
    {
      "service_name": "AWS Batch (N. Virginia)",
      "summary": "[RESOLVED] Job State Transition Delays",
      "date": "1606331520",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:15 AM PST</span>&nbsp;We are experiencing increased error rates for job state transitions and compute environment scaling in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:41 AM PST</span>&nbsp;We continue to experience increased error rates and delays for job state transitions and compute environment scaling in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:58 PM PST</span>&nbsp;We have identified the root cause of increased error rates and delays for job state transitions and compute environment scaling in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 1:35 PM PST</span>&nbsp;We are no longer seeing compute environment scaling delays but are still experiencing elevated job state transition times in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 3:23 PM PST</span>&nbsp;We are beginning to see recovery for the elevated job state transition times in the US-EAST-1 Region, and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 4:11 PM PST</span>&nbsp;We continue to see some recovery for the elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution.</div><div><span class=\"yellowfg\"> 6:15 PM PST</span>&nbsp;We continue to see recovery for elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution. Customers can expect jobs to work correctly, but may still see issues with Multi-node Parallel workloads.</div><div><span class=\"yellowfg\"> 7:23 PM PST</span>&nbsp;We continue to see recovery for elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution. Customers can expect most jobs to work correctly, but may still see Multi-node Parallel job delays.</div><div><span class=\"yellowfg\"> 9:23 PM PST</span>&nbsp;We have full recovery for AWS Batch jobs not using AWSvpc networking mode. Customers running Multi-Node Parallel jobs may see deprovisioning delays while we wait for ECS recovery.</div><div><span class=\"yellowfg\">10:31 PM PST</span>&nbsp;Between 5:17 AM and 7:20 PM PST we experienced delayed job state transitions and compute environment scaling delays in AWS Batch Jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "batch-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (N. Virginia)",
      "summary": "[RESOLVED] Increased provisioning error rates",
      "date": "1606334820",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:09 PM PST</span>&nbsp;We are experiencing increased error rates in provisioning of new Amazon WorkSpaces in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 2:06 PM PST</span>&nbsp;We are continuing to experience increased error rates in the provisioning of new Amazon WorkSpaces in the US-EAST-1 Region. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:24 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting provisioning of Amazon WorkSpaces in the US-EAST-1 Region. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time.</div><div><span class=\"yellowfg\"> 7:56 PM PST</span>&nbsp;We have identified the root cause of the issue affecting provisioning of Amazon WorkSpaces in the US-EAST-1 Region. We expect to see recovery once the on-going Kinesis issue is fully resolved. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time.</div><div><span class=\"yellowfg\"> 9:22 PM PST</span>&nbsp;We are beginning to see recovery on provisioning of Linux and non-BYOL Windows WorkSpaces in the US-EAST-1 Region. We recommend customers continue to not perform modify, migrate, reboot, rebuild and/or restore operations on all existing WorkSpaces until complete recovery. We continue to work toward full resolution.</div><div><span class=\"yellowfg\">11:31 PM PST</span>&nbsp;We are continuing to see recovery on provisioning of Linux and Windows (including BYOL) WorkSpaces in the US-EAST-1 Region. At this time, we expect customers can modify, migrate, reboot, rebuild and restore their existing WorkSpaces normally. We continue to work towards full resolution.</div><div><span class=\"yellowfg\">Nov 26, 12:01 AM PST</span>&nbsp;Between 5:15 AM PST and 11:55 PM PST, we experienced increased error rates in provisioning of new Amazon WorkSpaces in the US-EAST-1 Region. The issue has now been resolved and the service is operating normally.</div>",
      "service": "workspaces-us-east-1"
    },
    {
      "service_name": "Amazon CloudWatch (N. Virginia)",
      "summary": "[RESOLVED] Increased CloudWatch API error rates",
      "date": "1606412297",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:38 AM PST</span>&nbsp;We are investigating increased error rates with the CloudWatch GetMetricStatistics and GetMetricData APIs when requesting metrics older than 3 hours in the US-EAST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if the period of the alarm is longer than 3 hours. Querying metrics less than 3 hours as well as publishing metrics remains unimpacted.</div><div><span class=\"yellowfg\">10:42 AM PST</span>&nbsp;Between 7:59 AM and 10:05 AM PST, we experienced increased error rates with the CloudWatch GetMetricStatistics and GetMetricData APIs when requesting metrics older than 3 hours in the US-EAST-1 Region. CloudWatch alarms may have transitioned into \"INSUFFICIENT_DATA\" state if the period of the alarm was longer than 3 hours. Querying metrics less than 3 hours as well as publishing metrics were unimpacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudwatch-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sao Paulo)",
      "summary": "[RESOLVED] Instances unavailable in a single availability zone ",
      "date": "1607344379",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:32 AM PST</span>&nbsp;We can confirm connectivity and power issues affecting some instances in a single Availability Zone (sae1-az3) in the SA-EAST-1 Region. We have identified the cause of the issue and are working towards resolution.</div><div><span class=\"yellowfg\"> 5:03 AM PST</span>&nbsp;At 11:10 PM PST on December 6 a small number of underlying hosts experienced power loss, affecting some customers in sae1-az3 in the SA-EAST-1 Region. Impacted customers were automatically notified via the Personal Health Dashboard. We quickly identified root cause and began working toward recovery. At 2:25 AM PST on December 7 additional hosts were impacted. Shortly after that we began to successfully mitigate impact and as of 4:00 AM PST we have begun to see recovery. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 6:18 AM PST</span>&nbsp;Between December 6, 2020 at 11:10 PM PST and December 7, 2020 at 5:45 AM PST we experienced connectivity and power issues affecting some instances in a single Availability Zone (sae1-az3) in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-sa-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Oregon)",
      "summary": "[RESOLVED] API Error Rates and Latencies",
      "date": "1607992032",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:27 PM PST</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 4:40 PM PST</span>&nbsp;We can confirm increased error rates and latencies for the EC2 network-related APIs and elevated launch failures for newly launched instances within the US-WEST-2 Region. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:46 PM PST</span>&nbsp;We are beginning to see signs of recovery with error rates and latencies returning to normal levels for the EC2 APIs in the US-WEST-2 Region. Launches of new instances are once again succeeding, and we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 4:55 PM PST</span>&nbsp;Between 4:05 PM and 4:39 PM PST, we experienced increased error rates and latencies for the EC2 APIs, and elevated launch failures for new EC2 instances in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Seoul)",
      "summary": "[RESOLVED] NLB의 네트워크 연결 관련 | Network connectivity for NLB",
      "date": "1608306650",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:50 AM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제를 조사하고 있습니다. 다수의 가용 영역을 사용하도록 구성된 Network Load Balancer는 이슈의 영향을 받는 가용 영역으로부터 자동으로 장애 조치되어 그 영향을 완화하게 됩니다. 영향을 받은 한개의 가용 영역 만을 사용하도록 구성된 Network Load Balancer에 대해선, 이슈가 완전히 해결 될 때까지 문제를 완화하기 위해 고객이 다른 가용 영역에 대체 Network Load Balancer를 생성 할 수 있습니다. 질문이 있으시거나 운영상의 문제가 발생하는 경우 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 기술지원 부서에 문의하십시오. | We are investigating network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. Network Load Balancers configured for multiple Availability Zones will automatically fail away from the affected Availability Zone, mitigating any impact. For Network Load Balancers configured for only the affected Availability Zone, customers can create a replacement Network Load Balancer in another Availability Zone to mitigate the issue until it is fully resolved. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div><div><span class=\"yellowfg\"> 8:08 AM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제에 대해 복구 징후가 보입니다. 우리는 완전한 복구를 위해 계속 노력하고 있습니다. 질문이 있으시거나 운영상의 문제가 발생하는 경우 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 기술지원 부서에 문의하십시오. | We are seeing signs of recovery for the network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. We continue to work towards full recovery. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div><div><span class=\"yellowfg\"> 8:59 AM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제에 영향을 미치는 문제를 해결했습니다. 태평양 시간 오전12:30 (한국시간 오후5:30) 시부터 AP-NORTHEAST-2 리전 내의 일부 Network Load Balancer에서 네트워크 연결 문제가 발생했습니다. 태평양 시간 7:13시에 (한국시간 오전 12:13)단일 가용 영역 (APNE2-AZ1) 내의 Network Load Balancer에서 TLS 연결에 대한 연결 문제가 발생했습니다. 태평양 시간 오전7:56 시에(한국시간 오전 12:56) AP-NORTHEAST-2리전의 Network Load Balancers와 관련된 모든 연결 문제는 해결 되었으며, 서비스는 정상적으로 운영되고 있습니다. 고객분들은 서비스의 가용률을 복원하기 위한 추가 조치를 취할 필요가 없습니다. 이와 관련하여 질문이나 운영관련 문제를 겪고 있으시다면 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 지원 부서에 문의하십시오. | We have resolved the issue affecting network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. Starting at 12:30 AM PST, some Network Load Balancers within the AP-NORTHEAST-2 Region experienced periods of network connectivity issues. At 7:13 AM, Network Load Balancers within a single Availability Zone (APNE2-AZ1) experienced connectivity issues for TLS connections. At 7:56 AM PST, all connectivity issues for Network Load Balancers within the AP-NORTHEAST-2 Region were resolved. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "elb-ap-northeast-2"
    },
    {
      "service_name": "AWS NAT Gateway (Seoul)",
      "summary": "[RESOLVED] 네트워크 연결 | Network connectivity",
      "date": "1608318576",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:09 AM PST</span>&nbsp;태평양시간 오전12:30 (한국시간 오후 5:30) 에서 오전7:10 (한국시간 오전 12:10) 사이에 AP-NORTHEAST-2 리전 내에서 NAT 게이트웨이에 대한 네트워크 연결 문제가 발생합니다. 문제가 해결되었으며 서비스가 정상적으로 작동하고 있습니다. 고객분들은 서비스의 가용률을 복원하기 위한 추가 조치를 취할 필요가 없습니다. 이와 관련하여 질문이나 운영관련 문제를 겪고 있으시다면 https://console.aws.amazon.com/support AWS 지원 센터를 통해 AWS 지원 부서에 문의하십시오. | Between 12:30 AM and 7:10 AM PST we experience periods of network connectivity issues for NAT Gateway within the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "natgateway-ap-northeast-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Traffic Impacted for NAT Gateways",
      "date": "1608329580",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:13 PM PST</span>&nbsp;We are investigating connectivity issues affecting NAT Gateway in a single Availability Zone (use1-az1) in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:16 PM PST</span>&nbsp;We wanted to provide you with some more details on the periods of network connectivity issues for NAT Gateway within a single Availability Zone (use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.</div><div><span class=\"yellowfg\"> 3:59 PM PST</span>&nbsp;We continue to investigate root cause of the issue affecting network connectivity for NAT Gateway within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.</div><div><span class=\"yellowfg\"> 4:33 PM PST</span>&nbsp;We have not seen any further network connectivity issues for NAT Gateway since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.</div><div><span class=\"yellowfg\"> 5:55 PM PST</span>&nbsp;We have seen no further network connectivity issues for NAT Gateway since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS VPCE PrivateLink (N. Virginia)",
      "summary": "[RESOLVED] Traffic Impacted for VPC Endpoints",
      "date": "1608329794",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:18 PM PST</span>&nbsp;We are investigating connectivity issues affecting VPCE PrivateLink Endpoints in a single Availability Zone (use1-az1) in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:17 PM PST</span>&nbsp;We wanted to provide you with some more details on the periods of network connectivity issues for VPCE Private Link within a single Availability Zone (use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.</div><div><span class=\"yellowfg\"> 3:59 PM PST</span>&nbsp;We continue to investigate root cause of the issue affecting network connectivity for PrivateLink within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.</div><div><span class=\"yellowfg\"> 4:33 PM PST</span>&nbsp;We have not seen any further network connectivity issues for PrivateLink since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.</div><div><span class=\"yellowfg\"> 5:37 PM PST</span>&nbsp;We have seen no further network connectivity issues for PrivateLink since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally.</div>",
      "service": "privatelink-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Traffic impacted for Network Load Balancer ",
      "date": "1608331345",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:42 PM PST</span>&nbsp;We are investigating connectivity issues affecting Network Load Balancers in a single Availability Zone (use1-az1) in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:27 PM PST</span>&nbsp;We wanted to provide you with some more details on the periods of network connectivity issues for Network Load Balancer within a single Availability Zone(use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.</div><div><span class=\"yellowfg\"> 4:02 PM PST</span>&nbsp;We continue to investigate root cause of the issue affecting network connectivity for Network Load Balancer within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.</div><div><span class=\"yellowfg\"> 4:35 PM PST</span>&nbsp;We have not seen any further network connectivity issues for Network Load Balancer since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.</div><div><span class=\"yellowfg\"> 5:34 PM PST</span>&nbsp;We have seen no further network connectivity issues for Network Load Balancer since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Oregon)",
      "summary": "[RESOLVED] Increase Error Rate",
      "date": "1610053270",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;We are investigating increased error rates for the Kinesis in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 1:17 PM PST</span>&nbsp;We are seeing recovery for the increased error rates affecting Kinesis in the US-WEST-2 Region and continue to work towards full resolution.</div><div><span class=\"yellowfg\"> 1:42 PM PST</span>&nbsp;Between 12:40 PM and 1:07 PM PST, we experienced increased error rates for Kinesis in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-us-west-2"
    },
    {
      "service_name": "Amazon EventBridge (Oregon)",
      "summary": "[RESOLVED] Increased API Errors and Event Delivery Latencies",
      "date": "1610053516",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:05 PM PST</span>&nbsp;We are experiencing elevated API errors and event delivery latencies in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 1:16 PM PST</span>&nbsp;We are seeing recovery for the increased error rates affecting EventBridge in the US-WEST-2 Region and continue to work towards full resolution.</div><div><span class=\"yellowfg\"> 1:46 PM PST</span>&nbsp;Between 12:40 PM and 1:07 PM PST, we experienced elevated API errors and event delivery latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "events-us-west-2"
    },
    {
      "service_name": "Amazon CloudWatch (Oregon)",
      "summary": "[RESOLVED] Increased Error Rates and Latencies",
      "date": "1610054463",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:21 PM PST</span>&nbsp;We are investigating increased error rates and latencies for CloudWatch Logs APIs in the US-WEST-2 Region. Log events and metrics may be delayed. Alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics.</div><div><span class=\"yellowfg\"> 1:42 PM PST</span>&nbsp;Between 12:41 PM and 1:07 PM PST, we experienced elevated error rates when calling CloudWatch Logs APIs in the US-WEST-2 Region. Some log events and metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics and log events are in the process of backfilling in CloudWatch console graphs and for API retrieval. The issue is resolved and the service is operating normally.</div>",
      "service": "cloudwatch-us-west-2"
    },
    {
      "service_name": "Amazon Personalize (N. Virginia)",
      "summary": "[RESOLVED] Increased API Faults",
      "date": "1611523748",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:29 PM PST</span>&nbsp;We are currently experiencing increased error rates in the GetRecommendations API in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:46 PM PST</span>&nbsp;Between 12:36 PM and 1:42 PM PST, we experienced increased error rates in the GetRecommendations API in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "personalize-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (N. Virginia)",
      "summary": "[RESOLVED] External Network Connectivity Issues",
      "date": "1611681635",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:20 AM PST</span>&nbsp;We are investigating connectivity issues with an internet provider, mainly affecting the East Coast of the United States, outside of the AWS Network. We are investigating the issue with the external provider.</div><div><span class=\"yellowfg\">10:05 AM PST</span>&nbsp;\n\nWe are starting to see recovery for the connectivity issues from end customers in the East Coast of the United States to AWS services. Connectivity to instances and services within the Region are not impacted by the event. Internet traffic to/from other external providers is also not impacted. All AWS services are operating normally at this time.\n</div><div><span class=\"yellowfg\">10:14 AM PST</span>&nbsp;Between 8:26 AM and 9:46 AM PST, some customers experienced connectivity issues from the East Coast of the United States to AWS services. Connectivity to instances and services within the Region were not impacted by the event. Internet traffic to/from other external providers was also not impacted. All AWS services continue to operate normally. </div>",
      "service": "internetconnectivity-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] 인스턴스 기동실패 | Elevated Launch Failures",
      "date": "1611812973",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:49 PM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 발생하고 있는 EC2 인스턴스의 기동 실패와 VPC 업데이트의 네트워크 전파 시간 지연에 대해 조사하고 있습니다. | We are investigating elevated launch failures for EC2 instances and delayed network propagation times for VPC update in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region.</div><div><span class=\"yellowfg\"> 9:55 PM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 새로운 EC2 인스턴스 시작 및 VPC에서의 네트워크 전파 시간에 영향을 미치는 문제에 대한 복구 징후가 보입니다. AWS는 문제 해결을 위해 계속 노력하고 있습니다. | We are seeing signs of recovery for the issue affecting new EC2 instance launches and network propagation times for VPC in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region. We continue to work on resolving the issues.</div><div><span class=\"yellowfg\">10:37 PM PST</span>&nbsp;태평양 표준시 (PST) 기준으로 오후 8시 13 분에서 오후 8시 33 분 사이에 AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 EC2 인스턴스에 대한 기동 실패가 발생했습니다. 인스턴스 기동 실패는 PST기준 오후 8시 33 분에 복구되었지만, 영향을 받은 가용 영역 내에서 새로 시작된 EC2 인스턴스에 대한 VPC 네트워크 구성 전파 시간이 계속 지연되었습니다. PST 기준 오후 9시 46 분에 VPC 네트워크 구성 전파가 정상 수준으로 돌아 왔습니다. 발생했던 문제는 해결되었으며 현재 서비스가 정상적으로 작동하고 있습니다. 이 서비스의 가용성을 복원하기 위해 고객이 추가 조치를 취하실 필요는 없습니다. 혹시라도 질문이 있거나 AWS 서비스 운영에 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support)를 통해 AWS 서포트팀에 문의하시기 바랍니다. | Between 8:13 PM and 8:33 PM PST, we experience elevated launch failures for EC2 instances in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region. Although the launch failures recovered at 8:33 PM PST, we continued to experience delayed propagation times of VPC network configuration for newly launched EC2 instances within the affected Availability Zone. By 9:46 PM PST, VPC network configuration propagation had returned to normal levels. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS CloudFormation (Tokyo)",
      "summary": "[RESOLVED] Increased Error Rates and Latencies",
      "date": "1611869244",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:27 PM PST</span>&nbsp;AP-NORTHEAST-1 リージョンにおける AWS CloudFormation スタックを作成、更新、削除する際のエラーレートおよびレイテンシーの増加について調査中です。| We are investigating increased error rates and latencies when creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 1:48 PM PST</span>&nbsp;AP-NORTHEAST-1 リージョンで AWS CloudFormation スタックを作成、更新、削除するときのエラーレートおよびレイテンシーの増加について確認いたしました。現在解決に向けて対応しております。We can confirm increased error rates and latencies when creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region and are actively working toward resolution.</div><div><span class=\"yellowfg\"> 2:02 PM PST</span>&nbsp;現在本問題の回復を確認しており、現在解決に向けて引き続き対応しております。| We are beginning to see recovery and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 2:10 PM PST</span>&nbsp;日本時間 1/29 午前 4:25 から 6:55 にかけて、AP-NORTHEAST-1 リージョンで AWS CloudFormation スタック作成、更新、削除のエラーレートおよびレイテンシーが増加しました。問題は解決され、サービスは正常に動作しています。 | Between 11:25 AM and 1:55 PM PST we experienced increased error rates and latencies creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] Instance Connectivity",
      "date": "1612182273",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:24 AM PST</span>&nbsp;We are experiencing instance connectivity issues in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region.</div><div><span class=\"yellowfg\"> 5:34 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. Services that use EC2 and EBS including Elastic Load Balancing, EFS, Elasticache, ECS Fargate, Sagemaker and ELB, are also impacted. </div><div><span class=\"yellowfg\"> 6:08 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are experiencing elevated error rates for new instance launches in the impacted Availability Zone. Services that use EC2 and EBS including Connect, EFS, Elasticache, ECS Fargate, Managed Streaming for Apache Kafka and Sagemaker are also impacted.</div><div><span class=\"yellowfg\"> 6:50 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are starting to see recovery for new instance launches in the impacted Availability Zone. Services that use EC2 and EBS including EFS, Elasticache, ECS Fargate, Managed Streaming for Kafka, EMR and Sagemaker are also impacted. Between 3:02 AM and 5:20 AM PST, Amazon Connect customers experienced degraded call and Chat connectivity and handling for agents in the EU-WEST-2 Region. Amazon Connect issues have been resolved and the service is operating normally.</div><div><span class=\"yellowfg\"> 7:41 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. The elevated error rates that impacted some new instance launches in the affected Availability Zone have fully recovered.</div><div><span class=\"yellowfg\"> 8:29 AM PST</span>&nbsp;We have resolved the connectivity issues for the vast majority of affected instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are also seeing recovery for the vast majority of EBS volumes experiencing degraded performance due to this event. Services that use EC2 and EBS including RDS, EFS, Elasticache, ECS Fargate, Managed Streaming for Apache Kafka, EMR and Sagemaker are also seeing recovery. We continue to work towards full recovery for the remaining instances and volumes.</div><div><span class=\"yellowfg\"> 8:53 AM PST</span>&nbsp;Starting at 2:57 AM PST we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone (euw2-az3). By 3:59 AM PST, power and networking connectivity had been restored to the majority of affected instances and, by 7:32 AM PST, degraded performance for the majority of affected EBS volumes had been resolved. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the event. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "AWS IoT Core (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latency",
      "date": "1612294997",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:43 AM PST</span>&nbsp;We are investigating increased error rates and latency for the Identity, Registry, and Rules Engine APIs in the US-EAST-1 Region. Device connectivity, messaging and rules evaluations are unaffected.</div><div><span class=\"yellowfg\">11:59 AM PST</span>&nbsp;Between 10:34 AM and 11:46 AM PST we experienced increased error rates and latency for the Identity, Registry and Rules Engine APIs in the US-EAST-1 Region. Device connectivity, messaging and rules evaluations remained unaffected during the event. The issue has been resolved and the service is operating normally.</div>",
      "service": "awsiot-us-east-1"
    },
    {
      "service_name": "AWS CloudFormation (N. Virginia)",
      "summary": "[RESOLVED] Increased Stack Create, Delete and Update Times",
      "date": "1612437136",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:12 AM PST</span>&nbsp;We are investigating increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-EAST-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 3:56 AM PST</span>&nbsp;We can confirm increased latencies for creating and deleting stacks in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 4:16 AM PST</span>&nbsp;Between 2:03 AM and 4:03 AM PST we experienced increased latencies for stack creation and deletion in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Connectivity Issues for Network Load Balancers",
      "date": "1612988354",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:19 PM PST</span>&nbsp;We are investigating increased connectivity issues for Network Load Balancers within the US-EAST-1 Region. </div><div><span class=\"yellowfg\">12:45 PM PST</span>&nbsp;We continue to work to identify the root cause of the event resulting in connectivity issues for some Network Load Balancers within the US-EAST-1 Region. Some Network Load Balancers in USE1-AZ2, USE1-AZ4, and USE1-AZ5 are experiencing connection issues for TLS (SSL) requests. Non-TLS (SSL) requests are not affected by this issue. We are also experiencing some delays in the provisioning of new Network Load Balancers and registration of new targets behind existing Network Load Balancers. We have identified the underlying subsystems responsible for this issue and continue to work towards root cause and resolution.</div><div><span class=\"yellowfg\"> 1:12 PM PST</span>&nbsp;We have identified root cause and are now working to test a fix for the issue affecting connectivity for Networking Load Balancers in the US-EAST-1 Region. Once we have confirmed that it resolves the issue, we will update all affected Network Load Balancers. The issue continues to affect TLS (SSL) connections for Network Load Balancers within the USE1-AZ2, USE1-AZ4, and USE1-AZ5 Availability Zones. The issue does not affect other Availability Zones in the Region and is not expected to affect any additional Network Load Balancers that are currently operating normally. Affected customers can create new Network Load Balancers (with or without TLS) in Availability Zones that are not affected by this issue for immediate mitigation.</div><div><span class=\"yellowfg\"> 1:37 PM PST</span>&nbsp;We have confirmed that the fix to resolve the issue is working as expected and are currently deploying it to the affected Network Load Balancers. We will be deploying one Availability Zone at a time, in the following order: USE1-AZ4, USE1-AZ2, and USE1-AZ5. We expect to see recovery as we progress through the affected Availability Zones.</div><div><span class=\"yellowfg\"> 2:03 PM PST</span>&nbsp;We have made steady progress and are close to completing the rollout of the fix in the USE1-AZ4 Availability Zone. Most of the affected Networking Load Balancers in that Availability Zone are now operating normally. We’ll continue at this pace through the remaining Network Load Balancers in USE1-AZ4, followed by the Network Load Balancers in USE1-AZ2 and USE1-AZ5 Availability Zones. Some customers have reported impact for PrivateLink endpoints, which is related to this issue and will see recovery as we continue to deploy the fix.\n</div><div><span class=\"yellowfg\"> 2:43 PM PST</span>&nbsp;We continue to make progress in deploying the fix to Network Load Balancers in the USE1-AZ4 Availability Zone. While the vast majority of the affected Network Load Balancers in that Availability Zone have now recovered, we are making slower progress than expected on the final cell within the Availability Zone. We are working to resolve that issue before moving onto the remaining Availability Zones. We’ll continue to keep you updated on our progress.</div><div><span class=\"yellowfg\"> 3:27 PM PST</span>&nbsp;We have resolved the issue that affected our deployment of the fix to the final cell in the USE1-AZ4 Availability Zone, and continue to deploy to Availability Zones USE1-AZ2 and USE1-AZ5. We have taken steps to accelerate recovery but are also being cautious to not apply the fix to more than one Availability Zone at a time out of an abundance of caution. For customers that have seen their Network Load Balancer recover, we do not expect any further impact. For customers that are still waiting for recovery, we expect to see that in the coming hour or two as we work through the remaining Availability Zone. For immediate mitigation, customers can create a new Network Load Balancer in the unaffected Availability Zones and register the existing back-end targets with it. We’ll continue to provide updates as we progress through the remaining Availability Zones.</div><div><span class=\"yellowfg\"> 4:03 PM PST</span>&nbsp;We continue to make progress and are now seeing recovery for affected Network Load Balancers in the USE1-AZ2 Availability Zone. We will continue on the remainder of that Availability Zone before completing USE1-AZ5, the final Availability Zone. Again, for Network Load Balancers that have recovered, we do not expect further impact.</div><div><span class=\"yellowfg\"> 4:28 PM PST</span>&nbsp;We have completed applying the fix in both the USE1-AZ4 and USE1-AZ2 Availability Zones. We have started the process of updating the final Availability Zone, USE1-AZ5. We expect to see full recovery within the next 30 minutes. </div><div><span class=\"yellowfg\"> 4:38 PM PST</span>&nbsp;We have completed the update in all affected Availability Zones and all Network Load Balancers are now operating normally. We are working through a backlog of Network Load Balancer created and back-end instance registrations, but at this stage all operations should be operating normally. We will continue to monitor recovery and post an update shortly. </div><div><span class=\"yellowfg\"> 5:07 PM PST</span>&nbsp;Starting at 11:24 AM PST, some load balancers in the USE1-AZ2, USE1-AZ4, and USE1-AZ5 Availability Zones experienced connectivity issues when terminating TLS (SSL) connections in the US-EAST-1 Region. Engineers worked to identify the root cause of the event, ultimately deploying an update to recover USE1-AZ4 at 1:50 PM PST, USE1-AZ2 at 4:26 PM PST and USE1-AZ5 at 4:31 PM PST. Network Load Balancers not terminating TLS (SSL) connections were not affected by this event. Some PrivateLink endpoints were also affected by the issue. The issue has been resolved and the service is operating normally. </div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Virtual Private Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased Console Error Rates",
      "date": "1613516078",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:54 PM PST</span>&nbsp;We are investigating increased error rates displaying Virtual Private Cloud (VPC) in the VPC Management Console in the US-EAST-1 Region. This issue only affects the new VPC Management Console experience, so switching back to the previous VPC Management Console experience (top left toggle switch) will resolve the issue. The VPC Command Line Tools and APIs are not affected by this issue. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 3:10 PM PST</span>&nbsp;We have resolved the issue causing increased error rates displaying Virtual Private Cloud (VPC) in the VPC Management Console in the US-EAST-1 Region. This issue only affected the new experience of the VPC Management Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "vpc-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Tokyo)",
      "summary": "[RESOLVED] インスタンスの障害について | Instance impairments",
      "date": "1613747375",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:09 AM PST</span>&nbsp;(12:09AM JST)現在、東京リージョン AP-NORTHEAST-1 のひとつのアベイラビリティゾーン apne1-az1 において、インスタンスに影響を及ぼす接続性の問題が発生しており、対応を行っております。 |  We are investigating connectivity issues affecting instances in a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 7:58 AM PST</span>&nbsp;(12:58AM JST)現在、東京リージョン AP-NORTHEAST-1 における一つのアベイラビリティゾーン（apne1-az1）の一部で、周囲の温度が上昇している状況を確認いたしました。影響を受けているアベイラビリティーゾーンの一部 EC2 インスタンスでは、接続性の問題または温度上昇の影響に伴い、電源が切れている問題が発生しております。当該問題の影響により、一部 EBS ボリュームにてパフォーマンスが低下しております。本問題の根本原因を特定し、現在解決に向けて対応しております。東京リージョン AP-NORTHEAST-1 におけるその他アベイラビリティゾーンは、この問題の影響を受けておりません。 |  We can confirm that a small area of a single Availability Zone (apne1-az1) is experiencing an increase in ambient temperature in the AP-NORTHEAST-1 Region. Some EC2 instances within the affected section of the Availability Zone have experienced connectivity issues or have powered down as a result of the increasing temperatures. Some EBS volumes are also experiencing degraded performance as a result of the event. We have identified the root cause of the issue and are working towards resolution. Other Availability Zones within the AP-NORTHEAST-1 Region are not affected by this event.</div><div><span class=\"yellowfg\"> 8:40 AM PST</span>&nbsp;(1:40AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画での温度上昇に対処するために引き続き取り組んでいます。温度の上昇は、当該セクション内の冷却システムへの電力の損失によって発生しました。引き続き、電源の回復に取り組んでおりこれまでに冷却システムの 1つを正常に復旧させました。引き続き温度を通常レベルに復元し、影響を受けた EC2 インスタンスと EBS ボリュームの回復に取り組んでまいります。EC2 および EBS API を含むその他のシステムは、影響を受けたアベイラビリティーゾーン内で正常に動作しています。影響のあった EC2 インスタンスおよび EBS ボリュームをお持ちのお客様は、影響を受けたアベイラビリティーゾーン、または AP-NORTHEAST-1 リージョン内のその別のアベイラビリティーゾーンで再起動を試みることができます。 | We continue to work on addressing the increase in ambient temperature affecting a small section of a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 region. The increase in temperature is caused by a loss of power to the cooling systems within the affected section of the Availability Zone. We are working to restore power and have successfully brought online one of the cooling systems. We continue to work on restoring temperatures to normal levels and then recovering affecting EC2 instances and EBS volumes. Other systems, including EC2 and EBS APIs, are operating normally within the affected Availability Zone. Customers with affected EC2 instances and EBS volumes can attempt to relaunch in the affected Availability Zone, or another Availability Zone within the AP-NORTHEAST-1 Region. </div><div><span class=\"yellowfg\"> 9:43 AM PST</span>&nbsp;(2:43AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画での温度上昇に対処するために引き続き取り組んでいます。温度の上昇は当該セクション内の冷却装置への電力損失によって発生しました。当該セクション内のいくつかの冷却ユニットの電力はすでに復元しており、温度が低下し始めていることを確認しております。残りのオフラインの冷却ユニットは引き続き作業を続け、温度を通常レベルに戻します。温度が回復次第、影響を受ける EC2 インスタンスと EBS ボリュームが回復します。EC2 および EBS API を含むその他のシステムは、影響を受けるアベイラビリティーゾーン内で正常に動作しています。影響を受けた EC2 インスタンスおよび EBS ボリュームをお持ちのお客様は、影響を受けたアベイラビリティーゾーン、または AP-NORTHEAST-1 リージョン内の別のアベイラビリティーゾーンでインスタンスの再作成を試みることができます。| We continue to work on addressing the increase in ambient temperature affecting a small section of a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 region. The increase in temperature is caused by a loss of power to the cooling units within the affected section of the Availability Zone. We have now restored power to a number of the cooling units within this section of the Availability Zone and are starting to see temperatures decreasing. We will continue to work through the remaining cooling units that are still offline, which will return temperatures to normal levels. Once temperatures have recovered, we would expect to see affected EC2 instances and EBS volumes begin to recover. Other systems, including EC2 and EBS APIs, are operating normally within the affected Availability Zone. Customers with affected EC2 instances and EBS volumes can attempt to relaunch in the affected Availability Zone, or another Availability Zone within the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\">10:42 AM PST</span>&nbsp;(3:42AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画で影響を受けていた冷却ユニットの多くの電源が回復しました。室温は通常のレベルに近い状況まで戻り、ネットワーク、EC2 および EBS ボリュームの回復処理を開始しています。ネットワークはすでに回復し、EC2とEBSボリューム の回復処理に着手しております。回復処理が始まると再起動が発生するため、お客様にはお使いのインスタンスでアクションをとっていただく場合がございます。EBSボリュームに関しましては、ボリュームが回復するにつれ、degraded I/Oパフォーマンスが通常に戻ります。 ”stopping” もしくは ”shutting-down” のまま止まってしまっているインスタンスに関しましては、回復処理が進むにつれ、 ”stopped” もしくは “terminated” に戻ります。| We have now restored power to the majority of the cooling units within the affected section of the Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region. Temperatures are now close to normal levels and we have begun the process of restoring networking, EC2 instances and EBS volumes. The network has been restored within the affected section of the Availability Zone and we are now working on EC2 instances and EBS volumes. As they begin to recover, customers may need to take action on their instance as it will have experienced a reboot. For EBS volumes, degraded I/O performance will return to normal levels as volumes recover. For instances that are stuck “stopping” or “shutting-down”, these will return to the “stopped” or “terminated” state as recovery proceeds.\n</div><div><span class=\"yellowfg\">11:26 AM PST</span>&nbsp;(4:26AM JST) AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) で影響を受けていた冷却サブシステムの電源が回復しました。現在、室温は通常レベルで運用されています。大部分の ES2 インスタンスと EBS ボリュームが復旧しておりますが、残りのインスタンスとボリュームの復旧作業に引き続き取り組んでいます。| We have now restored power to the cooling subsystem within the affected section of the Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region. Temperatures are now operating at normal levels. We are also seeing recovery for the majority of EC2 instances and EBS volumes and continue to work on the remaining instance and volumes.</div><div><span class=\"yellowfg\">12:09 PM PST</span>&nbsp;(5:09AM JST)アベイラビリティゾーン (apne1-az1) で影響を受けた一部の区画の室温は安定し、通常のレベルに戻りました。多くの EC2インスタンスは回復済みとなっております。多くの EBSボリュームも回復済みですが、残りの少数のボリュームの復旧作業に引き続き取り組んでおります。| Temperatures within the affected section of the Availability Zone (apne1-az1) remain stable and at normal levels. We have now recovered the vast majority of EC2 instances. The majority of EBS volumes have also recovered but there are a few that have required some engineering intervention that we are working on.</div><div><span class=\"yellowfg\">12:54 PM PST</span>&nbsp;(5:54AM JST)日本時間 02/19 11:01 PM から、AP-NORTHEAST-1 リージョンのうちの１つのアベイラビリティーゾーンの一部の区画で室温の上昇を確認いたしました。日本時間 02/19 11:03 PM から、室温が上昇した結果として、一部の EC2インスタンスが影響を受け、一部のEBSボリュームではパフォーマンスが低下しました。根本的な原因は、影響を受けたアベイラビリティーゾーンのセクション内の冷却システムへの電力の喪失であり、すでに回復済みです。日本時間 02/20 03:30 AM までに、電力は冷却システム内のほとんどのユニットで復旧し、室温は通常のレベルに戻りました。日本時間 02/20 04:00 AM までに、EC2 インスタンスと EBS ボリュームの回復が始まり、日本時間 02/20 05:30 AM 時点で、影響を受けた EC2 インスタンスと EBS ボリュームの大部分は通常通り動作しております。一部のインスタンスとボリュームは、イベントによって影響を受けたハードウェア上でホストされていました。引き続き影響を受けたすべてのインスタンスとボリュームの復旧に取り組み、Personal Health Dashboard を通じて、現在も影響を受けているお客様に対し通知を行います。即時の復旧が必要な場合は、影響を受けているインスタンスまたはボリュームを置き換えていただくことをお勧めします。| Starting at 6:01 AM PST, we experienced an increase in ambient temperatures within a section of a single Availability Zone within the AP-NORTHEAST-1 Region. Starting at 6:03 AM PST, some EC2 instances were impaired and some EBS volumes experienced degraded performance as a result of the increase in temperature. The root cause was a loss of power to the cooling system within a section of the affected Availability Zone, which engineers worked to restore. By 10:30 AM PST, power had been restored to the majority of the units within the cooling system and temperatures were returning to normal levels. By 11:00 AM PST, EC2 instances and EBS volumes had begun to recover and by 12:30 PM PST, the vast majority of affected EC2 instances and EBS volumes were operating normally. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the event. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes, if possible.</div>",
      "service": "ec2-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Tokyo)",
      "summary": "[RESOLVED] API Error Rate",
      "date": "1613755243",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:20 AM PST</span>&nbsp;(2:20AM JST)現在、AP-NORTHEAST-1 リージョンでの、ELB API エラー率の上昇について調査を進めております。既存のロードバランサーへの接続には影響はありません。 |  We are investigating increased error rates for ELB APIs in the AP-NORTHEAST-1 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\"> 9:27 AM PST</span>&nbsp;(2:27AM JST)日本時間 2/20 AM 2:00 から AM 2:18 にかけて AP-NORTHEAST-1 リージョンにおいて API エラーレートの増加を確認しました。すでに問題は復旧し、通常通り動作しております。 |  Between 9:00 AM and 9:18 AM PST we experienced increased API error rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-ap-northeast-1"
    },
    {
      "service_name": "Amazon OpenSearch Service (N. Virginia)",
      "summary": "[RESOLVED]  Increased Domain Operation Error Rates",
      "date": "1614293418",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:50 PM PST</span>&nbsp;Beginning at 4:06 AM PST we began experiencing increased error rates impacting Create and Modify operations for Elasticsearch Domains in the US-EAST-1 Region. While many customers have been notified in their Personal Health Dashboard, we wanted to share more information about this event as the impact is ongoing. We have identified the component responsible for these errors are actively working toward identifying the root cause and mitigating the issue.</div><div><span class=\"yellowfg\"> 3:08 PM PST</span>&nbsp;We have identified the root cause of the increased error rates impacting Create and Modify operations for Elasticsearch Domains in the US-EAST-1 Region. We have begun mitigating the issue and are working towards recovery.</div><div><span class=\"yellowfg\"> 3:52 PM PST</span>&nbsp;We are continuing to make progress in mitigating the issue. The rate of elevated latencies and errors for affected customers will begin declining, however we expect it may take several hours for the issue to be resolved completely. </div><div><span class=\"yellowfg\"> 4:22 PM PST</span>&nbsp;We continue to make progress in mitigating the issue and are more than halfway complete. The rate of elevated latencies and errors for affected customers is declining. However, we expect it may take 2-3 hours for the issue to be resolved completely. </div><div><span class=\"yellowfg\"> 5:37 PM PST</span>&nbsp;From 4:06 AM to 5:15 PM PST, Create and Modify operations were experiencing increased error rates and latencies. The issue has been resolved and the service is now operating normally. We've contacted customers directly on their Personal Health Dashboard who have domains created during the impact period and are still experiencing issues.</div>",
      "service": "elasticsearch-us-east-1"
    },
    {
      "service_name": "AWS Transfer for SFTP (N. Virginia)",
      "summary": "[RESOLVED] Increased Login Failures",
      "date": "1614879450",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:37 AM PST</span>&nbsp;Between 3:34 AM and 6:39 AM PST we experienced increased login failures for Transfer Family in the US-EAST-1 Region. While many customers have been notified in their Personal Health Dashboard, we wanted to share more information about this event. We have identified the component responsible for these errors, and have mitigated the issue. No customer actions are required. The issue has been resolved and the service is operating normally.</div>",
      "service": "transfer-us-east-1"
    },
    {
      "service_name": "AWS CloudFormation (N. Virginia)",
      "summary": "[RESOLVED] Increased Console Error Rates",
      "date": "1614879598",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:39 AM PST</span>&nbsp;We are investigating increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:51 AM PST</span>&nbsp;We can confirm increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs are unaffected by this issue and can still be accessed via the CLI or SDK.</div><div><span class=\"yellowfg\">10:18 AM PST</span>&nbsp;We are beginning to observe recovery for the increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs continue to be unaffected by this issue.</div><div><span class=\"yellowfg\">10:40 AM PST</span>&nbsp;Between 9:14 AM and 9:52 AM PST we experienced increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs were unaffected by this issue and were still able to be accessed via the CLI or SDK during this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Sydney)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1615136782",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:06 AM PST</span>&nbsp;We are investigating an issue with an external provider outside of our network, which may be impacting Internet connectivity between some customer networks and the AP-SOUTHEAST-2 Region. Connectivity to instances and services within the Region is not impacted by the event.</div><div><span class=\"yellowfg\"> 9:39 AM PST</span>&nbsp;We are continuing to experience connectivity issues with an internet provider external to our network. This affects international connectivity from AP-SOUTHEAST-2 Region to internet destinations outside Australia. Connectivity within the AP-SOUTHEAST-2 or to internet destinations within Australia are not impacted at this time. We are continuing to work towards shifting traffic from the external provider to recover from the event.</div><div><span class=\"yellowfg\">10:24 AM PST</span>&nbsp;We are continuing to experience connectivity issues with an internet provider external to our network. This affects internet traffic both to and from the AP-SOUTHEAST-2 Region to international internet destinations outside Australia. Connectivity within the AP-SOUTHEAST-2 Region or to internet destinations within Australia are not impacted at this time. We are continuing to work towards shifting traffic from the external provider to recover from the event.</div><div><span class=\"yellowfg\">11:51 AM PST</span>&nbsp;Between 8:19 AM and 10:59 AM PST we experienced internet connectivity issue with an external provider outside of our network. This impacted internet traffic both to and from the AP-SOUTHEAST-2 Region to internet destinations outside Australia. Connectivity from locations within Australia to the AP-SOUTHEAST-2 Region or to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored.\n</div>",
      "service": "internetconnectivity-ap-southeast-2"
    },
    {
      "service_name": "Amazon Cognito (Ohio)",
      "summary": "[RESOLVED] Increased Errors",
      "date": "1615329039",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:30 PM PST</span>&nbsp;We are investigating increased Cognito User Pool API errors in the US-EAST-2 Region.</div><div><span class=\"yellowfg\"> 2:54 PM PST</span>&nbsp;We can confirm increased Cognito User Pool API errors in the US-EAST-2 Region. We have confirmed the root cause to be a subsystem deployment which we are now in the process of rolling back. We are observing steady signs of recovery as the rollback has progressed.</div><div><span class=\"yellowfg\"> 3:04 PM PST</span>&nbsp;Between 1:56 and 2:50 PM PST, we experienced increased Cognito User Pool API errors in the US-EAST-2 Region. We have confirmed the root cause to be a subsystem deployment and the rollback of that deployment has completed, resulting in recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "cognito-us-east-2"
    },
    {
      "service_name": "AWS Lambda (N. Virginia)",
      "summary": "[RESOLVED] Increased Invoke Errors",
      "date": "1615925287",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:08 PM PDT</span>&nbsp;We are investigating increased invoke times and errors in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:21 PM PDT</span>&nbsp;Between 12:10 PM and 12:58 PM PDT, we experienced increased invoke error rates and invoke times in the US-EAST-1 Region. This issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased DNS Edit Propagation Time ",
      "date": "1616455921",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:32 PM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to Route 53 DNS servers. Queries to existing DNS records are not affected by this issue.</div><div><span class=\"yellowfg\"> 4:50 PM PDT</span>&nbsp;We have identified the issue that has caused increased propagation times of DNS edits, and are working towards recovery. Queries to existing DNS records are not affected by this issue and are being answered normally.</div><div><span class=\"yellowfg\"> 5:05 PM PDT</span>&nbsp;Between 4:08 PM and 4:58 PM PDT we experienced increased propagation times of DNS edits. Queries to existing DNS records were not affected by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "AWS WAF",
      "summary": "[RESOLVED] AWS WAF - Increase in blocked requests and delayed rule change propagation in AP-NORTHEAST-1",
      "date": "1617335548",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:52 PM PDT</span>&nbsp;Some customers who use AWS WAF with Application Load Balancer (ALB) or API Gateway in the AP-NORTHEAST-1 Region are experiencing slow propagation of AWS WAF rule changes and an increase in blocked web requests. We have identified the root cause and are working to fix the issue. Until the issue is resolved, customers using AWS WAF with Application Load Balancer can optionally configure ALB to forward requests to targets by enabling the WAF fail open attribute (https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#load-balancer-waf), and customers using AWS WAF with API Gateway can temporarily disable affected Web ACLs.</div><div><span class=\"yellowfg\"> 9:13 PM PDT</span>&nbsp;Between 7:10 PM and 8:25 PM PDT, some customers who use AWS WAF with Application Load Balancer (ALB) or API Gateway in the AP-NORTHEAST-1 Region experienced slow propagation of AWS WAF rule changes and an increase in blocked web requests. The issue has been resolved and the service is now operating normally.</div>",
      "service": "awswaf"
    },
    {
      "service_name": "Amazon Route 53 Resolver (N. Virginia)",
      "summary": "[RESOLVED] Elevated DNS query failures",
      "date": "1617345325",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:35 PM PDT</span>&nbsp;We are investigating elevated failures when querying Route 53 Resolver in a single Availability Zone (use1-az4) of the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:58 PM PDT</span>&nbsp;We are seeing recovery, but are still working to confirm root cause at this time.</div><div><span class=\"yellowfg\">Apr 2, 12:13 AM PDT</span>&nbsp;Between 9:39 PM and 11:36 PM PDT, customers experienced elevated DNS failures when querying Route 53 Resolver for certain hostnames in a single Availability Zone (use1-az4) in the US-EAST-1 Region. The issue has been resolved and all queries are now being answered normally.</div>",
      "service": "route53resolver-us-east-1"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Amazon Connect login performance",
      "date": "1617678631",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:10 PM PDT</span>&nbsp;We are investigating reports of increased error rates in the US-EAST-1 region for users to log onto Amazon Connect.</div><div><span class=\"yellowfg\"> 9:21 PM PDT</span>&nbsp;We are continuing to investigate reports of increased error rates in the us-east-1 region for users to log onto Amazon Connect</div><div><span class=\"yellowfg\"> 9:33 PM PDT</span>&nbsp;Between 6:57 PM and 9:24 PM PDT Amazon Connect users experienced increased login error rates in the us-east-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon WorkMail (N. Virginia)",
      "summary": "[RESOLVED] Degraded WorkMail WebMail performance",
      "date": "1617678855",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:14 PM PDT</span>&nbsp;We are investigating a degraded customer experience with the WorkMail Web client in the US-EAST-1 Region impacting logins.</div><div><span class=\"yellowfg\"> 9:12 PM PDT</span>&nbsp;We are continuing to investigate a degraded customer experience with the WorkMail Web client in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 9:33 PM PDT</span>&nbsp;Between 6:57 PM and 9:25 PM PDT we experienced a degraded customer experience logging in to the WorkMail Web client in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workmail-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Degraded instance registration",
      "date": "1617683231",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:27 PM PDT</span>&nbsp;Between 7:05 PM and 8:50 PM PDT, we experienced increased load balancer provisioning and back-end target registration times in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Lambda (Oregon)",
      "summary": "[RESOLVED] Lambda Management Console Errors",
      "date": "1617754061",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:07 PM PDT</span>&nbsp;We are investigating increased error rates for the Lambda Management Console in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 5:46 PM PDT</span>&nbsp;Between 3:04 PM and 5:18 PM PDT, we experienced increased error rates for the Lambda Management Console in the US-WEST-2 Region. The event was limited to customers accessing the AWS Lambda console. The AWS Lambda data plane and API were not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-west-2"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased AWS Billing Console errors",
      "date": "1618316888",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;We are investigating increased error rates for customers in India when performing Credit Card and Net Banking payments in the AWS Billing Console.</div><div><span class=\"yellowfg\"> 6:08 AM PDT</span>&nbsp;We continue to investigate increased error rates for customers in India when performing Credit Card and Net Banking payments in the AWS Billing Console.</div><div><span class=\"yellowfg\"> 8:17 AM PDT</span>&nbsp;Between April 12 10:25 AM PDT and April 13 6:55 AM PDT, we experienced increased error rates for customers in India when performing Credit Card and Net Banking payments in the AWS Billing Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1618339468",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:17 PM PDT</span>&nbsp;We are working to resolve the issue resulting in increased error rates for the following EC2 APIs in the US-EAST-1 Region: RunInstances, *SecurityGroups, *NetworkInterfaces, *RouteTables, *AccountAttributes, and *NetworkAcls. These APIs will affect the ability to launch new EC2 instances and make mutating changes to Virtual Private Cloud (VPC) network configuration(s). Existing instances and networks continue to work normally. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\">12:56 PM PDT</span>&nbsp;We continue to work toward recovery for the issue resulting in increased API error rates for the EC2 APIs in the US-EAST-1 Region. We have identified the root cause and applied mitigations to reduce the impact, while we continue to work towards full mitigation. Some APIs may experience errors or “request limit exceeded” when calling an affected API or using the EC2 Management Console. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact via the Personal Health and/or Service Health Dashboards. We will provide an update in 30 minutes.</div><div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;We continue to work towards full resolution for the issue resulting in increased error rates for the EC2 APIs in the US-EAST-1 Region. We have applied some request throttling for the affected APIs, which has reduced error rates, allowing several APIs to see early recovery. We are adjusting these throttling for some of the affected APIs, which are causing some additional API errors and elevated errors in the EC2 Management Console. We would expect API error rates to continue to recover with the mitigation steps we have taken as we work towards full recovery.</div><div><span class=\"yellowfg\"> 2:13 PM PDT</span>&nbsp;We continue to work towards full recovery for the issue resulting in increased error rates for the EC2 APIs in the US-EAST-1 Region. We have adjusted request throttles to reduce error rates for the affected APIs. While this has worked for some of the affected APIs, such as RunInstances, some of the affected APIs are now returning “request limit exceeded”. If this does occur, attempt to reduce your request rate for the affected API and retry. With the request throttling, some of the affected services are also beginning to see recovery. We continue to work on resolving the underlying root cause and expect to be fully recovered within the next hour.</div><div><span class=\"yellowfg\"> 3:01 PM PDT</span>&nbsp;We have further adjusted request throttles to reduce error rates for the affected APIs, so “request limit exceeded” errors should now be significantly reduced. We are now in the final stages of resolving the issue with the underlying data store. Once resolved, we will remove all API throttles and expect all API operations to return to normal levels.</div><div><span class=\"yellowfg\"> 3:55 PM PDT</span>&nbsp;We have resolved the issue resulting in the increased error rates for the EC2 APIs, and removed all API request throttling, in the US-EAST-1 Region. Beginning at 11:11 AM PDT, we experienced an increase in API error rates for RunInstances and networking related EC2 APIs. At 12:57 PM PDT, request throttling was applied to several of the affected APIs, which helped to improve error rates for the RunInstances API. We continued to work towards full resolution, while removing request throttles, until 3:30 PM PDT, at which time all affected APIs returned to normal levels of operation. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased Provisioning and Scaling Latencies",
      "date": "1618343445",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:50 PM PDT</span>&nbsp;We can confirm increased provisioning and scaling latencies for Elastic Load Balancers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:38 PM PDT</span>&nbsp;We have determined the root cause of increased scaling latencies for Elastic Load Balancers in the US-EAST-1 Region and are working towards mitigation.</div><div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;The team is closely monitoring the issue with EC2 APIs, and continues to see reductions in scaling and provisioning latencies. Other load balancing operations, including target registrations and traffic processing are unaffected. Once the issue with the EC2 APIs is resolved, we will process the backlog of any pending operations before we observe full resolution.</div><div><span class=\"yellowfg\"> 3:44 PM PDT</span>&nbsp;Beginning at 11:13 AM PDT we experienced increased provisioning and scaling latencies for Elastic Load Balancers in the US-EAST-1 Region. Recovery for scaling latency occurred at 1:15 PM PDT, and recovery for provisioning latency occurred at 3:35 PM PDT. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Batch (Oregon)",
      "summary": "[RESOLVED] Compute Environments going INVALID",
      "date": "1618364745",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:45 PM PDT</span>&nbsp;We are investigating increased transitions to INVALID of some AWS Batch Compute Environments in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 7:12 PM PDT</span>&nbsp;We can confirm increased transitions to INVALID of some AWS Batch Compute Environments in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:00 PM PDT</span>&nbsp;Between 5:25 PM and 7:52 PM PDT, some AWS Batch Compute Environments transitioned to INVALID in the US-WEST-2 Region. The issue has been resolved and the service is working normally.</div>",
      "service": "batch-us-west-2"
    },
    {
      "service_name": "AWS AppSync (N. Virginia)",
      "summary": "[RESOLVED] Increased API Latencies",
      "date": "1619193400",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:56 AM PDT</span>&nbsp;We are investigating increased API latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:36 AM PDT</span>&nbsp;We are starting to see recovery for increased API latencies and timeouts in the US-EAST-1 Region and continue to work towards resolution. </div><div><span class=\"yellowfg\"> 9:51 AM PDT</span>&nbsp;Between 4:41 AM and 9:00 AM PDT, we experienced increased API latencies and timeouts in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "appsync-us-east-1"
    },
    {
      "service_name": "AWS Lambda (N. Virginia)",
      "summary": "[RESOLVED] Increased Latencies and Error Rates",
      "date": "1619482717",
      "status": "0",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:22 PM PDT</span>&nbsp;Between 4:14 PM and 4:22 PM PDT the Lambda invoke API experienced increased latencies and error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Change Propagation Delays",
      "date": "1620324983",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:16 AM PDT</span>&nbsp;We are investigating delays in propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We can confirm that actual invalidations are being propagated as usual, but the invalidation status confirmation through the console and API is delayed. This issue is not impacting propagation times for changes to CloudFront configurations as previously stated. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">12:10 PM PDT</span>&nbsp;We have identified the root cause of delays in reporting status changes of CloudFront invalidations. We continue to work toward resolution. All CloudFront edge locations are consuming configuration changes and invalidations normally. Also, End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\">12:43 PM PDT</span>&nbsp;Between 10:04 AM PDT and 12:20 PM PDT, customers might have experienced delays in reporting status change of CloudFront invalidations. During this time, all CloudFront edge locations were consuming configuration changes and invalidations normally but were not updating status changes in console or via CloudFront APIs. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Sao Paulo)",
      "summary": "[RESOLVED] Network Connectivity Issue",
      "date": "1620331000",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:56 PM PDT</span>&nbsp;We are investigating connectivity issues for some instances in a single Availability Zone (sae1-az2) in the SA-EAST-1 Region. </div><div><span class=\"yellowfg\"> 1:19 PM PDT</span>&nbsp;Starting at 12:20 PM PDT, we experienced low levels of packet loss for Internet Connectivity for some instances in a single Availability Zone (sae1-az2) in the SA-EAST-1 Region. Between 12:48 PM and 12:59 PM PDT, DNS resolution within the affected Availability Zone (sae1-az2) and connectivity between the affected Availability Zone (sae1-az2) and other Availability Zones using public IP addressing also experienced low levels of packet loss. At 1:12 PM PDT, all packet loss issues were resolved. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-sa-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] API Errors, Increased Provisioning Times / Registration Latencies",
      "date": "1620993901",
      "status": "0",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:47 AM PDT</span>&nbsp;We are investigating increased API error rates and increased provisioning/registration latencies for ELBs in the US-EAST-1 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;Starting at 5:05 AM PDT, we experienced periods of increased error rates and provisioning/registration latencies for ELB APIs in the US-EAST-1 Region. The periods of elevated API error rates were resolved at 7:55 AM PDT. The periods of increased provisioning/latencies were resolved for the vast majority of customers by 8:31 AM PDT, with full recovery at 9:11 AM PDT. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon WorkSpaces (N. Virginia)",
      "summary": "[RESOLVED] Increased Errors Connecting to WorkSpaces",
      "date": "1621023810",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:23 PM PDT</span>&nbsp;We are investigating connectivity issues and WorkSpaces automatically rebooting in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:05 PM PDT</span>&nbsp;We can confirm an issue causing a loss of connectivity and reboots for a subset of Windows WorkSpaces in the US-EAST-1 Region. We have identified the root cause and are rolling out a mitigation.</div><div><span class=\"yellowfg\"> 3:26 PM PDT</span>&nbsp;We are currently deploying a mitigation for the issue that is causing loss of connectivity and reboots for a subset of Windows WorkSpaces in the US-EAST-1 Region. Workspaces will recover as this mitigation is deployed.</div><div><span class=\"yellowfg\"> 5:01 PM PDT</span>&nbsp;We continue to see recovery for some Windows WorkSpaces as our mitigation is deployed in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 6:10 PM PDT</span>&nbsp;Recently, we experienced an issue that caused loss of connectivity and reboots for a subset of Windows WorkSpaces in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "workspaces-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1621884419",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:26 PM PDT</span>&nbsp;We are experiencing increased API error rates for the EC2 APIs in the US-WEST-2 Region. We have identified root cause and are actively working to mitigate impact. Some APIs may experience errors or “request limit exceeded” when calling an affected API or using the EC2 Management Console. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact via the Personal Health and/or Service Health Dashboards. We'll continue to update this post as we have more information to share.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;Between 11:02 AM and 12:40 PM PDT, we experienced increased API error rates in the US-WEST-2 Region. During this time, customers may have experienced 500 Errors or \"Request Limit Exceeded\" when calling an affected API, or using the EC2 Management Console. In many cases, retries of failed API requests were successful. Other AWS services that utilize the affected APIs for their own workflows also experienced impact. Those AWS Services have communicated with affected customers via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-west-2"
    },
    {
      "service_name": "AWS Sign-In (N. Virginia)",
      "summary": "[RESOLVED] Increased Latencies for SAML Sign-In",
      "date": "1623264111",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We are investigating periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. </div><div><span class=\"yellowfg\">12:31 PM PDT</span>&nbsp;We can confirm periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\">12:58 PM PDT</span>&nbsp;We are seeing signs of recovery for the timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;Between 7:46 AM and 11:37 AM PDT, we experienced periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to investigate likely root causes and continue to drive to full resolution. The service is operating normally.</div>",
      "service": "signin-us-east-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates and Latencies for SAML Based Federation",
      "date": "1623266658",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:24 PM PDT</span>&nbsp;We can confirm periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We have identified the root cause and continue to work toward resolution.</div><div><span class=\"yellowfg\">12:58 PM PDT</span>&nbsp;We are seeing signs of recovery for the timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:21 PM PDT</span>&nbsp;Between 7:46 AM and 11:37 AM PDT, we experienced periods of request timeouts and increased latencies when attempting to sign-in to the AWS Management Console using SAML based federation. We continue to investigate likely root causes and continue to drive to full resolution. The service is operating normally.</div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Frankfurt)",
      "summary": "[RESOLVED] Connectivity Issues &amp; API Errors",
      "date": "1623356647",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:24 PM PDT</span>&nbsp;We are investigating connectivity issues for some EC2 instances in a single Availability Zone (euc1-az1) in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 1:55 PM PDT</span>&nbsp;We can confirm increased API error rates and latencies for the EC2 APIs and connectivity issues for instances within a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region, caused by an increase in ambient temperature within a subsection of the affected Availability Zone. Other Availability Zones within the EU-CENTRAL-1 Region are not affected by the issue and we continue to work towards resolving the issue.</div><div><span class=\"yellowfg\"> 2:36 PM PDT</span>&nbsp;We continue to work on resolving the connectivity issues affecting EC2 instances in a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region. Ambient temperatures within the affected subsection of the Availability Zone have begun to return to normal levels and we are working to recover the affected EC2 instances and networking devices within the affected Availability Zone. For the vast majority of affected EC2 instances, once network connectivity is restored, the instances will recover. A small number of EC2 instances may have power cycled as a result of the increased temperatures. While we continue to make progress in resolving the issue, we continue to recommend failing away to other Availability Zones in the region if you are able to do so.</div><div><span class=\"yellowfg\"> 3:26 PM PDT</span>&nbsp;We continue to work on resolving the connectivity issues affecting EC2 instances in a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region. While temperatures continue to return to normal levels, engineers are still not able to enter the affected part of the Availability Zone. We believe that the environment will be safe for re-entry within the next 30 minutes, but are working on recovery remotely at this stage. Once we have access to the affected subsection of the Availability Zone, we will be working towards restoring network connectivity and any EC2 instances that were impaired by the high ambient temperatures. We continue to recommend failing away to other Availability Zones in the region if you are able to do so.</div><div><span class=\"yellowfg\"> 4:12 PM PDT</span>&nbsp;We continue to work on resolving the connectivity issues affecting EC2 instances in a single Availability Zone (euc1-az1) within the EU-CENTRAL-1 Region. Unfortunately, we continue to wait for environmental conditions to improve within the subsection of the affected Availability Zone where it is safe to send in engineers. Some EBS volumes are also experiencing degraded performance within the affected Availability Zone, but are expected to recover once network connectivity is restored. We are working to resolve the issue and will keep you updated on our progress.</div><div><span class=\"yellowfg\"> 4:33 PM PDT</span>&nbsp;We have restored network connectivity within the affected Availability Zone and continue to work on full recovery of EC2 instances and EBS volumes. Customers should begin to see recovery at this stage.</div><div><span class=\"yellowfg\"> 5:19 PM PDT</span>&nbsp;We have restored network connectivity within the affected Availability Zone in the EU-CENTRAL-1 Region. The vast majority of affected EC2 instances have now fully recovered but we’re continuing to work through some EBS volumes that continue to experience degraded performance. The environmental conditions within the affected Availability Zone have now returned to normal levels. We will provide further details on the root cause in a subsequent posts, but can confirm that there was no fire within the facility.</div><div><span class=\"yellowfg\"> 6:54 PM PDT</span>&nbsp;Starting at 1:18 PM PDT we experienced connectivity issues to some EC2 instances, increased API errors rates, and degraded performance for some EBS volumes within a single Availability Zone in the EU-CENTRAL-1 Region. At 4:26 PM PDT, network connectivity was restored and the majority of affected instances and EBS volumes began to recover. At 4:33 PM PDT, increased API error rates and latencies had also returned to normal levels. The issue has been resolved and the service is operating normally.\n\nThe root cause of this issue was a failure of a control system which disabled multiple air handlers in the affected Availability Zone. These air handlers move cool air to the servers and equipment, and when they were disabled, ambient temperatures began to rise. Servers and networking equipment in the affected Availability Zone began to power-off when unsafe temperatures were reached. Unfortunately, because this issue impacted several redundant network switches, a larger number of EC2 instances in this single Availability Zone lost network connectivity.\n\nWhile our operators would normally had been able to restore cooling before impact, a fire suppression system activated inside a section of the affected Availability Zone. When this system activates, the data center is evacuated and sealed, and a chemical is dispersed to remove oxygen from the air to extinguish any fire. In order to recover the impacted instances and network equipment, we needed to wait until the fire department was able to inspect the facility. After the fire department determined that there was no fire in the data center and it was safe to return, the building needed to be re-oxygenated before it was safe for engineers to enter the facility and restore the affected networking gear and servers. The fire suppression system that activated remains disabled. This system is designed to require smoke to activate and should not have discharged. This system will remain inactive until we are able to determine what triggered it improperly. In the meantime, alternate fire suppression measures are being used to protect the data center. \n\nOnce cooling was restored and the servers and network equipment was re-powered, affected instances recovered quickly. A very small number of remaining instances and volumes that were adversely affected by the increased ambient temperatures and loss of power remain unresolved. We continue to work to recover those last affected instances and volumes, and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery of those resources, we recommend replacing any remaining affected instances or volumes if possible.</div>",
      "service": "ec2-eu-central-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Frankfurt)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1623359538",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:12 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the eu-central-1 Region.</div><div><span class=\"yellowfg\"> 3:05 PM PDT</span>&nbsp;We have identified the root cause of the issue causing increased elevated error rates and latencies in the EU-CENTRAL-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:46 PM PDT</span>&nbsp;We are starting to see some recovery in API errors and latencies in the EU-CENTRAL-1 Region and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;Between 1:10 PM and 5:17 PM PDT we experienced increased error rates and latencies for EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-eu-central-1"
    },
    {
      "service_name": "Amazon Kinesis Firehose (Frankfurt)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1623360846",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:34 PM PDT</span>&nbsp;We are investigating increased API error rates in the eu-central-1 Region.</div><div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;We have identified the root cause of the issue causing increased elevated error rates in the eu-central-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:47 PM PDT</span>&nbsp;We are starting to see some recovery in API errors in the EU-CENTRAL-1 Regions and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;Between 1:10 PM PDT and 5:26 PM PDT we experienced increased error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "firehose-eu-central-1"
    },
    {
      "service_name": "Amazon Relational Database Service (Frankfurt)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1623363601",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:20 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some database instances in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 4:28 PM PDT</span>&nbsp;We are continuing to investigate connectivity affecting some instances in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\"> 4:59 PM PDT</span>&nbsp;Network connectivity has been restored within the affected Availability Zone in the EU-CENTRAL-1 Region and we are starting to see some recovery of affected RDS instances. We are continuing to work toward full recovery of the remaining RDS instances.</div><div><span class=\"yellowfg\"> 5:50 PM PDT</span>&nbsp;Network connectivity has been restored within the affected Availability Zone (euc1-az1) in the EU-CENTRAL-1 Region and we have recovered the vast majority of affected RDS instances. We are continuing to work toward full recovery of the remaining RDS instances.\n</div><div><span class=\"yellowfg\"> 6:57 PM PDT</span>&nbsp;Between 1:10 PM and 5:45 PM PDT a small number of instances were unavailable in the EU-CENTRAL-1 Region. The issue has been resolved for nearly all instances. A small number of remaining instances are hosted on hardware which was adversely affected by increased ambient temperatures and loss of power. We continue to work to recover all affected instances and have opened notifications for the remaining impacted customers via the Personal Health Dashboard.</div>",
      "service": "rds-eu-central-1"
    },
    {
      "service_name": "AWS CloudFormation (Frankfurt)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1623370690",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:18 PM PDT</span>&nbsp;We are investigating elevated error rates for the AWS CloudFormation APIs in the EU-CENTRAL-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:38 PM PDT</span>&nbsp;We are starting to see some recovery in API errors and latencies in the EU-CENTRAL-1 Regions and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 5:46 PM PDT</span>&nbsp;Between 4:45 PM and 5:29 PM PDT, AWS CloudFormation experienced elevated error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-eu-central-1"
    },
    {
      "service_name": "AWS CodeBuild (N. Virginia)",
      "summary": "[RESOLVED] Increased Latency for Build Operations",
      "date": "1623692973",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:49 AM PDT</span>&nbsp;Beginning at 5:20 AM PDT, we experienced increased queue latency for Build operations in the US-EAST-1 Region. We have identified the subsystem responsible for the increase in latency, and have begun mitigation efforts. At this time, we are beginning to see signs of recovery, and continue to work toward full resolution.</div><div><span class=\"yellowfg\">11:24 AM PDT</span>&nbsp;Beginning at 5:15 AM we began experiencing increased latencies for Build operations. At 7:15 AM, we began notifying impacted customers via the Personal Health Dashboard. By 9:48 AM, we had processed the backlog of latent build operations, and at 10:20 AM, we verified that all new build operations were progressing normally. The issue has been resolved and the service is operating normally.</div>",
      "service": "codebuild-us-east-1"
    },
    {
      "service_name": "AWS CodeBuild (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rate when creating a build",
      "date": "1623763587",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:26 AM PDT</span>&nbsp;We are investigating an issue where customers may run into errors preventing them from creating new builds in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:02 AM PDT</span>&nbsp;We have mitigated the issue and are monitoring for recovery. </div><div><span class=\"yellowfg\"> 8:11 AM PDT</span>&nbsp;Changes have been reverted and we continue to monitor recovery. </div><div><span class=\"yellowfg\"> 8:31 AM PDT</span>&nbsp;Recently, AWS CodeBuild experienced elevated AccountLimitExceededException error rates when starting a build.  The issue was resolved at 8:10 AM PDT, and the service is operating normally.</div>",
      "service": "codebuild-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1623887035",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:43 PM PDT</span>&nbsp;We are investigating increased error rates for ELB APIs in the US-EAST-2 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\"> 5:30 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for ELB APIs in the US-EAST-2 Region and continue to work towards resolution. Connectivity to existing load balancers remains unaffected.</div><div><span class=\"yellowfg\"> 6:42 PM PDT</span>&nbsp;The increased error rates and latencies for ELB APIs in the US-EAST-2 Region are related to a Amazon Relational Database Service issue. Connectivity to existing load balancers remains unaffected. For further updates, please use the RDS Service Health Dashboard post.</div><div><span class=\"yellowfg\"> 9:40 PM PDT</span>&nbsp;Between 3:30 PM and 8:14 PM PDT, we experienced increased API error rates and latencies in the US-EAST-2 Region which were related to the Amazon Relational Database Service issue. The root cause has been addressed, and the service is operating normally.</div>",
      "service": "elb-us-east-2"
    },
    {
      "service_name": "Amazon Relational Database Service (Ohio)",
      "summary": "[RESOLVED] Instance Unavailablility",
      "date": "1623887565",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:52 PM PDT</span>&nbsp;We are investigating connectivity issues to some RDS instances in the US-EAST-2 region.</div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;We can confirm connectivity issues to some RDS instances in the US-EAST-2 Region, and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 6:04 PM PDT</span>&nbsp;We are actively working to mitigate connectivity issues for some RDS instances in the US-EAST-2 Region and continue to work towards resolution. The issue with Aurora PostgreSQL is also impacting Elasticache, ELB, API Gateway and Elastic Beanstalk APIs. Existing Elasticache, ELB and API Gateway instances continue to operate normally.</div><div><span class=\"yellowfg\"> 6:33 PM PDT</span>&nbsp;We have narrowed the cause of the connectivity issues impacting some RDS instances in the US-EAST-2 Region and we are continuing to work towards mitigation. The issue with Aurora PostgreSQL is also impacting Elasticache, ELB, API Gateway and Elastic Beanstalk APIs. Existing Elasticache, ELB and API Gateway instances continue to operate normally.</div><div><span class=\"yellowfg\"> 7:09 PM PDT</span>&nbsp;We have begun to see some recovery for the connectivity issues impacting some RDS instances in the US-EAST-2 Region, and we are continuing to work towards mitigation.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We are making progress towards restoring connectivity for the set of impacted Aurora PostgreSQL instances. Some database instances may see intermittent periods of recovery, but may not be fully recovered and we continue to work towards full recovery of these instances.</div><div><span class=\"yellowfg\"> 9:45 PM PDT</span>&nbsp;Between 3:30 PM and 9:30 PM PDT, some Amazon Relational Database Service (RDS) Aurora PostgreSQL instances in the US-EAST-2 Region experienced increased latency and fault rates. This issue also caused issues with Elasticache, ELB, and API Gateway APIs. Existing Elasticache, ELB, and API Gateway instances were not impacted by this event and were operating normally during this entire impact period. We have recovered the vast majority of affected instances and will contact any customers who are still impacted through the Personal Health Dashboard (PHD).</div>",
      "service": "rds-us-east-2"
    },
    {
      "service_name": "Amazon Simple Storage Service (Stockholm)",
      "summary": "[RESOLVED] Elevated IAM Error Rate",
      "date": "1623976331",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:32 PM PDT</span>&nbsp;We are investigating an elevated failure rate for AWS IAM authentication API requests in the EU-NORTH-1 Region.  This issue will also cause API requests for other AWS services in the EU-NORTH-1 Region to experience elevated failures. </div><div><span class=\"yellowfg\"> 5:45 PM PDT</span>&nbsp;We are seeing signs of recovery of the IAM authentication API error rates, and this is also showing improvement for other services.  We are actively working towards full mitigation.</div><div><span class=\"yellowfg\"> 5:52 PM PDT</span>&nbsp;The IAM authentication API error rates continue to improve, and many services have fully recovered.  We are actively working towards full mitigation.</div><div><span class=\"yellowfg\"> 5:58 PM PDT</span>&nbsp;Between 4:59 PM and 5:32 PM PDT we experienced elevated failure rate for AWS IAM authentication API requests in the EU-NORTH-1 Region. While some other services continue to move toward recovery, requests made to S3 are no longer experiencing elevated error rates. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-eu-north-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Stockholm)",
      "summary": "[RESOLVED] Elevated IAM Error Rate",
      "date": "1623977653",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:54 PM PDT</span>&nbsp;The IAM authentication API error rates continue to improve, and many services have fully recovered.  We are actively working towards full mitigation.</div><div><span class=\"yellowfg\"> 6:14 PM PDT</span>&nbsp;Starting at 4:59 PM PDT, the AWS IAM authentication API experienced elevated error rates in the EU-NORTH-1 Region. This issue caused AWS services to experience increased error rates, as they rely on the IAM service to authenticate their API requests. Services began to see recovery at 5:24 PM, with full recovery at 6:02 PM. All services are operating normally in the EU-NORTH-1 Region at this time.</div>",
      "service": "ec2-eu-north-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Stockholm)",
      "summary": "[RESOLVED] ELB API and Connectivity Issues",
      "date": "1623980954",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:49 PM PDT</span>&nbsp;Starting at 4:59 PM PDT, the AWS IAM authentication API experienced elevated error rates in the EU-NORTH-1 Region. This issue caused ELB to experience increased API error rates and latencies, as the APIs rely on the IAM service to authenticate requests. Recovery for the APIs began recovery at 5:24 PM, with full recovery at 6:11 PM. Between 6:03 PM and 6:28 PM PDT, we experienced an increase in connectivity issues and WAF errors for some load balancers. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\"> 6:51 PM PDT</span>&nbsp;Starting at 4:59 PM PDT, the AWS IAM authentication API experienced elevated error rates in the EU-NORTH-1 Region. This issue caused ELB to experience increased API error rates and latencies, as the APIs rely on the IAM service to authenticate requests. Recovery for the APIs began recovery at 5:24 PM, with full recovery at 6:11 PM. Between 6:03 PM and 6:28 PM PDT, we experienced an increase in connectivity issues and WAF errors for some load balancers. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-eu-north-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Hong Kong)",
      "summary": "[已解決] API 錯誤率增加 | [已解决] API 错误率增加 | [RESOLVED] Increased API Error Rates",
      "date": "1623995010",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:43 PM PDT</span>&nbsp;我们正在调查AP-EAST-1区域中EC2 API错误率上升的原因 | 我們正在調查AP-EAST-1區域中EC2 API錯誤率上升的原因 | We are investigating increased API error rates for the EC2 APIs in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">10:51 PM PDT</span>&nbsp;我们确认在 AP-EAST-1 区域内的单个可用区中 EC2 API 的错误率上升和 某些EBS卷的性能下降 | 我們確認在 AP-EAST-1 區域內的單個可用區中 EC2 API 的錯誤率上升和 某些EBS卷的性能下降 | We can confirm increased error rates for the EC2 APIs and degraded performance for some EBS volumes within a single Availability Zone within the AP-EAST-1 Region</div><div><span class=\"yellowfg\">11:08 PM PDT</span>&nbsp;我们持续调查 EC2 API 的错误率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 区域中单个可用区 (ape1-az2) 内的 EC2 实例连接问题。 AP-EAST-1 区域内的其他可用区不受此事件的影响。|我們持續調查 EC2 API 的錯誤率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 區域中單個可用區 (ape1-az2) 內的 EC2 實例連接問題。 AP-EAST-1 區域內的其他可用區不受此事件的影響。| We continue to investigate increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. Other Availability Zones within the AP-EAST-1 Region are not affected by this event. </div><div><span class=\"yellowfg\">11:27 PM PDT</span>&nbsp;我们持续调查 EC2 API 的错误率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 区域中单个可用区 (ape1-az2) 内的 EC2 实例连接问题。 我们正在确认此事件的根本原因，但现阶段还无法确定。由于 AP-EAST-1 区域内的其他可用区不受此事件的影响，我们建议您暂时不要使用受影响的可用区。| 我們持續調查 EC2 API 的錯誤率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 區域中單個可用區 (ape1-az2) 內的 EC2 實例連接問題。 我們正在確認此事件的根本原因，但現階段還無法確定。由於 AP-EAST-1 區域內的其他可用區不受此事件的影響，我們建議您暫時不要使用受影響的可用區。| We continue to investigate increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. We are making progress towards determining root cause for this event, but have not been able to determine it at this stage. Since other Availability Zones within the AP-EAST-1 Region are not affected by this event, we recommend that you fail away from the affected Availability Zone.</div><div><span class=\"yellowfg\">Jun 18, 12:03 AM PDT</span>&nbsp;我们持续调查 EC2 API 的错误率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 区域中单个可用区 (ape1-az2) 内的 EC2 实例连接问题。 我们正在确认此事件的根本原因，我们认为这与 EC2 实例和附加 EBS 卷之间的通信有关。这会导致可用区内受影响 EBS 卷的性能下降，进而发生操作系统内的 IO 卡住而导致 EC2 实例受损。在我们继续努力解决问题的同时，我们建议您从受影响的可用区进行故障转移。 | 我們持續調查 EC2 API 的錯誤率增加、部分 EBS 卷的性能下降以及 AP-EAST-1 區域中單個可用區 (ape1-az2) 內的 EC2 實例連接問題。 我們正在確認此事件的根本原因，我們認為這與 EC2 實例和附加 EBS 卷之間的通信有關。這會導致可用區內受影響 EBS 卷的性能下降，進而發生操作系統內的 IO 卡住而導致 EC2 實例受損。在我們繼續努力解決問題的同時，我們建議您從受影響的可用區進行故障轉移。 | We continue to investigate increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. We are making progress in determining the root cause and believe it is related to communication between EC2 instances and attached EBS volumes. This leads to degraded performance for affected EBS volumes within the Availability Zone and can also lead to impaired EC2 instances due to stuck IO within the operating system. While we continue to work towards resolving the issue, we recommend that you fail away from the affected Availability Zone if you have not already done so.</div><div><span class=\"yellowfg\">Jun 18, 12:47 AM PDT</span>&nbsp;我們已確定導致 EC2 API 的錯誤率增加、AP-EAST-1 區域中某些 EBS 磁碟區和 EC2 實例連線問題的效能降低問題的根本原因。在受影響的可用區內的某些 EBS 儲存伺服器受損，導致受影響的 EBS 磁碟區效能降低。我們正在採取措施解決 EBS 儲存伺服器損害，這應該能開始解決受影響的 EC2 實例和 EBS 磁碟區的問題。在我們繼續努力解決問題的同時，我們建議您暫時不要使用受影響的可用區。| We have determined the root cause of the issue causing increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. Some EBS storage servers within the affected Availability Zone are impaired, which is causing the degraded performance for the affected EBS volumes. We are taking steps to address the EBS storage server impairments, which should begin to resolve the issue for affected EC2 instances and EBS volumes. We continue to recommend that you fail away from the affected Availability Zone if you have not already done so.</div><div><span class=\"yellowfg\">Jun 18, 12:57 AM PDT</span>&nbsp;我們持續看到受影響的可用性區域內 EBS 磁碟區的效能降低。絕大多數受影響的 EBS 磁碟區現在已經復原，我們正在處理剩餘的 EBS 磁碟區。雖然大部分的服務現在都在受影響的可用性區域內看到復原，但我們還是不建議在完成完整復原之前，還是不建議回復至可用區域。我們將繼續為您提供更新。\n我們已經開始採取措施來解決 EBS 儲存伺服器問題，並且正在看到某些受影響的 EBS 磁碟區的復原。我們將繼續處理剩餘的受損 EBS 儲存伺服器，以完全解決問題. | We have begun taking steps to address the EBS storage server impairments, and are seeing recovery for some of the affected EBS volumes. We will continue to work on remaining impaired EBS storage servers to fully resolve the issue.</div><div><span class=\"yellowfg\">Jun 18,  1:27 AM PDT</span>&nbsp;我们继续看到受影响的可用区内 EBS 卷的性能下降。大多数受影响的 EBS 卷现已恢复，我们将继续努力恢复其余受影响的卷。我们继续努力争取全面解决这个问题 | 我們持續看到受影響的可用性區域內 EBS 磁碟區的效能降低。大部分受影響的 EBS 磁碟區現在都已經復原，我們會繼續修復其餘受影響的磁碟區。我们继续努力全面解决这一问题 | We continue to see an improvement in degraded performance for EBS volumes within the affected Availability Zone. The majority of the affected EBS volumes have now been recovered and we continue to work on recovering the remaining affected volumes. We continue to work toward full resolution of the issue.</div><div><span class=\"yellowfg\">Jun 18,  1:53 AM PDT</span>&nbsp;我们继续看到受影响的可用区内 EBS 卷的性能下降。绝大多数受影响的 EBS 卷现已恢复，我们正在处理剩余的 EBS 卷。虽然大多数服务现在都在受影响的可用区内看到恢复，但我们建议在完成完全恢复之前，不要回到可用区。我们将继续为您提供最新信息. 我们已开始采取措施来解决 EBS 存储服务器损坏问题，并且看到一些受影响的 EBS 卷正在恢复。我们将继续处理剩余受损的 EBS 存储服务器，以充分解决此问题。| We continue to see an improvement in degraded performance for EBS volumes within the affected Availability Zone. The vast majority of affected EBS volumes have now been recovered and we are working on the remaining EBS volumes. While most services are now seeing recovery within the affected Availability Zone, we do not yet recommend failing back to the Availability Zone until we have completed full recovery. We will continue to provide you with updates.</div><div><span class=\"yellowfg\">Jun 18,  2:36 AM PDT</span>&nbsp;从 12:58 PM UTC+8 开始，在 AP-EAST-1 地区的一个可用区（ape1-az2）内，我们遇到了 EC2 APIs 的错误率增加、一些 EBS 卷的性能下降和 EC2 实例连接问题。该问题的根本原因是受影响的可用性区域内底层 EBS 存储服务器的性能下降。工程师采取了行动，缓解和解决 EBS 存储服务器性能下降的问题，从而解决了这个问题。在 03:45 PM UTC+8 性能下降的 EBS 卷开始恢复，到 03:58 PM UTC+8，绝大部分受影响的 EBS 卷已经恢复。我们继续在少数仍在经历性能下降的 EBS 卷上工作，并将通过个人健康仪表板或这些卷提供进一步的更新。所有的服务现在都在受影响的可用性区域内正常运行。这个问题已经得到解决，服务运行正常。| 從 12:58 PM UTC+8 開始，在 AP-EAST-1 地區的一個可用區（ape1-az2）內，我們遇到了 EC2 APIs 的錯誤率增加、一些 EBS 卷的性能下降和 EC2 實例連接問題。該問題的根本原因是受影響的可用性區域內底層 EBS 存儲服務器的性能下降。工程師採取了行動，緩解和解決 EBS 存儲服務器性能下降的問題，從而解決了這個問題。在 03:45 PM UTC+8 性能下降的 EBS 捲開始恢復，到 03:58 PM UTC+8，絕大部分受影響的 EBS 卷已經恢復。我們繼續在少數仍在經歷性能下降的 EBS 捲上工作，並將通過個人健康儀表板或這些卷提供進一步的更新。所有的服務現在都在受影響的可用性區域內正常運行。這個問題已經得到解決，服務運行正常。| Starting at 9:58 PM PDT we experienced increased error rates for the EC2 APIs, degraded performance for some EBS volumes and EC2 instance connectivity issues within a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. The root cause of the issue was degraded performance for underlying EBS storage servers within the affected Availability Zone. Engineers took action to mitigate and resolve the degraded EBS storage server performance, which resolved the issue. At 12:41 AM PDT, EBS volumes with degraded performance began to recover and by 12:58 AM PDT, the vast majority of affected EBS volumes had recovered. We continue to work on a small number of EBS volumes that are still experiencing degraded performance, and will provide further updates via the Personal Health Dashboard for those volumes. All services are now operating normally within the affected Availability Zone. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-east-1"
    },
    {
      "service_name": "Amazon Relational Database Service (Hong Kong)",
      "summary": "[已解决] 实例不可用 ｜[已解決] 實例不可用 ｜[RESOLVED] Instance Unavailability",
      "date": "1623996605",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:10 PM PDT</span>&nbsp;我們正在調查 AP-EAST-1 區域中影響某些實例的連接問題 |We are investigating connectivity issues affecting some instances in the AP-EAST-1 Region.</div><div><span class=\"yellowfg\">11:55 PM PDT</span>&nbsp;我們持續調查 AP-EAST-1 區域中，影響到單一可用區(ape1-az2)實例的連接問題，這問題與許多 EC2 API 的錯誤有關係。| We continue to investigate connectivity issues affecting instances in a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. The issues are related to the increased EC2 API error rates.</div><div><span class=\"yellowfg\">Jun 18,  2:51 AM PDT</span>&nbsp; 從 12:58 PM UTC+8 開始，我們遇到了影響 AP-EAST-1 區域的單一可用區域 (ape1-az2) 執行個體的連線能力和 EBS 儲存問題。問題的根本原因與 EC2 API 錯誤率的增加有關。恢復後的 EC2，並且在 5:45PM UTC+8之前，絕大多數受影響的執行個體已經復原。我們會繼續處理少數仍遇到問題的執行個體，並透過個人健康儀表板提供進一步的更新。問題已解決，服務正常運作。| Starting at 9:58 PM PDT, we experienced connectivity and EBS storage issues affecting instances in a single Availability Zone (ape1-az2) in the AP-EAST-1 Region. The root cause of the issue was related to increased EC2 API error rates. Recovery followed EC2's, and by 2:45 AM PDT, the vast majority of affected instances had recovered. We continue to work on a small number of instances that are still experiencing issues, and will provide further updates via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-ap-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Ohio)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1624564393",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:53 PM PDT</span>&nbsp;Between 12:15 PM and 12:21 PM PDT we observed connectivity issues between some customer networks and the US-EAST-2 Region. Connectivity within the Region was not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-east-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Network Connectivity Issues",
      "date": "1624992057",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:40 AM PDT</span>&nbsp;Between 11:07 AM and 11:15 AM PDT we experienced a network connectivity issue that impacted Amazon VPC Inter-Region Peering in the US-EAST-2 Region. Connectivity to instances and services within the Region was not impacted by the event. The issue has been resolved and connectivity has been restored. </div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS Certificate Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latency",
      "date": "1624993969",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:12 PM PDT</span>&nbsp;We are investigating increased API latency and error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:36 PM PDT</span>&nbsp;We can confirm increased API latency and increased API error rates for the ACM APIs in the US-EAST-1 Region. During this time, you may be unable to Request new certificates, and may also observe errors when attempting to List and/or nodify existing certificates. This issue impacts both the AWS Management Console, and the ACM APIs. Additionally, you may also receive API errors when attempting to associate new resources. Existing associated resources are unaffected, and continue to operate as normal. We have identified the root cause of the issue and are working toward mitigation and resolution. We will provide further updates as we have more information to share.</div><div><span class=\"yellowfg\"> 1:14 PM PDT</span>&nbsp;We continue to work toward mitigating the affected subsystem responsible for the increase in API Errors and Latencies for the ACM APIs. Other AWS Services (such as ClientVPN) who attempt to create or associate new certificates may also be impacted by this issue. Existing resources remain unaffected by this issue and continue to operate normally. </div><div><span class=\"yellowfg\"> 2:13 PM PDT</span>&nbsp;We are continuing to drive to root cause and work toward mitigating the affected subsystem responsible for the increase in API Errors and Latencies for the ACM APIs. Other AWS Services (such as ClientVPN) who attempt to create or associate new certificates may also be impacted by this issue. Existing resources remain unaffected by this issue and continue to operate normally.</div><div><span class=\"yellowfg\"> 2:56 PM PDT</span>&nbsp;We have identified some workloads on the affected subsystem of the ACM API that may be causing the increase in API errors and latency, and we are reviewing and testing procedures to mitigate their impact.  We do not have an ETA at this time.  This issue does affect services like CloudFront and ElasticSearch that rely on ACM for their certificate needs. It would also impact CloudFormation workflows that either directly or indirectly need to manipulate ACM certificates.\n\nAll workflows that depend on ACM certificates that are already created are not impacted by this event, and continue to operate normally.</div><div><span class=\"yellowfg\"> 3:33 PM PDT</span>&nbsp;We continue to work toward mitigating the increased latencies and error rates affecting the ACM APIs. Until this point, some requests and retries have been succeeding. At this time, we are temporarily not accepting additional API requests, in order to help accelerate mitigation and recovery. Once we begin accepting new API requests, requests will be throttled. We will continue to provide updates as we progress.</div><div><span class=\"yellowfg\"> 4:49 PM PDT</span>&nbsp;We are starting to see some ACM API calls succeed for CloudFront and ELB and we are starting to propagate changes for CloudFront distributions to our edge locations.  Customer facing APIs are still throttled.  ACM is continuing to make progress towards recovery.  </div><div><span class=\"yellowfg\"> 5:47 PM PDT</span>&nbsp;We are seeing recovery for customers and throttling has been removed from most APIs.  We are working through the final changes to unblock the following APIs: RequestCertificate, ListCertificates, and ImportCertificate and expect to have those final changes in-place shortly. We will update as we make progress towards full recovery. </div><div><span class=\"yellowfg\"> 7:05 PM PDT</span>&nbsp;We are seeing recovery for customers and throttling has been removed from most APIs.  We have unblocked RequestCertificate for most use cases and are working to have the final changes in-place shortly. We will update as we make progress towards full recovery.</div><div><span class=\"yellowfg\"> 7:46 PM PDT</span>&nbsp;Between 11:45 AM and 7:42 PM PDT, customers experienced increased ACM API errors and latency in the US-EAST-1 Region that impacted the ability to issue new certificates, import certificates and retrieve information about certificates from ACM. Existing certificates that were already vended to services such as CloudFront and ELB continued to operate and were unaffected. This issue also impacted provisioning and scaling workflows for services that depend on ACM for certificate management needs, such as CloudFront and ELB, as well as CloudFormation operations that involve mutating ACM certificates. This issue was caused by a previously unknown limit in an ACM storage subsystem. We have identified the limit issue and have mitigated it.  The issue has been fully resolved and all ACM API requests are being answered normally.  During this time, all existing resources that had a configured ACM certificate (such as ELB load balancers and CloudFront distributions) continued to operate normally, and were not impaired by this issue.</div>",
      "service": "certificatemanager-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Propagation Delays",
      "date": "1625000294",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:58 PM PDT</span>&nbsp;We are investigating delays in propagating changes to CloudFront distributions to our edge locations. This is related to the ACM issue in the US-EAST-1 Region that we have posted to the Service Health Dashboard. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 4:08 PM PDT</span>&nbsp;CloudFront distribution changes continue to be affected by the AWS Certificate Manager issue in US-EAST-1. ACM is continuing to make progress towards recovery, but change propagation for CloudFront distribution changes will continue to be affected until the ACM issue is fully mitigated. At that point, we will begin to process the backlog of distribution changes. The backlog of changes may take additional time to complete. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 5:06 PM PDT</span>&nbsp;We are starting to see some ACM API calls succeed for CloudFront and we are starting to propagate changes for CloudFront distributions to our edge locations. ACM is continuing to make progress towards recovery. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 6:11 PM PDT</span>&nbsp;We continue to process the backlog of distribution changes and propagate the updates to our edge locations. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 7:38 PM PDT</span>&nbsp;Between 11:45 AM and 7:25 PM PDT, we experienced delays in propagating changes to CloudFront distributions to our edge locations due to the ACM issue in US-EAST-1. This issue has been resolved and the service is operating normally. During this time, previous configured distributions continued to operate without any issues and there were no issues with serving content from our CloudFront edge locations.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Frankfurt)",
      "summary": "[RESOLVED] Instance connectivity",
      "date": "1626179354",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:29 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs and connectivity issues for some instances in a single Availability Zone in the EU-CENTRAL-1 Region</div><div><span class=\"yellowfg\"> 6:05 AM PDT</span>&nbsp;We are seeing increased error rates and latencies for the RunInstances and CreateSnapshot APIs, and increased connectivity issues for some instances in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. We have resolved the networking issues that affected the majority of instances within the affected Availability Zone, but continue to work on some instances that are experiencing degraded performance for some EBS volumes. Other Availability Zones are not affected by this issue. We would recommend failing away from the affected Availability Zone until this issue has been resolved.</div><div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We continue to make progress in resolving the connectivity issues affecting some instances in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. The increased error rates and latencies for the RunInstance and CreateSnapshot APIs have been resolved, as well as the degraded performance for some EC2 volumes within the affected Availability Zone. We continue to work on the remaining EC2 instances that are still impaired as a result of this event, some of which may have experienced a power cycle. While we do not expect any further impact at this stage, we would recommend continuing to utilize other Availability Zones in the EU-CENTRAL-1 region until this issue has been resolved.</div><div><span class=\"yellowfg\"> 7:24 AM PDT</span>&nbsp;Starting at 5:07 AM PDT we experienced increase connectivity issues for some instances, degraded performance for some EBS volumes and increased error rates and latencies for the EC2 APIs in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. By 6:03 AM PDT, API error rates had returned to normal levels, but some Auto Scaling workflows continued to see delays until 6:35 AM PDT. By 6:10 AM PDT, the vast majority of EBS volumes with degraded performance had been resolved as well, and by 7:05 AM PDT, the vast majority of affected instances had been recovered, some of which may have experienced a power cycle. A small number of remaining instances are hosted on hardware which was adversely affected by this event and require additional attention. We continue to work to recover all affected instances and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances if possible.</div>",
      "service": "ec2-eu-central-1"
    },
    {
      "service_name": "Amazon CloudWatch (Tokyo)",
      "summary": "[RESOLVED] エラー率およびレイテンシーの上昇 | Increased Error rates and Latencies",
      "date": "1626280770",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:39 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの増加を調査しています。現在この問題の解決に取り組んでいます。 | We are investigating increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:04 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける API エラー率の上昇およびログイベントの遅延を確認しています。現在この問題の解決に取り組んでいます。| We can confirm elevated API error rates and some delayed log events in AP-NORTHEAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける API エラー率の上昇およびログイベントの遅延の原因を特定しました。復旧の兆候を確認しており、解決に向け引き続き対応を行っています。 | We have identified the root cause of the elevated API error rates and some delayed log events in the AP-NORTHEAST-1 Region. We are beginning to see signs of recovery and continue working towards resolution.</div><div><span class=\"yellowfg\">12:07 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの増加について引き続き調査および解決に取り組んでいます。| We continue to work on investigating and resolving increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 2:28 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの上昇を解消すると予測される緩和策の実施を完了しました。エラー率の改善が見られる一方で引き続き高い水準のレイテンシーを確認しており、完全な解決に向け引き続き対応を行っています。 | We have completed a mitigation strategy that we anticipated would resolve the increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. While we have seen some recovery in error rates, we continue to see elevated levels of latency and continue working towards full resolution.</div><div><span class=\"yellowfg\"> 3:42 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のレイテンシー上昇と残存しているエラーの解消のため、第二の緩和策の展開を行なっております。解決に向け引き続き対応を行っております。 | We are deploying a second mitigation strategy to resolve elevated latency and the remaining level of errors for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. We continue working towards recovery.</div><div><span class=\"yellowfg\"> 5:05 PM PDT</span>&nbsp;日本時間 2021/07/14 23:01 から 2021/07/15 08:20 の間、AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの上昇が発生しておりました。この問題は解決し、現在サービスは正常に動作しています。 | Between 7:01 AM and 4:20 PM PDT we experienced increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudwatch-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Milan)",
      "summary": "[RESOLVED] Increased Error rates",
      "date": "1626910891",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;We are investigating increased API error rates for the RunInstances API in the EU-SOUTH-1 Region. </div><div><span class=\"yellowfg\"> 5:04 PM PDT</span>&nbsp;We can confirm increased API error rates for the RunInstances API in the EU-SOUTH-1 Region. This is also affecting services that depend on EC2 such as Auto Scaling, and launches of service instances that are built on EC2, such as RDS and ElastiCache. Instances that are already launched are operating normally. We have identified the root cause and are actively testing a mitigation plan. We expect to have an update on the success of this mitigation effort in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:19 PM PDT</span>&nbsp;Between 3:59 PM and 5:07 PM PDT customers experienced increased error rates for the EC2 RunInstances API in the EU-SOUTH-1 Region. This is also affected services that depend on EC2 such as Auto Scaling, and launches of service instances that are built on EC2, such as RDS and ElastiCache. The issue has been resolved and the RunInstances API is now operating normally.  This issue only affected new instance launches, instances that were already running were not affected. </div>",
      "service": "ec2-eu-south-1"
    },
    {
      "service_name": "AWS Lambda (Milan)",
      "summary": "[RESOLVED] Increased Invoke Error Rate",
      "date": "1628028846",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:14 PM PDT</span>&nbsp;We are investigating increased invoke error rates in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 3:45 PM PDT</span>&nbsp;Between 2:39 PM and 3:16 PM PDT, we experienced increased invoke error rates in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "lambda-eu-south-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Management Console Error Rates",
      "date": "1628029136",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:18 PM PDT</span>&nbsp;We are investigating increased errors for AWS Management Console in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 3:46 PM PDT</span>&nbsp;Between 2:38 PM and 3:16 PM PDT, we experienced increased errors for AWS Management Console in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon ElastiCache (Ireland)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1628216388",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:19 PM PDT</span>&nbsp;We are experiencing increased latencies while provisioning new ElastiCache nodes and elevated API error rates in the EU-WEST-1 Region. Existing ElastiCache clusters are not impacted and are continuing to serve traffic. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:33 PM PDT</span>&nbsp;Between 6:22 PM and 7:31 PM PDT, Amazon ElastiCache experienced increased latencies while provisioning new ElastiCache nodes and elevated API error rates in the EU-WEST-1 Region. Existing ElastiCache clusters were not impacted and continued to serve traffic. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticache-eu-west-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1628298493",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:08 PM PDT</span>&nbsp;We are investigating increased error rate and latencies for DynamoDB in US-EAST-1 region.</div><div><span class=\"yellowfg\"> 6:32 PM PDT</span>&nbsp;We can confirm increased error rates for DynamoDB in the US-EAST-1 Region. We have identified the root cause of the issue and are working towards resolution.</div><div><span class=\"yellowfg\"> 7:03 PM PDT</span>&nbsp;We have seen some improvement to the error rates for DynamoDB in the US-EAST-1 Region and continue to work towards full resolution. For customers that are experiencing 503 errors, retries may resolve the issue in some cases. In other cases, recreating the connection to DynamoDB may address the error rates. We continue to take steps towards full resolution for all affected tables.</div><div><span class=\"yellowfg\"> 8:11 PM PDT</span>&nbsp;We continue to make progress in addressing the increased error rates for DynamoDB in the US-EAST-1 Region. The root cause of the issue is a problem with the metadata subsystem used by DynamoDB, where several nodes are in an unhealthy state. We continue to work towards restoring the health of these nodes. The issue affected a subset of DynamoDB tables that are associated with the unhealthy nodes in the metadata subsystem. For these tables, customers will experience increased error rates until we have resolved the issue. DynamoDB tables that are not associated with the affected metadata nodes, are not affected by this issue. We continue to work towards full resolution.</div><div><span class=\"yellowfg\"> 9:00 PM PDT</span>&nbsp;We continue to see an improvement in the error rates for affected DynamoDB tables in the US-EAST-1 Region. Since the start of the event, we have seen a 75% reduction in error rates and are now working on resolving the errors for the remaining DynamoDB tables. </div><div><span class=\"yellowfg\">10:20 PM PDT</span>&nbsp;We have resolved the error rates for the majority of the affected DynamoDB tables and now have a small number of DynamoDB tables that are still experiencing error rates and a small number of global secondary indexes that are experiencing propagation delays. While all the nodes in the metadata store are now healthy, some are not yet able to process incoming requests, which we are working to resolve.</div><div><span class=\"yellowfg\">11:11 PM PDT</span>&nbsp;We have now resolved the error rates affecting DynamoDB tables in the US-EAST-1 Region. A small number of DynamoDB tables continue to experience delayed propagation for global secondary indexes, but these are moving towards full recovery as well. We’re continuing to monitor the service, but customers should be seeing recovery for their DynamoDB tables at this stage.</div><div><span class=\"yellowfg\">Aug 7,  1:50 AM PDT</span>&nbsp;We have now resolved the Global Secondary index propagation delays for most customers in the US-EAST-1 region. Our mitigation efforts are working as expected and we continue to work towards full recovery.</div><div><span class=\"yellowfg\">Aug 7,  2:53 AM PDT</span>&nbsp;Between August 6 5:23 PM and August 7 2:48 AM PDT, DynamoDB customers experienced API errors and delayed propagation for global secondary indexes in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "dynamodb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Kubernetes Service (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1628306280",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:18 PM PDT</span>&nbsp;We are investigating increased error rates for Cluster create and update operations in the US-EAST-1 Region. Some existing clusters are also impacted.</div><div><span class=\"yellowfg\"> 9:30 PM PDT</span>&nbsp;We are continuing to investigate increased error rates for Cluster create and update operations in the US-EAST-1 Region. We are seeing recovery for some clusters, and continue to work on recovery for all clusters.</div><div><span class=\"yellowfg\">10:34 PM PDT</span>&nbsp;We can confirm errors creating and updating Clusters in the US-EAST-1 Region, and are continuing to work towards resolution. We are starting to see recovery for existing clusters, and continue to work on recovery for all clusters.</div><div><span class=\"yellowfg\">11:33 PM PDT</span>&nbsp;We continue to see improvement in the error rates for creating and upgrading clusters in the US-EAST-1 Region. We are starting to see recovery in availability of existing clusters. We are continuing to work towards a full resolution.</div><div><span class=\"yellowfg\">Aug 7, 12:27 AM PDT</span>&nbsp;Between August 6 5:50 PM PDT and 11:27 PM PDT, we experienced errors for creating and upgrading clusters in the US-EAST-1 Region. Some existing clusters were also impacted during this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "eks-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased error rates",
      "date": "1628306461",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:21 PM PDT</span>&nbsp;We are investigating increased provisioning latencies for new load balancers, delayed propagation of load balancer CloudWatch metrics, and inconsistent health check statuses from the DescribeTargetHealth API for Network Load Balancers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:08 PM PDT</span>&nbsp;We can confirm delayed propagation of load balancer CloudWatch metrics and provisioning latencies for Network Load Balancers in the US-EAST-1 Region, and are continuing to work towards resolution. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\">11:22 PM PDT</span>&nbsp;Between 6:08 PM and 11:12 PM PDT, we experienced increased provisioning latencies for Network Load Balancers in the US-EAST-1 Region, which has now fully recovered. We are continuing to experience delays in the propagation of CloudWatch metrics for load balancers. The system used to generate these metrics has begun to recover, and we anticipate full recovery in the next few hours after we process the backlog of metrics.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] DescribeImages API",
      "date": "1628699197",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:26 AM PDT</span>&nbsp;We are investigating an issue where the ‘virtualization type’ field is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. The virtualization type for some images is being returned as “paravirt” instead of “hvm”. This can impact the instance selection in the console and also cause some EMR jobs to fail to start as EMR does not support “paravirt” images. We have identified the root cause of the issue and are working to resolve it. Note that the underlying images (AMI) are not affected by this issue, which has only affected the returning of the metadata for the ‘virtualization type’ field. Once resolved, images will continue to operate as they did before. </div><div><span class=\"yellowfg\">10:25 AM PDT</span>&nbsp;We continue to make progress in resolving the issue where the virtualization type is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. The issue will prevent some images from launching on the current generation (e.g. c5, m5, r5, t3, etc.) instance types and some previous generation instances (e.g t2) as they do not support “paravirt” virtualization. Customers could choose to launch on a previous generation instance type that does support ‘paravirt’ virtualization, or attempt launching via the EC2 command line tools, which in many cases when allow for the instance to be launched. We continue to work on resolving the issue.</div><div><span class=\"yellowfg\">11:01 AM PDT</span>&nbsp;We have begun to see recovery for some of the affected images (AMI), which are now returning the correct virtualization type. We expect to see full recovery within the next hour, at which time all operations affected by this issue should be operating normally. We will provide an update once we have fully recovered.</div><div><span class=\"yellowfg\">11:24 AM PDT</span>&nbsp;We have resolved the issue where the virtualization type is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. Starting at 2:43 AM PDT, some images (AMI) returned the incorrect virtualization type (‘paravirt’ instead of ‘hvm’) in a DescribeImages API call. Since some instance types do not support ‘paravirt’ images, this prevented the EC2 Management Console and some EMR jobs from being able to launch new EC2 instances. As of 11:17 AM PDT, the issue has been fully resolved. Instances launched during this time do not need to be relaunched. The issue has been resolved and the service is operating normally.\n</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1629201355",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:55 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:24 AM PDT</span>&nbsp;We can confirm increased API error rates for ECS in the US-EAST-1 Region. We have identified the issue and are working towards mitigation. In most cases, a retry of the API operation will succeed.</div><div><span class=\"yellowfg\"> 6:15 AM PDT</span>&nbsp;We can confirm increased API error rates for ECS in the US-EAST-1 Region. We have identified the root cause and continue to work towards recovery. In most cases, a retry of the API operation will succeed.\n</div><div><span class=\"yellowfg\"> 7:12 AM PDT</span>&nbsp;Between 3:40 AM and 6:45 AM PDT we experienced increased API error rates for ECS in the US-EAST-1 Region. In most cases, API calls succeeded when retried.  We continue to work to recover residual impact and have opened notifications for the remaining impacted customers via the Personal Health Dashboard.</div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "Amazon EventBridge (N. Virginia)",
      "summary": "[RESOLVED] Elevated Event Delivery Latency",
      "date": "1629229180",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:39 PM PDT</span>&nbsp;We can confirm elevated event delivery latency for EventBridge events in the US-EAST-1 Region. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency. </div><div><span class=\"yellowfg\"> 1:53 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We have taken initial mitigation actions, which have reduced the delivery latency for some requests, and continue to work toward full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 3:12 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We are seeing further reduction of the delivery latency for some requests. We are exploring further mitigation steps as we continue to work toward full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 4:39 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We are seeing further reduction of the delivery latency for more requests. We have identified the component that is contributing to the event delivery latency and are mitigating impact within this component as we work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 5:36 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We continue to observe further reduction of the delivery latency for more requests. We have completed additional mitigation steps that have led to the reduction in delivery latency and continue to work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 6:17 PM PDT</span>&nbsp;We continue to take mitigation actions to improve elevated event delivery latency for EventBridge events in the US-EAST-1 Region. While we observe improvement in event delivery latency, we are still experiencing elevated latency in the component responsible for matching events and continue to work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 7:07 PM PDT</span>&nbsp;We previously took steps towards mitigating event delivery latency based upon the suspected root cause and observed improvements in event delivery latency in the US-EAST-1 Region. We have now identified that we are continuing to see event delivery latency due to the component responsible for matching events with EventBridge rules and are taking further steps to mitigate the issue as we work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 8:06 PM PDT</span>&nbsp;We have completed our actions towards mitigating event delivery latency for EventBridge events in the US-EAST-1 Region. Event delivery latency for new events has fully recovered and we are processing the backlog of previously published events. Other AWS Services that make use of EventBridge are observing improvements in elevated event delivery latency as well.\n</div><div><span class=\"yellowfg\"> 9:05 PM PDT</span>&nbsp;Between 8:31 AM and 9:00 PM PDT, we experienced elevated event delivery latency for EventBridge events in the US-EAST-1 Region. Other AWS services that make use of EventBridge are no longer experiencing elevated event delivery latency. The issue has been resolved and the service is operating normally. </div>",
      "service": "events-us-east-1"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] IAM errors and propagation latency",
      "date": "1629996723",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:52 AM PDT</span>&nbsp;We are investigating elevated latencies and errors on the IAM APIs. In addition, we are investigating propagation delays for recently created or recently updated IAM users, credentials, roles, policies. Authentication and authorization of existing users, credentials, roles, policies are not impacted. Other AWS services like AWS CloudFormation that use IAM roles were also impacted.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;Between 6:44 AM and 9:12 AM PDT, customers experienced elevated latency and error rates in response to IAM API requests, as well as delays in describing recently created or modified IAM resources. In addition, between 6:44 AM and 10:02 AM, propagation of IAM API updates was delayed in the ME-SOUTH-1, EU-SOUTH-1, AP-EAST-1, and AF-SOUTH-1 regions, and newly created or recently updated IAM users, credentials, roles, and policies may not have been available for authentication and authorization in those regions. Other AWS Services that rely on IAM changes to provision resources, such as CloudFormation, were also impacted. Authentication and authorization for existing users, credentials, roles, policies were not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "AWS Internet Connectivity (Oregon)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1630434320",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:25 AM PDT</span>&nbsp;We are investigating an issue which is affecting network traffic for some customers using AWS services in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:13 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. While we continue to work towards root cause, we believe that the issue is affecting connectivity to Network Load Balancers from EC2 instances, connectivity from Lambda to EC2 instances and other AWS services, as well as connectivity between EC2 and some AWS services using PrivateLink. In an effort to further mitigate the impact, we are shifting some services and network flows away from the affected Availability Zone to mitigate the impact.</div><div><span class=\"yellowfg\"> 1:00 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. We have narrowed down the issue to an increase in packet loss within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services. The issue continues to only affect the single Availability Zone (usw2-az2) within the US-WEST-2 region, so shifting traffic away from Networking Load Balancer and NAT Gateway within the affected Availability Zone can mitigate the impact. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, are seeing impact as a result of this issue. </div><div><span class=\"yellowfg\"> 2:26 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region and are actively working on mitigation. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in usw2-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in usw2-az2.</div><div><span class=\"yellowfg\"> 3:23 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:02 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. Beginning at 10:58 AM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the US-WEST-2 Region. At 2:45 PM, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:35 PM, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Oregon)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1630435612",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:46 AM PDT</span>&nbsp;We are investigating connectivity, provisioning, and target registration issues for load balancers in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:10 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. While we continue to work towards root cause, we believe that the issue is affecting connectivity to Network Load Balancers from EC2 instances, connectivity from Lambda to EC2 instances and other AWS services, as well as connectivity between EC2 and some AWS services using PrivateLink. In an effort to further mitigate the impact, we are shifting some services and network flows away from the affected Availability Zone to mitigate the impact.</div><div><span class=\"yellowfg\"> 1:00 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. We have narrowed down the issue to an increase in packet loss within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services. The issue continues to only affect the single Availability Zone (usw2-az2) within the US-WEST-2 region, so shifting traffic away from Networking Load Balancer and NAT Gateway within the affected Availability Zone can mitigate the impact. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, are seeing impact as a result of this issue. </div><div><span class=\"yellowfg\"> 2:24 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region and are actively working on mitigation. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in usw2-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in usw2-az2. </div><div><span class=\"yellowfg\"> 3:22 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:02 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. Beginning at 10:58 AM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the US-WEST-2 Region. At 2:45 PM, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:35 PM, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (Ireland)",
      "summary": "[RESOLVED] Connectivity Issues",
      "date": "1630445941",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;We are investigating connectivity issues for load balancers in a single Availability Zone (euw1-az2) in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;We can confirm network connectivity issues affecting a single Availability Zone (euw1-az2) in the EU-WEST-1 Region and are actively working on mitigation. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, may also see impact as a result of this issue. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in euw1-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in euw1-az2.</div><div><span class=\"yellowfg\"> 3:35 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:04 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (euw1-az2) in the EU-WEST-1 Region. Beginning at 2:19 PM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the EU-WEST-1 Region. At 3:25 PM PDT, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:40 PM PDT, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-eu-west-1"
    },
    {
      "service_name": "AWS Direct Connect (Tokyo)",
      "summary": "[RESOLVED] ネットワーク接続性 | Network Connectivity",
      "date": "1630543140",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;日本時間 2021/09/02 07:30 から一部の AWS Direct Connect 接続と AP-NORTHEAST-1 リージョン間にネットワーク接続性の問題が発生していることを確認しております。この問題について調査を行っております。| Starting at 3:30 PM PDT, we began to experience network connectivity issues, impacting AWS Direct Connect connectivity between some AWS Direct Connections and the AP-NORTHEAST-1 Region. We are actively investigating the issue. </div><div><span class=\"yellowfg\"> 6:02 PM PDT</span>&nbsp;一部の AWS Direct Connect 接続と AP-NORTHEAST-1 リージョン間にネットワーク接続性の問題について追加の情報をご案内いたします。日本時間 2021/09/02 07:30 からコアネットワークデバイスに複数の問題が発生していることを確認しております。現在、問題が発生したデバイスについて復旧を進めており、デバイスがオンラインの状態に戻ることで接続性の問題が解消することが期待されます。現状では復旧の目途に関する情報はございません。進展がございましたら、随時更新致します。| We wanted to provide some more information for the event affecting some Direct Connect network connectivity in the AP-NORTHEAST-1 Region. Starting at 3:30 PM PDT, we began to experience network connectivity issues due to some failures in core networking devices. We are currently working on restoring these devices and we expect some restoration of connectivity as these devices come back online. We currently do not have an ETA on full recovery and will update further as information comes to hand. </div><div><span class=\"yellowfg\"> 6:43 PM PDT</span>&nbsp;現在引き続き故障したデバイスの復旧を試みており、完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。| We are still trying to recover the failed devices and do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover are recommended to do so to achieve recovery.</div><div><span class=\"yellowfg\"> 7:33 PM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョン内の故障したデバイスの復旧に取り組んでおりますが、現時点において完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。| We are continuing to work on recovering a number of failed devices within the AP-NORTHEAST-1 Region, but do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover to VPN are recommended to do so to achieve recovery.</div><div><span class=\"yellowfg\"> 8:20 PM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョン内の故障したデバイスの復旧に取り組んでおりますが、現時点において完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。Direct Connect Gateway と Transit Gateway をご利用のお客様に関しては、AWS Site-to-Site VPN をご作成いただき Transit Gateway にアタッチしてご利用いただくことをお勧めいたします。こちらの VPN へのフェイルオーバーの設定手順に関しては次の記事をご参照ください: <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/\">https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/</a> | We are continuing to work on recovering a number of failed devices within the AP-NORTHEAST-1 Region, but do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover to VPN are recommended to do so to achieve recovery. For customers using Direct Connect gateway and Transit Gateway, we recommend creating an AWS Site-to-Site VPN and attach it to your Transit Gateway. Instructions for how to do this failover can be found here: <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/\">https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/</a></div><div><span class=\"yellowfg\"> 9:06 PM PDT</span>&nbsp;復旧の兆しが確認できておりますが、引き続き事象の完全な解消に取り組んでおります。VPN を使用するワークアラウンドを実施いただいているお客様につきましては、完全な復旧のご連絡まではワークアラウンドを継続してご利用いただくことをお勧めいたします。| We are beginning to see signs of recovery, and continue to work toward full resolution. We suggest that customers that may have implemented the suggested workaround via VPN continue to use this workaround until we advise of full recovery.</div><div><span class=\"yellowfg\"> 9:51 PM PDT</span>&nbsp;日本時間 2021/09/02 07:30 から 13:42 の間、Direct Connect 接続を利用した AP-NORTHEAST-1 リージョン内の AWS サービスへの通信においてパケットロスの増加が発生しました。今回の事象は、 Direct Connect を利用したネットワークトラフィックを AP-NORTHEAST-1 リージョン内の全てのアベイラビリティーゾーンに接続するのに使用される複数のコアネットワークデバイスの問題に起因しておりました。現在問題は解消し、サービスは正常に稼働しています。\n| Between 3:30 PM and 9:42 PM PDT we experienced elevated packet loss for customers connecting to AWS services within AP-NORTHEAST-1 Region through their Direct Connect connections. This was caused by the loss of serveral core networking devices that are used to connect Direct Connect network traffic to all Availability Zones in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">Sep 6,  8:25 PM PDT</span>&nbsp;日本時間 9月2日 の Direct Connect のイベントに関する詳細情報を提供いたします。詳細については、以下をご参照ください。ご質問がある場合は、AWS サポートにお問い合わせください。<a href=\"https://aws.amazon.com/message/17908\">https://aws.amazon.com/message/17908</a> | We'd like to share more information about the Direct Connect event on Wednesday September 2nd. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/17908\">https://aws.amazon.com/message/17908</a></div>",
      "service": "directconnect-ap-northeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Hong Kong)",
      "summary": "[RESOLVED] API错误率的上升 | API的錯誤率上升 | Increased API error rates",
      "date": "1631281173",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:39 AM PDT</span>&nbsp;我们正在调查AP-EAST-1地区EC2 API错误率的上升，若有新的进展时会立即更新。| 我們正在調查AP-EAST-1地區EC2 API的錯誤率上升，若有新的進展時會立即更新。| We are investigating increased EC2 API error rates in the AP-EAST-1 Region. We will provide more information as it becomes available.</div><div><span class=\"yellowfg\"> 7:30 AM PDT</span>&nbsp;在 20:23 至 21:56 (UTC+8) 之间，我们在 AP-EAST-1 区域遇到了 API 和新实例启动错误率增加的情况。发生原因是子系统中的少数主机无法处理 API 请求。 在多数情况下，重试调用 API 之后可成功执行。 目前问题已解决，服务已恢复正常运行。| 在 20:23 至 21:56 (UTC+8) 之間，我們在 AP-EAST-1 區域遇到了 API 和新實例啟動錯誤率增加的情況。發生原因是子系統中的少數主機無法處理 API 請求。 在多數情況下，重試調用 API 之後可成功執行。 目前問題已解決，服務已恢復正常運行。| Between 5:23 AM and 6:56 AM PDT we experienced increased error rates for APIs and new instance launches in the AP-EAST-1 Region. The root cause was a small number of hosts in a subsystem that failed to process API requests. Retries of API calls would have succeeded in many cases. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Degraded EBS Volume Performance",
      "date": "1632712292",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:11 PM PDT</span>&nbsp;We are investigating degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. Some new EC2 instance launches, within the affected Availability Zone, are also impacted by this issue. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 8:41 PM PDT</span>&nbsp;We can confirm degraded performance for some EBS volumes within a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. Existing EC2 instances within the affected Availability Zone that use EBS volumes may also experience impairment due to stuck IO to the attached EBS volume(s). Newly launched EC2 instances within the affected Availability Zone may fail to launch due to the degraded volume performance. We continue to work toward determining root cause and mitigating impact but recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 9:17 PM PDT</span>&nbsp;We are making progress in determining the root cause and have isolated it to a subsystem within the EBS service. We are working through multiple steps to mitigate the issue and will continue to provide updates as we make progress. Other Availability Zones remain unaffected by this issue and affected EBS volumes and EC2 instances within the affected Availability Zone have plateaued at this stage. We continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\"> 9:47 PM PDT</span>&nbsp;We continue to make progress in determining the root cause of the issue causing degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. A subsystem within the larger EBS service that is responsible for coordinating storage hosts is currently degraded due to increased resource contention. We continue to work to understand the root cause of the elevated resource contention, but are actively working to mitigate the issue. Once mitigated, we expect performance for the affected EBS volumes to return to normal levels. We will continue to provide you with updates on our progress. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">10:23 PM PDT</span>&nbsp;We continue to make progress in determining the root cause of the issue causing degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. We have made several changes to address the increased resource contention within the subsystem responsible for coordinating storage hosts with the EBS service. While these changes have led to some improvement, we have not yet seen full recovery for the affected EBS volumes. We continue to expect full recovery of the affected EBS volumes once the subsystem issue has been addressed. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">11:19 PM PDT</span>&nbsp;We continue to work to address the increased resource contention within the EBS service subsystem responsible for coordinating EBS storage hosts. We have applied several mitigations, and while we have seen some improvements, we have not yet seen performance for affected volumes return to normal levels. We are currently rolling out another mitigation that we believe addresses the root cause and has shown promising signs in testing. We will know within the next 30-45 minutes as to whether this restores normal operations for current affected volumes. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">11:43 PM PDT</span>&nbsp;We can confirm that the deployed mitigation has worked and we have started to see recovery for some affected EBS volumes within the affected Availability Zone (USE1-AZ2). We are still finishing the deployment of the mitigation, but expect performance of affected EBS volumes in this single Availability Zone to return to normal levels over the next 60 minutes.</div><div><span class=\"yellowfg\">Sep 27, 12:30 AM PDT</span>&nbsp;We can confirm that the performance has returned to normal levels for the majority of the affected EBS volumes within the affected Availability Zone (USE1-AZ2). We continue to work towards full recovery for the remaining EBS volumes.</div><div><span class=\"yellowfg\">Sep 27,  1:15 AM PDT</span>&nbsp;We can confirm that the performance has returned to normal levels for the majority of the affected EBS volumes within the affected Availability Zone (USE1-AZ2). Starting at 12:12 AM PDT, we saw recovery slow down some affected EBS volumes as well as some degraded performance for a small number of additional volumes in the affected Availability Zone. We have investigated the root cause and mitigations are underway to complete the performance recovery of the affected EBS volumes.  We continue to work towards full resolution for all affected EBS volumes. In some cases, customers may be experiencing volume state transition delays, which we expect to clear up once volumes have fully recovered.</div><div><span class=\"yellowfg\">Sep 27,  2:26 AM PDT</span>&nbsp;We continue to apply mitigations to address the degraded performance for the smaller set of remaining EBS volumes affected by this issue. While the vast majority of affected EBS volumes were operating normally by 12:05 AM PDT, we have been working to recover a smaller set of affected volumes since 12:12 AM PDT. We continue to make progress on restoring performance for affected volumes, and expect full recovery to take another 2 hours. In some cases, customers may also be experiencing volume state transition delays, which will resolve when the underlying volume has fully recovered.</div><div><span class=\"yellowfg\">Sep 27,  3:36 AM PDT</span>&nbsp;We had restored performance for the vast majority of affected EBS volumes within the affected Availability Zone in the US-EAST-1 Region at 12:05 AM PDT and have been working to restore a remaining smaller set of EBS volumes. EC2 instances affected by this issue have now also recovered and new EC2 instance launches with attached EBS volumes have been succeeding since 1:30 AM PDT. Other services - including Redshift, OpenSearch, and Elasticache - are seeing recovery. Some RDS databases are still experiencing connectivity issues, but we’re working towards full recovery. We are in the process of restoring performance for the remaining small number of EBS volumes and EC2 instances that are still affected by this issue. </div><div><span class=\"yellowfg\">Sep 27,  4:21 AM PDT</span>&nbsp;Starting at 6:41 PM PDT on September 26th, we experienced degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. The issue was caused by increased resource contention within the EBS subsystem responsible for coordinating EBS storage hosts. Engineering worked to identify the root cause and resolve the issue within the affected subsystem. At 11:20 PM PDT, after deploying an update to the affected subsystem, IO performance for the affected EBS volumes began to return to normal levels. By 12:05 AM on September 27th, IO performance for the vast majority of affected EBS volumes in the USE1-AZ2 Availability Zone were operating normally. However, starting at 12:12 AM PDT, we saw recovery slow down for a smaller set of affected EBS volumes as well as seeing degraded performance for a small number of additional volumes in the USE1-AZ2 Availability Zone. Engineering investigated the root cause and put in place mitigations to restore performance for the smaller set of remaining affected EBS volumes. These mitigations slowly improved the performance for the remaining smaller set of affected EBS volumes, with full operations restored by 3:45 AM PDT. While almost all of EBS volumes have fully recovered, we continue to work on recovering a remaining small set of EBS volumes. We will communicate the recovery status of these volumes via the Personal Health Dashboard. While the majority of affected services have fully recovered, we continue to recover some services, including RDS databases and Elasticache clusters. We will also communicate the recovery status of these services via the Personal Health Dashboard. The issue has been fully resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Chime",
      "summary": "[RESOLVED] Increased contact search failures",
      "date": "1632832788",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:39 AM PDT</span>&nbsp;We are investigating increased latency in being able to search for contacts and start chime messages. We will provide more information as it becomes available</div><div><span class=\"yellowfg\"> 6:31 AM PDT</span>&nbsp;Between 2:55 AM and 6:10 AM PDT, we experienced increased error rates when searching for contacts and starting Chime messages. The issue is resolved and the service is operating normally.  </div>",
      "service": "chime"
    },
    {
      "service_name": "AWS Lambda (Ireland)",
      "summary": "[RESOLVED] Increased invoke timeouts",
      "date": "1633447955",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:32 AM PDT</span>&nbsp;We are investigating increased invoke timeout rates for AWS Lambda functions in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 9:15 AM PDT</span>&nbsp;We are still investigating increased invoke timeout rates for AWS Lambda functions in the EU-WEST-1 Region. A subset of functions making API calls to other AWS services within the region are affected.</div><div><span class=\"yellowfg\">10:04 AM PDT</span>&nbsp;We are still investigating increased invoke timeout rates and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. We have isolated the timeouts to network connectivity issues in a section of the AWS Lambda compute subsystem in the region, and are working through resolution and root cause. The issue affects a subset of functions making API calls to other AWS services within the region.</div><div><span class=\"yellowfg\">10:43 AM PDT</span>&nbsp;We can confirm increased invoke timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. The issue affects a subset of functions making API calls to other AWS services within the region. There are no identified workarounds for affected Lambda functions at this time. We continue to investigate a network connectivity issue in a section of the AWS Lambda compute subsystem. We will formulate a mitigation plan based on these findings as we work towards narrowing root cause and work towards resolution.</div><div><span class=\"yellowfg\">11:49 AM PDT</span>&nbsp;We continue to experience increased invoke timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. The issue affects a subset of functions making API calls to other AWS services within the region. At this point we have not identified any workarounds for affected Lambda functions, and we continue to investigate possible networking issues in a section of the AWS Lambda compute subsystem. We will make more information about our mitigation plan available as soon as we have more information to share.</div><div><span class=\"yellowfg\"> 2:03 PM PDT</span>&nbsp;We continue to investigate our subsystems to identify the root cause of timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. Impairment is limited to a small proportion of non-VPC Lambda functions that make outbound network connections. We will update information about workarounds and mitigation plans once we have more information to share.</div><div><span class=\"yellowfg\"> 3:12 PM PDT</span>&nbsp;Between October 4 9:46 AM and October 5 2:47 PM PDT, we experienced timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. Impairments were limited to a small proportion of non-VPC Lambda functions that made outbound network connections during this time. The issue has been resolved and the service is operating normally.   </div>",
      "service": "lambda-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Network Connectivity Issue",
      "date": "1633484779",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:46 PM PDT</span>&nbsp;We are investigating network connectivity for some instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have identified the root cause and are working to restore connectivity for the affected instances. Some EBS volumes are also experiencing degraded performance due to this issue.</div><div><span class=\"yellowfg\"> 7:13 PM PDT</span>&nbsp;We continue to work to mitigate the issue affecting connectivity to some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is a loss of power to a small number of EC2 instances and some networking devices within the affected Availability Zone. A small number of EC2 instances are also experiencing network connectivity issue as a result of the loss of power to the affected networking devices. We are working to restore power to the affected instances. If you are able to launch new EC2 instances, all APIs are operating normally with in the affected Availability Zone. We do not expect the issue to affect any other power line-ups within the affected Availability Zone or other Availability Zones within the Region.</div><div><span class=\"yellowfg\"> 7:34 PM PDT</span>&nbsp;We have restored power to the affected line-up and are seeing recovery for the majority of the affected EC2 instances and EBS volumes. We continue to work on the remaining EC2 instances and EBS volumes.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;Starting at 6:05 PM PDT some EC2 instances within a single Availability Zone experienced a loss of power. Other instances within the affected Availability Zone experienced connectivity issues and some EBS volumes experienced degraded performance. The root cause of the event was a loss of power to a single line-up within the affected Availability Zone. Power was restored to the affected line-up at 7:20 PM PDT and at 7:35 PM PDT, the affected EC2 instances and EBS volumes had recovered. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1633637388",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:09 PM PDT</span>&nbsp;PDT 시간으로 12:24 PM과 12:34 PM 사이에, AP-NORTHEAST-2 리전에서 API 에러율 증가가 발생했습니다. 이 이슈는 해결되었고 서비스는 현재 정상 동작 중입니다. 이슈가 영향을 미치는 중에는 API 실패가 발생했을 수 있고, 고객들이 API 호출들을 재시도 할 것을 권고합니다. 고객들이 서비스의 가용성을 복구하기 위한 추가적인 조치를 취할 필요는 없습니다. 이 기간 동안 다른 AWS 서비스들에서도 API 실패가 발생할 수 있었습니다. 만일 궁금한 점이나 서비스와 관련된 운영상의 이슈를 겪으신다면, AWS Support Center를 통해 AWS 기술지원팀에 문의해주십시오. https://console.aws.amazon.com/support  |  Between 12:24 PM and 12:34 PM PDT we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.  During the period of impact, API's may have failed and we recommend that customers retry these API calls.  Customers do not need to take additional measures to restore the availability of this service.  Additional AWS services may have seen API failures during this period.  If you have any questions or are experiencing any operational issues with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Elevated Error Rates for AWS Management Console ",
      "date": "1634052623",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:30 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:38 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:47 AM PDT</span>&nbsp;Between 8:10 AM and 8:39 AM PDT we experienced increased error rates and latency for the AWS Management Console. The issue has been resolved and the console is operating normally.  This issue did not affect any AWS service APIs, they were operating correctly during this time.</div>",
      "service": "management-console"
    },
    {
      "service_name": "AWS Directory Service (Sydney)",
      "summary": "[RESOLVED] Increased Authentication Error Rates ",
      "date": "1634086680",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:58 PM PDT</span>&nbsp;We are investigating authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Other AWS services, including Workspaces and Client VPN, are also affected by this event if they are configured to use AD Connector directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 7:04 PM PDT</span>&nbsp;We have identified the root cause of the authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. We are currently working on a fix to mitigate the issue. Other AWS services, including Workspaces, Client VPN and SSO, are also affected by this event if they are configured to use AD Connector directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 7:22 PM PDT</span>&nbsp;We have identified the root cause of the authentication errors on AD Connector Directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Our mitigation efforts are working as expected and customers should see resolution over the next 3 hours. We are currently working on steps to speed this up and will update resolution timings as we progress. Other AWS services, including Workspaces, Client VPN and SSO, are also affected by this event if they are configured to use AD Connector Directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;Our actions to expedite recovery have been successful and we now expect the fix to be completely deployed within the hour.</div><div><span class=\"yellowfg\"> 9:19 PM PDT</span>&nbsp;Between 1:00 PM and 9:15 PM PDT we experienced authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Other AWS services, including Workspaces, Client VPN and SSO were also affected by this event if they were configured to use AD Connector directories with MFA authentication enabled. The issue has been resolved and the service is operating normally.</div>",
      "service": "directoryservice-ap-southeast-2"
    },
    {
      "service_name": "AWS Internet Connectivity (US-West)",
      "summary": "[RESOLVED] Network Connectivity in US-GOV-WEST-1",
      "date": "1635874351",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:32 AM PDT</span>&nbsp;We are investigating connectivity issues in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">11:45 AM PDT</span>&nbsp;The issue is affecting network connectivity from the Internet to EC2 instances in a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region, between instances within this Availability Zone and between instances within this Availability Zone and other Availability Zones.</div><div><span class=\"yellowfg\">12:54 PM PDT</span>&nbsp;We have resolved the issue affecting Internet connectivity to a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-gov-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (US-West)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1635875642",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:54 AM PDT</span>&nbsp;Network Connectivity\nWe're investigating network connectivity issues for instances within a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region. We are also seeing increased error rates and latencies for the EC2 APIs within the region and are working to solve the issue.</div><div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue is affecting network connectivity from the Internet to instances in the affected Availability Zone, between instances within the affected Availability Zone and between instances within the affected Availability Zone and other Availability Zones. The EC2 APIs are also experiencing increased error rates and latencies within the US-GOV-WEST-1 Region, which is also affecting the AWS Management Console. We have made some progress in mitigating the impact for other AWS services, such as connectivity to Amazon S3, but continue to work on resolving the issue.</div><div><span class=\"yellowfg\">12:09 PM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. We have identified the root cause of the issue and are now focused on mitigating the issue. We are seeing some recovery in the error rates and latencies for the EC2 APIs and launches of new instances are once again working within the region. For recovery at this stage, we recommend focusing on shifting workloads and traffic away from the affected Availability Zone (USGW-AZ3).</div><div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;We are seeing recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. Once networking was restored to the affected Availability Zone, affected AWS services are also recovering. While the majority are seeing full recovery, we continue to work on the networking-related EC2 APIs, which are still seeing errors at this stage.</div><div><span class=\"yellowfg\"> 1:10 PM PDT</span>&nbsp;We have seen recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The increased error rates and latencies for the EC2 APIs have also recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-gov-west-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (US-West)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1635879208",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:53 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;Between 10:15 AM and 12:53 PM PDT we experienced increased API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-us-gov-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Sao Paulo)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1636421771",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:36 PM PST</span>&nbsp;We are investigating internet connectivity issues within the SA-EAST-1 Region. This is affecting connectivity into the SA-EAST-1 Region but also causing increased error rates and latencies for AWS service APIs within the region.</div><div><span class=\"yellowfg\"> 5:51 PM PST</span>&nbsp;Between 5:15 PM and 5:41 PM PST we experienced Internet connectivity issues for a single Availability Zone (sae1-az1) within the SA-EAST-1 Region. Some AWS services also experienced increased error rates and latencies during this time. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-sa-east-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1636495106",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:58 PM PST</span>&nbsp;We are investigating increased 5xx error rates and latencies for requests to the Amazon S3 APIs in the US-EAST-1 Region. Where possible, we recommend that requests that fail with a 5xx error be retried.</div><div><span class=\"yellowfg\"> 2:48 PM PST</span>&nbsp;We have identified the S3 subsystem responsible for increased 5xx error rates for the S3 PUT APIs, and are working to isolate the root cause within this subsystem. Customers may also be experiencing increased latency when performing PUT operations. During this time, we recommend customers retry any failed requests.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;We are continuing to see increased 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. We have narrowed down the root cause to a specific sub-system within S3 and continue to make progress in mitigating the impact to this service but have not yet seen significant improvement. S3 API error rates and latencies have stayed a consistent low level with the vast majority of request retries succeeding. While the vast majority of requests are being processed within the normal latency levels, request tail latencies are exceeding 1 second in some cases. In some applications, increasing client timeouts may also help to mitigate the issue. </div><div><span class=\"yellowfg\"> 4:52 PM PST</span>&nbsp;We are starting to see some improvement in the 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. The issue affected a subsystem that stores routing metadata used by Amazon S3 to map API requests to the storage nodes. A recent update caused increased load within this subsystem, which led to increased error rates and latencies for the S3 APIs. We have now successfully mitigated this increased load within this subsystem and are seeing early signs of recovery. As the sub-system processes the backlog of requests, S3 API error rates and latencies will continue to improve.</div><div><span class=\"yellowfg\"> 5:53 PM PST</span>&nbsp;We continue to see a gradual improvement in error rates as we process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We are currently working on mitigations to speed up the processing of the backlog during this event. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally. </div><div><span class=\"yellowfg\"> 6:58 PM PST</span>&nbsp;We continue to process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We have implemented two parallel mitigations to improve the speed of processing. Both mitigations are in process of deployment. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally.</div><div><span class=\"yellowfg\"> 7:51 PM PST</span>&nbsp;We have completed the mitigation to accelerate processing of the mappings between S3 API request metadata and storage. The backlog has been fully processed and S3 API errors and latencies have returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-standard"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift cluster reboot and degraded performance",
      "date": "1636496591",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:23 PM PST</span>&nbsp;We are investigating cluster reboots and degraded performance for Redshift Clusters in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:47 PM PST</span>&nbsp;We continue to investigate cluster reboots and degraded performance for Redshift RA3 Clusters in the US-EAST-1 Region. The cluster reboots are being triggered by writes that are being impacted by the elevated S3 put latencies. Redshift attempts to retry these writes automatically but if they are unsuccessful after extended attempts, the cluster may restart. If you are able to pause write workloads while we work towards resolving the S3 put latency issue, your clusters will no longer restart and can serve read queries normally.</div><div><span class=\"yellowfg\"> 4:53 PM PST</span>&nbsp;We have identified and continue to work on mitigating the root cause of the Redshift RA3 Clusters reboots and degraded performance in the US-EAST-1 Region. The cluster reboots are triggered by Redshift writes to S3 that have been impacted by the elevated S3 PUT API latencies. If you are impacted and able to pause Redshift write workloads on your RA3 clusters while we work towards recovery, your clusters will no longer restart and can serve read queries normally. As S3 API error rates and latencies continue to improve, we expect Redshift RA3 cluster restarts to decline as well.</div><div><span class=\"yellowfg\"> 7:19 PM PST</span>&nbsp;While the S3 API error rates and latencies continue to hold steady, we continue to see a low rate of Redshift RA3 Cluster restarts. While we continue to take steps to mitigate and reduce the risk of a restart for affected Redshift clusters, we expect to see full recovery when the S3 error rates and latencies have fully recovered. Please refer to the S3 Service Health Dashboard updates for progress towards recovery.</div><div><span class=\"yellowfg\"> 7:47 PM PST</span>&nbsp;As S3 has completed their mitigation and is operating normally, Redshift RA3 clusters are seeing writes succeed and clusters are no longer rebooting. For the vast majority of customers the issue has been resolved and the service is operating normally. We continue to work with a small number of impacted customers individually to recover their clusters and will reach out via the AWS Personal Health Dashboard and AWS Support.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Stockholm)",
      "summary": "[RESOLVED] Increased EC2 Console Error Rates",
      "date": "1637596278",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:51 AM PST</span>&nbsp;We are investigating increased error rates within the EC2 Management Console in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are starting to see recovery for the issue causing increased error rates in the EC2 Console in the EU-NORTH-1 Region. We expect to see full recovery shortly. In the meantime, the EC2 APIs and EC2 CLI remain unaffected by the issue.</div><div><span class=\"yellowfg\"> 8:38 AM PST</span>&nbsp;We have resolved the issue causing increased error rates for the EC2 Management Console in the EU-NORTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-north-1"
    },
    {
      "service_name": "Amazon EventBridge (Cape Town)",
      "summary": "[RESOLVED] Event Delivery Delays",
      "date": "1637641154",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating increased event delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:13 PM PST</span>&nbsp;We continue to investigate increased event delivery delays in AF-SOUTH-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:26 PM PST</span>&nbsp;We can confirm elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken initial mitigation actions based on our investigations and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23, 12:04 AM PST</span>&nbsp;We continue working to identify the root cause of elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken further mitigation actions which have reduced the delivery latency and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23,  1:59 AM PST</span>&nbsp;Between November 22 7:30 PM and November 23 1:32 AM PST, we experienced elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "events-af-south-1"
    },
    {
      "service_name": "Amazon CloudWatch (Cape Town)",
      "summary": "[RESOLVED] Metric Stream Delivery Delays ",
      "date": "1637641175",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating CloudWatch Metric Streams delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:09 PM PST</span>&nbsp;We can confirm elevated API error rates for CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. We are actively working to resolve the issue. Customers may experience issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams is not affected.</div><div><span class=\"yellowfg\"> 9:42 PM PST</span>&nbsp;Between 7:30 PM and 9:25 PM PST, we experienced elevated error rates when calling CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. Customers may have experienced issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudwatch-af-south-1"
    },
    {
      "service_name": "AWS Lambda (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1637779569",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:46 AM PST</span>&nbsp;We are investigating increased invoke error rates in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:10 AM PST</span>&nbsp;We continue to investigate increased function invoke error rates within the US-EAST-2 Region. We are working to determine the root cause of the issue and will continue to provide updates as we make progress in resolving the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">11:34 AM PST</span>&nbsp;We have identified the root cause within Lambda that is causing the increased error rates for function invocations within the US-EAST-2 Region. The subsystem responsible for executing functions is currently impaired and we are working to resolve it. We have seen some improvement in Lambda function invocation error rates and believe that this will continue as we take steps to resolve the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:27 AM and 12:03 PM PST we experienced increased Lambda invoke error rates in the US-EAST-2 Region. It may take some additional time to process backlogged async invoke traffic that accumulated during this period. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-2"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1637780142",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:55 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console is the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:46 AM PST</span>&nbsp;We are seeing some recovery in error rates and latencies for the AWS Management Console in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:13 PM PST</span>&nbsp;Between 10:26 AM and 12:05 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon API Gateway (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1637780385",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We are investigating increased API errors for API Gateway in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:24 AM PST</span>&nbsp;We have confirmed that the elevated API error rate is restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs).</div><div><span class=\"yellowfg\">12:20 PM PST</span>&nbsp;Between 10:27 AM and 12:06 PM PST, we experienced elevated API error rate in the US-EAST-2 Region, restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs). The issue has been resolved, and the service is operating normally. </div>",
      "service": "apigateway-us-east-2"
    },
    {
      "service_name": "AWS Batch (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates and Scaling Delays",
      "date": "1637781298",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:14 AM PST</span>&nbsp;We are investigating increased API error rates and delays in scaling of some AWS Batch Compute Environments in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:25 AM and 12:00 PM PST we experienced increased error rates for all AWS Batch APIs, as well as some Compute Environment scaling delays, in the US-EAST-2 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "batch-us-east-2"
    },
    {
      "service_name": "Amazon Simple Notification Service (Oregon)",
      "summary": "[RESOLVED] Increased Latency and Error Rates",
      "date": "1638021360",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:56 AM PST</span>&nbsp;We are investigating increased latency and error rates for API calls in the US-WEST-2 Region. We will provide more information as we continue to investigate.</div><div><span class=\"yellowfg\"> 6:12 AM PST</span>&nbsp;We are starting to see improved SNS API success rates and latency in the US-WEST-2 Region, and are working towards full recovery.</div><div><span class=\"yellowfg\"> 6:42 AM PST</span>&nbsp;Between 5:05 AM and 5:55 AM PST, we experienced increased SNS API latency and error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-us-west-2"
    },
    {
      "service_name": "Amazon CloudWatch (Ireland)",
      "summary": "[RESOLVED] Delayed CloudWatch Metrics",
      "date": "1638042899",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:55 AM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters.  We are are working towards resolution.</div><div><span class=\"yellowfg\">12:49 PM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters. We have isolated the likely root cause to a subsystem that saw an unexpected jump in resource consumption.  We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 1:52 PM PST</span>&nbsp;We have implemented a fix to address the CloudWatch log event processing delays in the EU-WEST-1 Region and are starting to see signs of recovery. We will provide an update once full recovery has been observed.</div><div><span class=\"yellowfg\"> 2:56 PM PST</span>&nbsp;Between 10:26 AM and 02:40 PM PST, we experienced increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. This was due to a subsystem that saw an unexpected jump in resource consumption. The issue has been resolved and the service is operating normally. New events are processing as normal, while we work through the message backlog. We expect to completely drain the backlog over the next 1 hour.</div>",
      "service": "cloudwatch-eu-west-1"
    },
    {
      "service_name": "AWS CloudShell (N. Virginia)",
      "summary": "[RESOLVED] Increased Latencies and Failure Rates",
      "date": "1638507226",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:53 PM PST</span>&nbsp;We are experiencing increased latencies and failures in launching CloudShell environments in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:26 PM PST</span>&nbsp;Between 7:05 PM and 9:15 PM PST we experienced increased latencies and failures launching CloudShell environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudshell-us-east-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1638894176",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:26 AM PST</span>&nbsp;We are experiencing API and console issues in the US-EAST-1 Region.  We have identified root cause and we are actively working towards recovery.  This issue is affecting the global console landing page, which is also hosted in US-EAST-1.  Customers may be able to access region-specific consoles going to https://console.aws.amazon.com/.  So, to access the US-WEST-2 console, try https://us-west-2.console.aws.amazon.com/</div><div><span class=\"yellowfg\"> 4:25 PM PST</span>&nbsp;We are seeing improvements in the error rates and latencies in the AWS Management Console in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:14 PM PST</span>&nbsp;Between 7:32 AM to 4:56 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1638895771",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:49 AM PST</span>&nbsp;We are experiencing elevated error rates for EC2 APIs in the US-EAST-1 region.  We have identified root cause and we are actively working towards recovery.</div><div><span class=\"yellowfg\"> 3:31 PM PST</span>&nbsp;Between 7:32 AM and 3:10 PM PST we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Contact Handling",
      "date": "1638896019",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:53 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:08 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 9:18 AM PST</span>&nbsp;We can confirm degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 4:47 PM PST</span>&nbsp;We are seeing improvements to contact handling in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:10 PM PST</span>&nbsp;Between 7:25 AM PST and 4:47 PM PST we experienced degraded Contact handling, increased user login errors, and increased API error rates in the US-EAST-1 Region. During this time, end-customers may have experienced delays or errors when placing a call or starting a chat, and agents may have experienced issues logging in or being connected with end-customers. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1638896271",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:57 AM PST</span>&nbsp;We are currently investigating increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:40 PM PST</span>&nbsp;Between 7:40 AM and 2:25 PM PST, we experienced increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region. Data plane operations were not impacted. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "dynamodb-us-east-1"
    },
    {
      "service_name": "AWS Support Center",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1638896490",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:01 AM PST</span>&nbsp;We are investigating increased error rates for the Support Center console and Support API in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:55 PM PST</span>&nbsp;We continue to see increased error rates for the Support Center console and Support API in the US-EAST-1 Region. Support Cases successfully created via the console or the API may not be successfully routed to Support Engineers. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Between 7:33 AM and 2:25 PM PST, we experienced increased error rates for the Support Center console and Support API in the US-EAST-1 Region. This resulted in errors in creating support cases and delays in routing cases to Support Engineers. The issue has been resolved and our Support Engineering team is responding to cases. The service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "supportcenter"
    },
    {
      "service_name": "Amazon EventBridge (N. Virginia)",
      "summary": "[RESOLVED] Event Delivery Delays",
      "date": "1638917670",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:54 PM PST</span>&nbsp;We have temporarily disabled event deliveries in the US-EAST-1 Region. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will not trigger targets. These events will still be received by EventBridge and will deliver once we recover.</div><div><span class=\"yellowfg\"> 3:00 PM PST</span>&nbsp;We have re-enabled event deliveries in the US-EAST-1 Region, but are experiencing event delivery latencies. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will be delayed.</div><div><span class=\"yellowfg\"> 4:31 PM PST</span>&nbsp;We continue to see event delivery latencies in the US-EAST-1 region. We have identified the root cause and are working toward recovery.</div><div><span class=\"yellowfg\"> 6:00 PM PST</span>&nbsp;Event delivery latency for new events in the US-EAST-1 Region have returned to normal levels. We continue to process a backlog of events.</div><div><span class=\"yellowfg\"> 9:21 PM PST</span>&nbsp;Between 7:30 AM and 8:40 PM PST we experienced elevated event delivery latency in the US-EAST-1 Region. Event delivery latencies have returned to normal levels. Some CloudTrail events for API calls between 7:35 AM and 6:05 PM PST may be delayed but will be delivered in the coming hours.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "events-us-east-1"
    },
    {
      "service_name": "Amazon API Gateway (N. Virginia)",
      "summary": "[RESOLVED] Elevated Errors and Latencies",
      "date": "1638919396",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:23 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 4:05 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are continuing to work towards resolution.</div><div><span class=\"yellowfg\"> 4:41 PM PST</span>&nbsp;We have seen improvement in error rates and latencies for invokes in the US-EAST-1 region. We continue to drive towards full recovery.</div><div><span class=\"yellowfg\"> 5:23 PM PST</span>&nbsp;Between 9:02 AM and 5:01 PM PST we experienced increased error rates and latencies for invokes in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "apigateway-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated Fargate task launch failures",
      "date": "1638919976",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:32 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day, but we are still investigating task launch failures using the Fargate launch type. Task launches using the EC2 launch type are not impacted.</div><div><span class=\"yellowfg\"> 4:44 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are working towards recovery.</div><div><span class=\"yellowfg\"> 5:31 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are starting to see recovery. As we work towards full recovery, customers may experience insufficient capacity errors and these are being addressed as well.</div><div><span class=\"yellowfg\"> 7:30 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">11:01 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this and have recently seen a decrease in these errors while continuing to work towards full recovery. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">Dec 8,  2:29 AM PST</span>&nbsp;Between 7:31 AM PST on December 7 and 2:20 AM PST on December 8, ECS experienced increased API error rates, latencies, and task launch failures. API error rates and latencies recovered by 6:10 PM PST on December 7. After this point, ECS customers using the EC2 launch type were fully recovered. ECS customers using the Fargate launch type along with EKS customers using Fargate continued to see decreasing impact in the form of insufficient capacity errors between 4:40 PM PST on December 7 and 2:20 AM on December 8. The service is now operating normally. A small set of customers may still experience low levels of insufficient capacity errors and will be notified using the Personal Health Dashboard in that case. There was no impact to running tasks during the event although any ECS task that failed health checks would have been stopped because of that failing health check.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "AWS Batch (N. Virginia)",
      "summary": "[RESOLVED] Increased Job Processing Delays",
      "date": "1638922058",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:07 PM PST</span>&nbsp;We have identified the root cause of increased delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 5:20 PM PST</span>&nbsp;We have seen improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 8:02 PM PST</span>&nbsp;Improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region is accelerating, we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:29 PM PST</span>&nbsp;Between 7:35 AM and 8:13 PM PST, we experienced increase job state transition delays of AWS Batch Jobs in the US-EAST-1 Region.  The issue has been resolved and the service is now operating normally for new job submissions.  Jobs that were delayed from earlier in the event will be processed in order until we clear the queue.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "batch-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639055699",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:15 AM PST</span>&nbsp;We are investigating elevated error rates for the Redshift Management Console in the US-EAST-1 region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:34 AM PST</span>&nbsp;We continue to investigate elevated error rates for the Redshift Management Console in the US-EAST-1 region. We have identified the issue causing elevated error rates and are actively working to resolve the issue. Customer clusters remain available and are operating normally. Customers can use API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries. </div><div><span class=\"yellowfg\"> 6:30 AM PST</span>&nbsp;We have identified the root cause of the issue causing elevated error rates and are in the process of deploying a fix that will resolve the issue. We do not have a precise ETA for the deployment to complete that we can share at this time. Customer clusters remain available and are operating normally. Customers can use the API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries.</div><div><span class=\"yellowfg\"> 6:58 AM PST</span>&nbsp;Between 12:00 AM and 6:25 AM PST we saw elevated error rates for the Amazon Redshift Management Console in the US-EAST-1 region. This initial error rate increase was observed at 12:00 AM PST and at 4:05 AM PST this error rate increased further. Customer clusters were operating normally throughout and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. We have completed the deployment of a fix. The issue has been resolved and the Management Console and the Amazon Redshift service are operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639067113",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:25 AM PST</span>&nbsp;Earlier today we reported elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. At 6:58 AM PST we reported the issue was resolved but we have since detected that some errors persist despite much lower rates. Redshift clusters remain available and customers can manage their clusters using the API and CLI. Customers can also execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:05 AM PST</span>&nbsp;We continue to investigate intermittent elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. Customers will be able to get the console to load if they refresh their browser tab several times. Once the console has loaded, the console will work as expected and customers will be able to execute queries normally. Redshift clusters remain available and customers can manage their clusters using the API and CLI and can execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;Between 7:25 AM and 10:05 AM PST we experienced increased error rates for the Amazon Redshift Management Console in the US-EAST-1 Region. During this time, clusters were operating normally  and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. The issue is resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Oregon)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639582979",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:43 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-2 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:14 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-2 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.\n</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-2"
    },
    {
      "service_name": "AWS Internet Connectivity (N. California)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583545",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:52 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (US-West)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583722",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:55 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:00 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-GOV-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-GOV-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:16 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-gov-west-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "[RESOLVED] Console Application Upload Errors",
      "date": "1640158400",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:33 PM PST</span>&nbsp;We are investigating an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. Customers who need to update or deploy a new application version should do so using the AWS CLI. Existing applications are not impacted by this issue</div><div><span class=\"yellowfg\">Dec 22, 12:34 AM PST</span>&nbsp;We continue to investigate an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. We are determining the root causes and working through steps to mitigate the issue. Customers who need to update or deploy a new application version should do so using the AWS CLI while we work towards resolving the issue. Existing applications are not impacted by this issue.</div><div><span class=\"yellowfg\">Dec 22,  1:20 AM PST</span>&nbsp;We have identified the root cause and prepared a fix to address the issue that prevents customers from uploading new application versions through the Elastic Beanstalk console in multiple Regions. The service team is testing this fix and preparing for deployment to the Regions that are affected by this issue. We expect to see full recovery by 3:00 AM PST and will continue to keep you updated if this ETA changes. Customers who need to update or deploy a new application version should do so using the AWS CLI until the issue is fully resolved.</div><div><span class=\"yellowfg\">Dec 22,  3:21 AM PST</span>&nbsp;Between December 21, 2021 at 6:37 PM  and December 22, 2021 at 03:17 AM PST, customers were unable to upload their code through the Elastic Beanstalk console due to a Content Security Policy (CSP) error. Customers were impacted when they attempted to upload a new application version for existing environments or upload their code when creating a new environment in multiple regions. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] API Error Rates",
      "date": "1640176551",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:35 AM PST</span>&nbsp;We are investigating increased EC2 launch failures and networking connectivity issues for some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 5:01 AM PST</span>&nbsp;We can confirm a loss of power within a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. This is affecting availability and connectivity to EC2 instances that are part of the affected data center within the affected Availability Zone. We are also experiencing elevated RunInstance API error rates for launches within the affected Availability Zone. Connectivity and power to other data centers within the affected Availability Zone, or other Availability Zones within the US-EAST-1 Region are not affected by this issue, but we would recommend failing away from the affected Availability Zone (USE1-AZ4) if you are able to do so. We continue to work to address the issue and restore power within the affected data center.</div><div><span class=\"yellowfg\"> 5:18 AM PST</span>&nbsp;We continue to make progress in restoring power to the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have now restored power to the majority of instances and networking devices within the affected data center and are starting to see some early signs of recovery. Customers experiencing connectivity or instance availability issues within the affected Availability Zone, should start to see some recovery as power is restored to the affected data center. RunInstances API error rates are returning to normal levels and we are working to recover affected EC2 instances and EBS volumes. While we would expect continued improvement over the coming hour, we would still recommend failing away from the Availability Zone if you are able to do so to mitigate this issue.</div><div><span class=\"yellowfg\"> 5:39 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. Network connectivity within the affected Availability Zone has also returned to normal levels. While all services are starting to see meaningful recovery, services which were hosting endpoints within the affected data center - such as single-AZ RDS databases, ElastiCache, etc. - would have seen impact during the event, but are starting to see recovery now. Given the level of recovery, if you have not yet failed away from the affected Availability Zone, you should be starting to see recovery at this stage. </div><div><span class=\"yellowfg\"> 6:13 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. We continue to make progress in recovering the remaining EC2 instances and EBS volumes within the affected Availability Zone. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 6:51 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. For the remaining EC2 instances, we are experiencing some network connectivity issues, which is slowing down full recovery. We believe we understand why this is the case and are working on a resolution. Once resolved, we expect to see faster recovery for the remaining EC2 instances and EBS volumes. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 8:02 AM PST</span>&nbsp;Power continues to be stable within the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have been working to resolve the connectivity issues that the remaining EC2 instances and EBS volumes are experiencing in the affected data center, which is part of a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have addressed the connectivity issue for the affected EBS volumes, which are now starting to see further recovery. We continue to work on mitigating the networking impact for EC2 instances within the affected data center, and expect to see further recovery there starting in the next 30 minutes. Since the EC2 APIs have been healthy for some time within the affected Availability Zone, the fastest path to recovery now would be to relaunch affected EC2 instances within the affected Availability Zone or other Availability Zones within the region.</div><div><span class=\"yellowfg\"> 9:28 AM PST</span>&nbsp;We continue to make progress in restoring connectivity to the remaining EC2 instances and EBS volumes. In the last hour, we have restored underlying connectivity to the majority of the remaining EC2 instance and EBS volumes, but are now working through full recovery at the host level. The majority of affected AWS services remain in recovery and we have seen recovery for the majority of single-AZ RDS databases that were affected by the event. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">11:08 AM PST</span>&nbsp;We continue to make progress in restoring power and connectivity to the remaining EC2 instances and EBS volumes, although recovery of the remaining instances and volumes is taking longer than expected. We believe this is related to the way in which the data center lost power, which has led to failures in the underlying hardware that we are working to recover. While EC2 instances and EBS volumes that have recovered continue to operate normally within the affected data center, we are working to replace hardware components for the recovery of the remaining EC2 instances and EBS volumes. We have multiple engineers working on the underlying hardware failures and expect to see recovery over the next few hours. As is often the case with a loss of power, there may be some hardware that is not recoverable, and so we continue to recommend that you relaunch your EC2 instance, or recreate you EBS volume from a snapshot, if you are able to do so.</div><div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;Over the last hour, after addressing many of the underlying hardware failures, we have seen an accelerated rate of recovery for the affected EC2 instances and EBS volumes. We continue to work on addressing the underlying hardware failures that are preventing the remaining EC2 instances and EBS volumes. For customers that continue to have EC2 instance or EBS volume impairments, relaunching affected EC2 instances or recreating affecting EBS volumes within the affected Availability Zone, continues to be a faster path to full recovery. </div><div><span class=\"yellowfg\"> 1:39 PM PST</span>&nbsp;We continue to make progress in addressing the hardware failures that are delaying recovery of the remaining EC2 instances and EBS volumes. At this stage, if you are still waiting for an EC2 instance or EBS volume to fully recover, we would strongly recommend that you consider relaunching the EC2 instance or recreating the EBS volume from a snapshot. As is often the case with a loss of power, there may be some hardware that is not recoverable, which will prevent us from fully recovering the affected EC2 instances and EBS volumes. We are not quite at that point yet in terms of recovery, but it is unlikely that we will recover all of the small number of remaining EC2 instances and EBS volumes. If you need help in launching new EC2 instances or recreating EBS volumes, please reach out to AWS Support.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Since the last update, we have more than halved the number of affected EC2 instances and EBS volumes and continue to work on the remaining EC2 instances and EBS volumes. The remaining EC2 instances and EBS volumes have all experienced underlying hardware failures due to the nature of the initial power event, which we are working to resolve. We expect to make further progress on this list within the next hour, but some of the remaining EC2 instances and EBS volumes may not be recoverable due to hardware failures. If you have the ability to relaunch an affected EC2 instance or recreate an affected EBS volume from snapshot, we continue to strongly recommend that you take that path.</div><div><span class=\"yellowfg\"> 4:22 PM PST</span>&nbsp;Starting at 4:11 AM PST some EC2 instances and EBS volumes experienced a loss of power in a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Instances in other data centers within the affected Availability Zone, and other Availability Zones within the US-EAST-1 Region were not affected by this event. At 4:55 AM PST, power was restored to EC2 instances and EBS volumes in the affected data center, which allowed the majority of EC2 instances and EBS volumes to recover. However, due to the nature of the power event, some of the underlying hardware experienced failures, which needed to be resolved by engineers within the facility. Engineers worked to recover the remaining EC2 instances and EBS volumes affected by the issue. By 2:30 PM PST, we recovered the vast majority of EC2 instances and EBS volumes. However, some of the affected EC2 instances and EBS volumes were running on hardware that has been affected by the loss of power and is not recoverable. For customers still waiting for recovery of a specific EC2 instance or EBS volume, we recommend that you relaunch the instance or recreate the volume from a snapshot for full recovery. If you need further assistance, please contact AWS Support.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640193970",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:26 AM PST</span>&nbsp;We are investigating increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">10:49 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Some customers may begin to see signs of recovery.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">11:56 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">12:10 PM PST</span>&nbsp;As the root cause of this impact is related to Directory Services, we will continue to provide updates on the new post we have just created for Directory Service in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:56 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. The issue has been resolved and the service is operating normally.  If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Directory Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640203590",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:06 PM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 2:29 PM PST</span>&nbsp;We continue to resolve increased error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication.  We are prioritizing the most impacted directories to expedite resolution.  Additional customers will see recovery as resolution takes place.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;Our mitigation efforts are working as expected and we are making steady progress toward recovery of error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication. We continue to prioritize the most impacted directories to expedite resolution. Additional customers will see recovery as resolution takes place. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 5:57 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Directory Services in US-EAST-1 Region. This also impacted some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. The issue has been resolved and the service is operating normally. Customers using other Active Directory functionality were not impacted by this issue. If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "directoryservice-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Mumbai)",
      "summary": "[RESOLVED] Internet connectivity",
      "date": "1640371277",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:41 AM PST</span>&nbsp;Between 8:59 AM and 9:32 AM PST and between 9:40 AM and 10:16 AM PST we observed Internet connectivity issues with a network provider outside of our network in the AP-SOUTH-1 Region. This impacted Internet connectivity from some customer networks to the AP-SOUTH-1 Region. Connectivity between EC2 instances and other AWS services within the Region was not impacted by this event. The issue has been resolved and we continue to work with the external provider to ensure it does not reoccur.\n</div>",
      "service": "internetconnectivity-ap-south-1"
    },
    {
      "service_name": "Amazon Pinpoint (N. Virginia)",
      "summary": "[RESOLVED] Pinpoint Sending/Receiving Delays",
      "date": "1642185596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:39 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of Pinpoint customers sending and receiving SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">10:53 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:42 AM PST</span>&nbsp;We can confirm that the sending and receiving of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:43 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while sending and receiving SMS messages using US toll-free numbers. Starting at 5:14 AM SMS message delivery receipts were delayed. These delays will continue while we work with our downstream partners through the backlog of delayed delivery receipts.  The issues have been resolved and the service is operating normally.</div>",
      "service": "pinpoint-us-east-1"
    },
    {
      "service_name": "Amazon Simple Notification Service (N. Virginia)",
      "summary": "[RESOLVED] SMS Delivery Delays",
      "date": "1642186756",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of SNS and Pinpoint customers delivering SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">11:00 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We can confirm that the delivery of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:44 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while delivering SMS messages using US toll-free numbers. Also starting at 5:14 AM, SMS message delivery receipts were delayed, which created a backlog of undelivered delivery receipts. We are continuing to work with our downstream partners to clear this backlog. Receipts for new SMS deliveries will also be delayed until this backlog clears. The issues have been resolved and the service is operating normally.</div>",
      "service": "sns-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Increased API Error Rates ",
      "date": "1642432540",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:15 AM PST</span>&nbsp;메시지: AP-NORTHEAST-2 리전에서 API 오류율 증가에 대해 조사하고 있습니다. | We are investigating increased API error rates in the ap-northeast-2 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PST</span>&nbsp;태평양 표준시(PST) 오전 6시 55분부터 오전 7시 40분 사이에 AP-NORTHEAST-2 리전에서 API 오류율이 증가했습니다. 현재 문제가 해결되었으며 서비스가 정상적으로 작동하고 있습니다. 궁금한 점이 있거나 서비스와 관련하여 운영상의 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 AWS 지원 부서에 문의 부탁 드립니다. | Between 6:55 AM and 7:40 AM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS Internet Connectivity (Seoul)",
      "summary": "[RESOLVED] 네트워크 연결 | Network Connectivity",
      "date": "1644150596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:29 AM PST</span>&nbsp;English follows Korean | 한국어버전 뒤에 영어버전이 있습니다\n\n태평양 표준시 (PST) 기준 오전 3시 32분에서 오전 3시 54분사이에 AP-NORTHEAST-2 리전의 단일 가용 영역(apne2-az3)에 있는 일부 EC2 인스턴스의 인터넷 연결에 영향을 미치는 연결 문제가 발생했습니다. 리전 내의 인스턴스 및 서비스에 대한 연결은 영향을 받지 않았습니다. 현재 문제가 해결되었고 연결이 복원되었습니다. 현재 이 문제를 해결하기 위해 필요한 조치는 없습니다. 궁금하신 점이 있으시거나 서비스 운영에 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 문의하시기 바랍니다.\n\nBetween 3:32 AM and 3.54 AM PST we experienced connectivity issues affecting Internet connectivity for some EC2 instances in a single Availability Zone (apne2-az3) in AP-NORTHEAST-2 Region. Connectivity to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored. No action is currently required to address this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "internetconnectivity-ap-northeast-2"
    },
    {
      "service_name": "Amazon Virtual Private Cloud (Frankfurt)",
      "summary": "[RESOLVED] Elevated Error Rates for VPC Console",
      "date": "1646295847",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:24 AM PST</span>&nbsp;We are investigating elevated error rates for the VPC Management Console in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">12:51 AM PST</span>&nbsp;We have identified the root cause and are starting to see recovery of the VPC Management Console in the EU-CENTRAL-1 Region. We recommend signing out and signing back into your account to refresh your session. We continue working towards full recovery and will continue to keep you updated.</div><div><span class=\"yellowfg\"> 1:06 AM PST</span>&nbsp;Between March 2 10:03 PM and March 3 12:30 AM PST, the VPC Management Console experienced elevated error rates in the EU-CENTRAL-1 Region. API and CLI access were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "vpc-eu-central-1"
    },
    {
      "service_name": "AWS Lambda (US-West)",
      "summary": "[RESOLVED] Increased API and Invoke Error Rates",
      "date": "1646856563",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:09 PM PST</span>&nbsp;We are investigating increased invoke and API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">12:37 PM PST</span>&nbsp;We can confirm increased invoke and API error rates in the US-GOV-WEST-1 Region. We have deployed a mitigation strategy and continue to work through full resolution.</div><div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;Between 11:07 AM and 12:50 PM PST, we experienced increased invoke and API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-gov-west-1"
    },
    {
      "service_name": "Multiple services (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1646859683",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.\n</div><div><span class=\"yellowfg\"> 1:16 PM PST</span>&nbsp;We are seeing recovery in API error rates for all services.</div><div><span class=\"yellowfg\"> 1:29 PM PST</span>&nbsp;Between 12:43 PM and 12:59 PM PST, we experienced increased error rates and latencies for some AWS services within the US-EAST-1 Region. All services are now operating normally, but S3 Event Notifications continues to process a backlog of events that developed during the event. \n\nThe root cause of this issue was an update to the SQS and Lambda endpoints that inadvertently prevented some traffic from reaching these endpoints.</div><div><span class=\"yellowfg\"> 1:45 PM PST</span>&nbsp;S3 Event Notifications have delivered the backlog of events. This issue is resolved and all services are now operating normally.</div>",
      "service": "multipleservices-us-east-1"
    },
    {
      "service_name": "AWS DataSync (N. Virginia)",
      "summary": "[RESOLVED] Elevated error rates",
      "date": "1647437391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We are investigating elevated error rates for DataSync tasks with EFS source or destination locations resulting in \"The DataSync destination location is not mounted correctly\".</div><div><span class=\"yellowfg\"> 7:00 AM PDT</span>&nbsp;We continue to investigate elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We'll provide an update at 8:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We have identified the cause of the elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We continue to work towards resolution. We'll provide another update at 9:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 9:11 AM PDT</span>&nbsp;We have started to deploy an update to mitigate the elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The deployment will take approximately 1 hour and 45 minutes to reach all affected regions. We will provide another update once the deployment is complete.</div><div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;Between March 15 10:57 PM and March 16 9:57 AM PDT we experienced elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The issue has been resolved and the service is operating normally.</div>",
      "service": "datasync-us-east-1"
    }
  ],
  "current": [
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "Delayed ENI attachment times",
      "date": "1648779488",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:18 PM PDT</span>&nbsp;We are investigating delayed ENI attachment times for EC2 instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Newly launched EC2 instances or new ENI attachments, may experience delay in establishing network connectivity within the affected Availability Zone. This issue may also affect resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We have identified the root cause and are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;While ENI attachment times have improved, they are still taking longer than normal in the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone. We have identified the root cause for this resource contention and are working to fully resolve the issue. For customers launching instances in the affected Availability Zone or attaching new ENIs to existing instances, full network connectivity on the ENIs may take several minutes to be established, instead of seconds. While we expect attachment times to continue to improve, full recovery here may take up to 2 hours. This issue also affects resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We continue to see an improvement in ENI attachment times, and while they are getting much closer to normal levels, we're still seeing some ENI attachments take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. We are making progress in resolving the resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, and remain on track for full recovery within 1.5 hours. This issue also affects resource provisioning for other services, such as ELB, EMR, ECS and Glue. Some of these services, such as EMR, have mitigated impact by shifting traffic away from the affected Availability Zone, and others like ELB, are now seeing recovery as ENI attachment latencies have improved. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:58 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We remain on track for full recovery within the next hour. At these ENI attachments times, many of the affected services are seeing recovery, or limited impact, as a result of the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 9:31 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We expect to see full recovery within the next 30 to 60 minutes. At these ENI attachments times, ELB and Glue are seeing recovery, while other services - including EMR, EKS, ECS, and RDS - are seeing limited impact within the affected Availability Zone. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div>",
      "service": "ec2-us-east-1"
    }
  ]
}
