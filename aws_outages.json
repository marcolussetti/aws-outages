{
  "archive": [
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Delayed ENI attachment times",
      "date": "1648779488",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:18 PM PDT</span>&nbsp;We are investigating delayed ENI attachment times for EC2 instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Newly launched EC2 instances or new ENI attachments, may experience delay in establishing network connectivity within the affected Availability Zone. This issue may also affect resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We have identified the root cause and are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;While ENI attachment times have improved, they are still taking longer than normal in the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone. We have identified the root cause for this resource contention and are working to fully resolve the issue. For customers launching instances in the affected Availability Zone or attaching new ENIs to existing instances, full network connectivity on the ENIs may take several minutes to be established, instead of seconds. While we expect attachment times to continue to improve, full recovery here may take up to 2 hours. This issue also affects resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We continue to see an improvement in ENI attachment times, and while they are getting much closer to normal levels, we're still seeing some ENI attachments take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. We are making progress in resolving the resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, and remain on track for full recovery within 1.5 hours. This issue also affects resource provisioning for other services, such as ELB, EMR, ECS and Glue. Some of these services, such as EMR, have mitigated impact by shifting traffic away from the affected Availability Zone, and others like ELB, are now seeing recovery as ENI attachment latencies have improved. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:58 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We remain on track for full recovery within the next hour. At these ENI attachments times, many of the affected services are seeing recovery, or limited impact, as a result of the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 9:31 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We expect to see full recovery within the next 30 to 60 minutes. At these ENI attachments times, ELB and Glue are seeing recovery, while other services - including EMR, EKS, ECS, and RDS - are seeing limited impact within the affected Availability Zone. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:04 PM PDT</span>&nbsp;Over the last 15 minutes, we have seen a further improvement in ENI attachment times within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. The resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone has been resolved, and network state is now beginning to propagate through the affected Availability Zone. As this happens, we would expect to see ENI attachment times return to normal levels over the next 15 - 30 minutes. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:39 PM PDT</span>&nbsp;While we have continued to make some progress over the last 30 minutes, progress has been slower than expected and ENI attachment times have not yet returned to normal levels within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. ENI network state continues to propagate through the affected Availability Zone, but is expected to take another 15 - 30 minutes before we reach normal ENI attachment times. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">11:27 PM PDT</span>&nbsp;ENI network state continues to propagate through the affected Availability Zone (USE1-AZ4) further reducing ENI attachment times. Several affected services - including ELB, Glue and RDS - are now seeing full recovery, while other services - including EMR, EKS, and ECS - are experiencing limited impact at this stage. While we have not yet seen ENI attachment times return to normal levels just yet, we continue to make progress in resolving the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">Apr 1,  1:45 AM PDT</span>&nbsp;Starting at 5:03 PM PDT, we experienced increased ENI attachment times for newly launched EC2 instances and newly attached ENIs within a single Availability Zone  (USE1-AZ4) in the US-EAST-1 Region. The issue was caused by increased resource contention in the subsystem responsible for the propagation of ENI network mappings within the affected Availability Zone. Engineers worked to identify the root cause of the resource contention and took steps to resolve it. By 7:45 PM PDT, ENI attachment times had returned to low single digit minute levels, which allowed most workflows to proceed and limited the impact to other AWS services. Some internal services, such as EMR, weighted away from the affected Availability Zone, which mitigated the impact. ENI attachment times continued to improve as the resource contention issue was addressed, and by 12:50 AM PDT, ENI attachment times had returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Power event impacting some instances",
      "date": "1649238391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:46 AM PDT</span>&nbsp;Starting at 1:23 AM PDT, some EC2 instances experienced a loss of power and some EBS volumes experienced degraded performance within a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. Power was quickly restored to the affected instances and EBS volumes and by 1:40 AM PDT, the majority of EC2 instances and EBS volumes had fully recovered. By 2:05 AM PDT, the vast majority of affected EC2 instances and EBS volumes had fully recovered. Some RDS databases were also affected by the event, and recovered after power was restored. Customers with affected EC2 instances and EBS volumes were notified via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased Launch Failures",
      "date": "1650382771",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:39 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption by Default in the US-EAST-1 Region for three Availability Zones: use1-az2, use1-az6 and use1-az5.  Existing instances are not affected, and no other launch requests are affected.  We have identified the root cause and are working towards recovery. </div><div><span class=\"yellowfg\"> 9:18 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that use EBS Encryption by Default in the US-EAST-1 Region.  Existing instances are not affected, and no other launch requests are affected. We have identified the root cause.  We are seeing recovery in use1-az5.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:46 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT. We began seeing significant recovery in use-az6 at 10:20 AM PDT and are beginning to see recovery in use1-az2.</div><div><span class=\"yellowfg\">11:30 AM PDT</span>&nbsp;Between 6:35 AM and 11:00 AM PDT we experienced increased error rates for new instance launches in the US-EAST-1 Region. Existing instances were unaffected by this issue. By 9:05 AM, we had full recovery in use1-az5. By 10:20 AM, we had full recovery in use1-az6, and at 11:00 AM we had recovered the final Availability Zone, use1-az2. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Increased Launch Error Rates",
      "date": "1650969049",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:30 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption from an unencrypted AMI in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. For a small number of volumes in the Availability Zone, increased EBS I/O latencies may also be experienced. Existing instances are not affected, and no other launch requests are affected. We have identified the root cause and are working towards recovery.</div><div><span class=\"yellowfg\"> 4:30 AM PDT</span>&nbsp;Between 1:32 AM and 3:46 AM PDT, we experienced increased error rates for new launches that used EBS volumes in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. A small number of volumes in the Availability Zone may have experienced increased EBS I/O latencies during this time, and this issue has also been resolved for the vast majority of customers. We will be directly notifying a small subset of customers who may still be affected by increased EBS I/O latencies via the AWS Health Dashboard.</div>",
      "service": "ec2-ap-southeast-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1651071794",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We are investigating increased Amazon Redshift API and Console error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:58 AM PDT</span>&nbsp;We continue to investigate increased Amazon Redshift API and Console error rates in the US-EAST-1 Region that impact cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers using temporary credentials.</div><div><span class=\"yellowfg\"> 9:34 AM PDT</span>&nbsp;We are narrowing in on the root cause of the issue causing increased Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue is impacting cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;Between 7:12 AM and 9:52 AM PDT, we experienced increased error rates in Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue impacted cluster management operations and certain customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Network Connectivity Issues",
      "date": "1651363256",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:00 PM PDT</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ2) 에서 일부 EC2 인스턴스, RDS 데이터베이스의 연결 문제 및 일부 EBS 볼륨의 성능 저하를 조사하고 있습니다.이 문제를 해결하기 위해 노력하고 있습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우  AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We are investigating connectivity issues for some EC2 instances, RDS databases and degraded performance for some EBS volumes in a single Availability Zone (APNE2-AZ2) in the AP-NORTHEAST-2 Region. We are working to resolve the issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;태평양 표준시 기준 오후 4시 29분부터 AP-NORTHEAST-2 리전의 단일 가용 영역 내에 있는 일부 EC2 인스턴스 및 EBS 볼륨의 기본 하드웨어에 대한 전력 손실이 발생했습니다.전력은 태평양 표준시 기준 오후 4시 36분에 복원되었고, 영향을 받은 EC2 인스턴스 및 EBS 볼륨이 복구되기 시작했습니다.이 단계에서 영향을 받는 EC2 인스턴스 및 EBS 볼륨의 대부분이 복구되었으며, AWS에서는 여전히 영향을 받는 소수의 인스턴스에 대한 작업을 계속 진행하고 있습니다.일부 RDS 데이터베이스에서도 이 기간 동안 연결 문제가 발생했습니다.다중 AZ 데이터베이스는 영향을 받는 가용 영역에서 성공적으로 장애가 회복되었지만 단일 AZ 데이터베이스는 전원이 복원될 때까지 손상된 상태로 남아 있었습니다.다른 가용 영역은 이 문제의 영향을 받지 않습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support )를 통해 AWS 지원 부서에 문의하십시오. | Starting at 4:29 PM PDT, we experienced a loss of power to the underlying hardware for some EC2 instances and EBS volumes within a single Availability Zone in the AP-NORTHEAST-2 Region. Power was restored at 4:36 PM PDT, and the affected EC2 instances and EBS volumes began to recover. At this stage, the majority of affected EC2 instances and EBS volumes have recovered, and we continue to work on the small number that are still affected. Some RDS databases also experienced connectivity issues during this time period. Multi-AZ databases successfully failed away from the affected Availability Zone, but Single-AZ databases would remain impaired until the power was restored. Other Availability Zones are not affected by this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:31 PM PDT</span>&nbsp;영향을 받는 모든 EC2 인스턴스 및 EBS 볼륨의 기본 하드웨어의 전원이 복원되었음을 확인하였습니다. EC2 인스턴스와 EBS 볼륨은 계속 복구되며, 지금은 소수의  인스턴스만 남아있습니다 (대부분 i3 인스턴스) 그리고, 영향을 받는 가용 영역에서 소수의 EBS 볼륨의 성능이 저하되었습니다.RDS Aurora를 포함한 단일 AZ RDS 데이터베이스는 영향을 받는 가용 영역에 영향을 주었지만 전원이 복원되면서 복구되고 있습니다.모든 다중 AZ RDS 데이터베이스가 영향을 받는 가용 영역에서 장애 조치가 되었습니다.소수의 단일 AZ 애플리케이션 로드 밸런서가 영향을 받는 가용 영역 내에서 패킷 손실이 증가했지만, 다른 부하 분산 트래픽은 다른 가용 영역으로 이동되었습니다.API Gateway는 영향을 받는 가용 영역 내에서 M-TLS 요청에 대한 패킷 손실이 증가했지만 이제는 완전히 복구되었습니다.AWS에서는 여전히 영향을 받는 소수의 EC2 인스턴스 및 EBS 볼륨에 대해 계속 작업하고 있습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We can confirm that power has been restored to the underlying hardware of all affected EC2 instances and EBS volumes. EC2 instances and EBS volumes continue to recover and we now have a small number of instances – mostly i3 instances – and degraded performance for a small number of EBS volumes in the affected Availability Zone. Single-AZ RDS databases – including Amazon Aurora – experienced impact in the affected Availability Zone, but are recovering as power is restored. All Multi-AZ RDS databases have failed away from the affected Availability Zone. A small number of Single-AZ Application Load Balancers experienced elevated packet loss within the affected Availability Zone, but other load balancing traffic was shifted to other Availability Zones. API Gateway experienced elevated packet loss for M-TLS requests within the affected Availability Zone, but has now fully recovered. We continue to work on the small number of EC2 instances and EBS volumes that are still affected. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 6:00 PM PDT</span>&nbsp;영향을 받은 EC2 인스턴스 및 EBS 볼륨의 대부분을 복구했습니다.아직 성능 저하를 겪고 있는 적은수의 EBS 볼륨이 있으며, 이를 해결하기 위해 계속 노력하고 있습니다.일부 EC2 인스턴스 및 EBS 볼륨은 전력 손실 후 복구할 수 없는 하드웨어에서 호스팅되었을 수 있습니다.고객은 해당하는 EC2 인스턴스 및 EBS 볼륨에 대한 만료 통지를 받을것입니다. EC2 인스턴스, EBS 볼륨 또는 RDS 데이터베이스에서 여전히 영향이 있는 경우 영향을 받는 리소스를 다시 시작하거나 다시 생성하는 단계를 수행하는 것이 좋습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We have recovered the vast majority of the affected EC2 instances and EBS volumes. We have a small number of EBS volumes that are still experiencing degraded performance and continue to work to resolve them. Some EC2 instances and EBS volumes may have been hosted on hardware that was not recoverable after the loss of power. Customers will receive retirement notices for EC2 instances and EBS volumes where that is the case. If you continue to see impact for an EC2 instance, EBS volume, or RDS database, we recommend taking steps to relaunch or recreate the affected resource. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS IoT Core (Oregon)",
      "summary": "[RESOLVED] Increased API Errors and Latencies",
      "date": "1651634279",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for Connect, Subscribe &amp; Publish operations in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:40 PM PDT</span>&nbsp;We have identified the root cause for increased error rates and latency of Connect, Subscribe and Publish operations for new connections in the US-WEST-2 Region and are working towards resolution. A scheduled update to optimize the underlying infrastructure for the AWS IoT registry resulted in lower memory availability than required - this has been corrected, and recovery is in progress. Existing connections are not affected.</div><div><span class=\"yellowfg\"> 8:55 PM PDT</span>&nbsp;Between 7:17 PM and 8:41 PM PDT, we experienced increased error rates and latency for Connect, Subscribe, Publish and Registry operations in the US-WEST-2 Region. Existing connections were not affected by the event. The issue has been resolved and the service is operating normally.</div>",
      "service": "awsiot-us-west-2"
    },
    {
      "service_name": "Amazon API Gateway (Ireland)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1652721211",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:13 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Some other AWS services are also affected by this issue, which we will provide further details on shortly. We are working to determine root cause and resolve the issues.</div><div><span class=\"yellowfg\">10:31 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Both the error rates and latencies are at a low enough level where retries would allow for a request to succeed. We have identified the root cause and are working to fully resolve the issue. The following AWS services are experiencing low levels of error rates as a result of this issue: Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;Between 9:39 AM and 10:27 AM PDT, we experienced increased error rates and latencies for API Gateway in the EU-WEST-1 Region. All AWS services - including Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker - have now recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "apigateway-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API errors and latencies",
      "date": "1652830549",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:35 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. This issue does not affect traffic on running load balancers.</div><div><span class=\"yellowfg\"> 5:09 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic remains unaffected on running load balancers. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact to your account events. We’ll continue to update this post as we have more information to share.</div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for ELB APIs in the US-EAST-1 Region and have taken steps to mitigate the issue. We are now seeing recovery for the affected ELB APIs. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. We will continue to monitor until we can confirm full recovery.</div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;Between 3:58 PM and 5:28 PM PDT, we experienced increased error rates and latencies for some ELB APIs in the US-EAST-1 Region. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Stockholm)",
      "summary": "[RESOLVED] Elevated API latencies and error rates and packetloss",
      "date": "1652985887",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;We are investigating elevated API errors and latencies and elevated packet loss in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\">12:08 PM PDT</span>&nbsp;Between 11:12 AM and 11:45 AM PDT, we experienced elevated API errors and latencies and connectivity issues in the EU-NORTH-1 Region. The issue has been resolved and the services are operating normally.</div>",
      "service": "kinesis-eu-north-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased Errors Managing Payment Methods",
      "date": "1653519230",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:53 PM PDT</span>&nbsp;Between 12:30 PM and 2:10 PM PDT we experienced increased errors managing payment methods. The issue is resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Intermittent DNS resolution failures",
      "date": "1653618095",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:21 PM PDT</span>&nbsp;We are aware of intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside AWS.  DNS queries for domains hosted on Route 53 are operating without any issues at this time.  We are actively working with the third-party DNS provider to resolve the issue as quickly as possible.</div><div><span class=\"yellowfg\"> 8:07 PM PDT</span>&nbsp;We confirm intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside of AWS. The third-party DNS provider is working toward resolution. We are also working toward a resolution that addresses the issues the third-party provider has encountered. Queries for DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 8:42 PM PDT</span>&nbsp;We can confirm the start of recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider.  We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 9:01 PM PDT</span>&nbsp;We can confirm the broad recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider. We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\">10:24 PM PDT</span>&nbsp;Between 5:35 PM and 9:58 PM PDT, we experienced intermittent DNS resolution issues for certain specific AWS endpoints. We can confirm that DNS resolution issues for these AWS names have been resolved. Since DNS answers for some of these names that were affected by issues with the third-party DNS provider could have been cached on AWS DNS resolvers, we are flushing these resolver caches over the next few hours. If you run your own DNS Resolvers, and you experience DNS resolution issues we suggest to flush your DNS Resolver cache. Queries for the DNS records hosted on Route 53 were not affected by this issue.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1654825968",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:52 PM PDT</span>&nbsp;We are investigating an increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. Existing load balancers are not affected by the issue. We are working to identify the root cause and resolve the issue.</div><div><span class=\"yellowfg\"> 7:03 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. This is causing increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We are working to resolve the issue and expect to see an improvement in error rates as that progresses.</div><div><span class=\"yellowfg\"> 7:54 PM PDT</span>&nbsp;We continue to make progress towards resolving the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. While error rates have stabilized, we continue to see increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We will continue to keep you updated on our progress.</div><div><span class=\"yellowfg\"> 8:13 PM PDT</span>&nbsp;Starting at 7:46 PM PDT we experienced higher error rates and latencies for AWS services within the US-EAST-1 region. These error rates and latencies have seen some improvement from 7:55 PM PDT, but remain elevated. The issue is also affecting the AWS Management Console for the US-EAST-1 region. We continue to work towards mitigating the impact and will continue to provide updates as we progress.</div><div><span class=\"yellowfg\"> 8:38 PM PDT</span>&nbsp;We continue to work on addressing the error rates and latencies for AWS services in the US-EAST-1 Region. The issue initially affected the Elastic Load Balancing APIs, with some impact to other services, including Elastic Container Service, Amazon Certificate Manager, and Directory Services. At 7:46 PM PDT, other AWS services began to experience an increase in error rates and latencies. Action was taken and error rates started to improve at 7:55 PM PDT. This issue also affected access to the the AWS Management Console for the US-EAST-1 Region. Error rates and latencies for services in the US-EAST-1 Region remain elevated for a number of services, including Connect, DynamoDB, SQS, SNS, EC2, CloudFormation, CloudFront, amongst others. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\"> 9:13 PM PDT</span>&nbsp;We continue to see a reduction in error rates and latencies for AWS services within the US-EAST-1 Region as we work to resolve the issue. While many AWS services are experiencing elevated error rates and latencies, services that are experiencing higher error rates have been tagged on this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. The AWS Management Console is operating normally, however some customers may observe API Errors at times. AWS Connect error rates have improved as well and we continue to work towards full recovery. We are working on applying mitigations to fully resolve the issue.</div><div><span class=\"yellowfg\"> 9:32 PM PDT</span>&nbsp;We continue to see a reduction in API error rates and latencies for services within the US-EAST-1 Region. Elastic Load Balancer APIs have recovered and returned to normal levels. The AWS Management Console has recovered. AWS Connect error rates have returned to normal levels. CloudWatch metrics have recovered and the EC2 API error rates have returned to normal levels. DynamoDB has recovered and is operating normally. Services that were affected by this event remain tagged to this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. </div><div><span class=\"yellowfg\"> 9:58 PM PDT</span>&nbsp;Starting at 6:01 PM PDT, we experienced elevated error rates and latencies for AWS services within the US-EAST-1 Region. The issue affected AWS service APIs, with no impact to data plane services such as EC2 instances, EBS volumes, or Elastic Load Balancers. We started to see recovery at 7:55 PM PDT and were fully recovered by 9:25 PM PDT. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Incorrect Free Tier Email Alerts",
      "date": "1655342154",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:15 PM PDT</span>&nbsp;We are investigating customers receiving email alerts about Free Tier Usage which does not match what customers are seeing in the billing console.</div><div><span class=\"yellowfg\"> 6:39 PM PDT</span>&nbsp;Between 11:00 AM and 6:00 PM PDT, some customers received incorrect email alerts about their Free Tier Usage. This was caused by an incorrect software deployment, which has since been rolled back. During this time, the AWS Billing Console was reporting the correct information. Customers who received an incorrect email alert will receive a notification via the AWS Health Dashboard Event Log in your AWS account.</div><div><span class=\"yellowfg\"> 6:53 PM PDT</span>&nbsp;We want to clarify our previous post. Between 11:00 AM and 6:00 PM PDT, the Home page and the Bills page on the AWS Billing Console reported correct information. The Free Tier page on the AWS Billing Console displayed the same incorrect information included in the incorrect email alert.  We have corrected the issue that caused both the incorrect emails and the incorrect information in the Free Tier page on the AWS Billing Console, and they are all reporting correct information now.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Availability Decreased",
      "date": "1657032030",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:40 AM PDT</span>&nbsp;We can confirm a decrease in the Cost Explorer Console availability. We are actively working towards resolution.</div><div><span class=\"yellowfg\"> 8:39 AM PDT</span>&nbsp;We have identified the cause of the issue affecting Cost Explorer and are working towards resolution. Customers using the Cost Explorer console may experience long load times, timeouts and errors using the console. Cost Explorer APIs are experiencing elevated latencies and error rates. The issue is limited to the Cost Explorer console and APIs, AWS Billing is not otherwise impacted.</div><div><span class=\"yellowfg\"> 9:19 AM PDT</span>&nbsp;We continue to investigate the issue affecting Cost Explorer and are working towards resolution.  We will provide the next update by 10:00 AM PDT.</div><div><span class=\"yellowfg\">10:01 AM PDT</span>&nbsp;We continue to investigate the issue affecting Cost Explorer and are working towards resolution.  We will provide the next update by 10:30am PDT.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;Between 05:10 AM and 10:27 AM PDT, customers using the Cost Explorer console experienced long load times, timeouts, or errors using the console. Cost Explorer APIs also experienced elevated latencies and error rates. The issue has been resolved and the service is operating normally. AWS Billing was otherwise not impacted.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] Impaired EC2 Instances",
      "date": "1657475798",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:56 AM PDT</span>&nbsp;We are investigating instance impairments in a single Availability Zone (EUW2-AZ1) in the EU-WEST-2 Region. Other Availability Zones are not affected by the event and we are working to resolve the issue.</div><div><span class=\"yellowfg\">11:13 AM PDT</span>&nbsp;Some instances in a single Availability Zone are currently impaired or have lost power in the EU-WEST-2 Region. The root cause is a thermal event within a data center in the affected Availability Zone that we are working to resolve. Some instances may also be experiencing network connectivity issues in the affected Availability Zone. Elastic Load Balancing has shifted traffic away from the Availability Zone. All multi-AZ databases have failed away from the affected Availability Zone, however single-AZ databases will remain affected until we see full recovery. We do not yet have a an ETA for full recovery but expect it to be more than an hour. We will provide further guidance as soon as we have it. For customers that are able to fail away from the affected Availability Zone, we recommend doing so.</div><div><span class=\"yellowfg\">11:55 AM PDT</span>&nbsp;We continue to investigate instance impairments to a single Availability Zone in the EU-WEST-2 Region. We have experienced an increase in temperatures within a single data center in the affected Availability Zone, which in some cases has caused impairments for instances in the Availability Zone. We have engineers within the affected data center and are working to resolve the issue. ELB has shifted traffic away from the affected Availability Zone, and API Gateway has mitigated the majority of impact and continues to work on the remaining endpoints. Elastic File System (EFS) is experiencing errors within the affected Availability Zone. RedShift, OpenSearch, and ElastiCache are experiencing error rates for clusters within the affected Availability Zone. RDS has successfully mitigated impact for all multi-AZ databases, however single-AZ databases will remain affected until we see recovery. Lambda is largely unaffected by the event, however a very small number of functions may be experiencing invocation errors within the affected Availability Zone. The EC2 APIs are experiencing increased error rates within the affected Availability Zone, but instance launches continue to work on other Availability Zones. Some services, like EMR and Fargate, are seeing delays in provisioning new instances in the affected Availability Zone due to the EC2 API impact in that Availability Zone. Our engineering teams continue to work towards identifying the root cause of the thermal event and resolving it. At this stage, we do not have an ETA but still expect it to be more than an hour. We continue to recommend using other Availability Zones in the EU-WEST-2 Region, which remain unaffected by this event.</div><div><span class=\"yellowfg\">12:25 PM PDT</span>&nbsp;We have resolved the root cause of the thermal event and are starting to see recovery for impaired EC2 instances within the EU-WEST-2 Region. At this stage, the vast majority of EC2 instances have recovered and we continue to work on the instances that are still affected. ELB and API Gateway have shifted traffic away from the affected Availability Zone. EFS is only experiencing error rates for one zone filesystems within the affected Availability Zone; standard filesystems, which use multiple Availability Zones, are not affected. Customers should now be seeing recovery for instance impairments, and we expect to see recovery for the vast majority of instances within the next hour. We will continue to provide updates as recovery continues.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;We continue to make progress in resolving the EC2 impaired instances in a single Availability Zone in the EU-WEST-2 Region. At this stage, the vast majority of affected instances have recovered and we continue to work towards full recovery. ELB and API Gateway remain weighted away from the affected Availability Zone for now. EFS has recovered the availability of all One Zone file systems in the affected Availability Zone. RDS multi-AZ databases remain available and we are starting to see recovery for single-AZ databases within the affected Availability Zone. EKS pods within the affected Availability Zone are starting to see recovery, however EKS did experience failures during cluster creation during the event. RedShift, OpenSearch, and ElastiCache are starting to see recovery within the affected Availability Zone. EC2 APIs have fully recovered and new instances can once again be launched in the affected Availability Zone. Customers should be seeing most of their affected instances in recovery, although we continue to work on a small number of affected EC2 instances and EBS volumes that are still impaired. We will continue to provide updates as recovery progresses.</div><div><span class=\"yellowfg\"> 1:53 PM PDT</span>&nbsp;We have resolved the impairments for the vast majority of EC2 instances in the affected Availability Zone (euw2-az1) in the EU-WEST-2 Region. There are a small number of EC2 instances and EBS volumes that were hosted on hardware that have been affected by the loss of power during the thermal event. For these EC2 instances and EBS volumes, we will be opening Personal Health Dashboard notices to track recovery. We have seen full recovery for a number of AWS services, including AWS Transit Gateway, Amazon Connect, Amazon Relational Database Service, Amazon ElastiCache, Amazon Elastic Container Service, Amazon Elastic File System, Amazon, Elastic Kubernetes Service, Amazon Elastic MapReduce, Amazon OpenSearch Service, and Amazon Redshift. The remaining AWS services, including Amazon API Gateway, Amazon CloudFront, Amazon Elastic Load Balancing are very close to full recovery at this stage. Elastic Load Balancing and API Gateway will be shifting traffic back into the affected Availability Zone shortly. We’re working through the remaining EC2 instances and EBS volumes and expect to see full recovery in the next 30 minutes.</div><div><span class=\"yellowfg\"> 2:07 PM PDT</span>&nbsp;Starting at 10:25 AM PDT, some EC2 instances and EBS volumes were impaired in a single Availability Zone (euw2-az1) in the EU-WEST-2 Region. The issue was caused by a thermal event, which caused some of instances to lose power and some EBS volumes to experience degraded IO performance. Recovery started at 12:01 PM PDT and by 12:12 PM PDT, the vast majority of EC2 instances and EBS volumes had fully recovered. Other AWS services also experienced impact during the event, including API Gateway, Elastic Load Balancing, Database Migration Service, Transit Gateway, CloudFront, Connect, ElastiCache, Elastic Container Service, Elastic File System, Elastic Kubernetes Service, Elastic MapReduce, OpenSearch, Redshift, and Relational Database Service. These services are all now operating normally. There are a small number of EC2 instances and EBS volumes that were hosted on hardware that has been affected by the loss of power during the thermal event. For these EC2 instances and EBS volumes, we will be opening Personal Health Dashboard notices to track recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased AWS SSO Management Console Error Rates",
      "date": "1658156924",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:08 AM PDT</span>&nbsp;We are investigating increased error rates in the AWS SSO management console in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:49 AM PDT</span>&nbsp;We can confirm an issue with the AWS Single Sign-On (SSO) service, which is affecting customers who are using an external identity provider that is enabled in the US-EAST-1 Region.  Affected customers experiencing issues when attempting SSO authentication for the AWS console and services.  We are actively working towards identifying the root cause.</div><div><span class=\"yellowfg\"> 9:10 AM PDT</span>&nbsp;We are seeing recovery of this issue since 8:41 AM PDT, and all AWS console and API requests authenticated with an external SSO provider in the US-EAST-1 Region should be working properly now. We are continuing to monitor systems before confirming full recovery.</div><div><span class=\"yellowfg\"> 9:43 AM PDT</span>&nbsp;Between 7:34 AM and 8:41 AM PDT AWS Single Sign-On (SSO) customers using an external identity provider that is enabled in the US-EAST-1 Region experienced issues when attempting SSO authentication for the AWS console and services.  The issue has been identified and mitigated, and all SSO authentication requests are working properly now.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1658178914",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:15 PM PDT</span>&nbsp;We are investigating increased error rates for SSO in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:44 PM PDT</span>&nbsp;We can confirm an issue that is preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available. Existing sessions are not affected by this issue, but will experience difficultly re-authenticating should the existing session expire. This issue is affecting AWS services that use sign-in authentication, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. This issue does not affect authentication using IAM users or roles. We are working to resolve the issue and will provide you with regular updates.</div><div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;We continue to work on resolving the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. We have identified the root cause of the issue, which is within the subsystem responsible for hosting the authentication page. Customers attempting to establish a new session or re-authenticate an expired session may experience increased error rates (503/504) when attempting to access the authentication page. In some cases, retries will succeed and allow for the session to be re-authenticated. Engineers are working to resolve the issue, but we do not yet have an ETA for resolution. We are working on mitigations now, and will provide a crisper ETA in the next update. The issue continues to affect AWS services using sign-in authentication, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. Note that should these services be configured to use other authentication types, such as IAM or SAML, they will not be affected by this issue. This issue does not affect authentication using IAM users or roles. We are working to resolve the issue and will provide you with regular updates.</div><div><span class=\"yellowfg\"> 3:50 PM PDT</span>&nbsp;We continue to work on resolving the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. Over the last 30 minutes we have made progress in addressing the issue and error rates with the authentication page have begun to improve. In many cases, retries are now working, allowing for authentication to be completed. While we have seen an improvement in error rates, they have not yet returned to normal levels, so engineers remain engaged and working to fully resolve the issue. We expect error rates to continue to drop over the next hour, but are still not certain on an exact ETA for full resolution of the event.</div><div><span class=\"yellowfg\"> 4:37 PM PDT</span>&nbsp;We have seen error rates improve substantially for the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. In several cases, customers are seeing recovery and have been able to authenticate their sign-in requests. We continue to work on the remaining error rates, which continue to cause some issues with the access to the authentication page. At this stage, retrying requests should allow for authentication requests to succeed. This issue does not affect authentication using IAM users or roles. We will continue to provide updates as we work to fully resolve the issue.</div><div><span class=\"yellowfg\"> 4:58 PM PDT</span>&nbsp;We have resolved the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page was not available in the US-EAST-1 Region. Beginning at 12:35 PM PDT, customers experienced errors when attempting to authenticate using the sign-in page in the US-EAST-1 Region. Engineers worked to resolve the issue, and at 3:23 PM PDT error rates began to improve, allowing customers to authenticate their sign-in requests. At 3:41 PM PDT, error rates had returned to normal levels and the authentication page was once again available and serving sign-in requests from affected AWS services, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. Between 4:12 PM and 4:38 PM PDT, we experienced an increase in error rates once again, but this has also been resolved. This issue did not affect authentication using IAM users or roles, or SAML. The issue has been resolved and all services are operating normally.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Secrets Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Errors and Latencies",
      "date": "1658948667",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:04 PM PDT</span>&nbsp;We are investigating increased API connection timeout errors and latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:33 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for Secrets Manager APIs in the US-EAST-1 Region. We are working to identify root cause and resolve the issue. \n\nCustomers using Secrets Manager to store secrets may experience errors when using the Secrets Manager APIs, including customers invoking Secrets Manager from Lambda functions. \n\nLambda functions that are not calling Secrets Manager APIs are not affected by this event. We continue to work to understand the issue and will continue to provide updates. </div><div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;We have resolved the issue causing increased error rates and latencies for the Secrets Manager APIs in the US-EAST-1 Region. Customers using Secrets Manager to store secrets may have experienced errors when using the Secrets Manager APIs, including customers invoking Secrets Manager from Lambda functions. Lambda functions that are not using Secrets Manager were not affected by this event. The issue was resolved at 12:30 PM PDT, when error rates and latencies returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "secretsmanager-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Instance Impairments",
      "date": "1659028265",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:11 AM PDT</span>&nbsp;We are investigating network connectivity issues for some instances and increased error rates and latencies for the EC2 APIs within the US-EAST-2 Region.</div><div><span class=\"yellowfg\">10:25 AM PDT</span>&nbsp;We can confirm that some instances within a single Availability Zone (USE2-AZ1) in the US-EAST-2 Region have experienced a loss of power. The loss of power is affecting part of a single data center within the affected Availability Zone. Power has been restored to the affected facility and at this stage the majority of the affected EC2 instances have recovered. We expect to recover the vast majority of EC2 instances within the next hour. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">10:49 AM PDT</span>&nbsp;We continue to see recovery of EC2 instances that were affected by the loss of power in a single Availability Zone in the US-EAST-2 Region. At this stage, the vast majority of affected EC2 instances and EBS volumes have returned to a healthy state and we continue to work on the remaining EC2 instances and EBS volumes. Elastic Load Balancing has shifted traffic away from the affected Availability Zone. Single-AZ RDS databases were also affected and will recover as the underlying EC2 instance recovers. Multi-AZ RDS databases would have mitigated impact by failing away from the affected Availability Zone. While the vast majority of Lambda functions continue operating normally, some functions are experiencing invocation failures and latencies, but we expect this to improve over the next 30 minutes. Power has been restored to all affected resources and remains stable. We expect the recovery of EC2 instances and EBS volumes to continue to improve over the next 45 minutes. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">11:25 AM PDT</span>&nbsp;We continue to make progress in recovering the remaining EC2 instances and EBS volumes affected by the loss of power in a single Availability Zone in the US-EAST-2 Region. The vast majority of EC2 instances are now healthy, but we continue to work on recovering the remaining EBS volumes affected by the issue. EC2 API error rates and latencies have returned to normal levels. Elastic Load Balancing remains weighted away from the affected Availability Zone. Error rates and latencies for Lambda function invocations have now returned to normal levels. Power has been restored to all affected resources and remains stable. We expect the recovery of EC2 instances and EBS volumes to continue to improve over the next 30 minutes. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">11:45 AM PDT</span>&nbsp;At this stage, the vast majority of EC2 instances and EBS volumes, affected by the loss of power in a single Availability Zone in the US-EAST-2 Region, have fully recovered. A small number of EC2 instances and EBS volumes are on hardware that was adversely affected by the loss of power. Engineers continue to work on recovering the EC2 instances and EBS volumes on this hardware and will provide updates via the Personal Health Dashboard if any of these could not be recovered. Elastic Load Balancers affected by the issue have recovered and traffic has been shifted back into the affected Availability Zone. The vast majority of single-AZ databases have also recovered and the remaining databases are running on hardware that was affected by the event. We will provide updates via the Personal Health Dashboard if any of these databases can not be recovered. At this stage, if your EC2 instance or EBS volumes that has still not recovered, attempting a reboot of the EC2 instance could resolve the issue. If not, we recommend relaunching the affected EC2 instance or recreating the affected EBS volume.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;Starting at 9:57 AM PDT some EC2 instances and EBS volumes experienced a loss of power within a single Availability Zone in the US-EAST-2 Region. Power was restored at 10:19 AM PDT and EC2 instances and EBS volumes began to recover. By 10:23 AM PDT, the vast majority of EC2 instances and EBS volumes has fully recovered and by 11:37 AM PDT, all but a very small number of EC2 instances and EBS volumes had recovered. Elastic Load Balancing shifted traffic away from the affected Availability Zone, which has now been shifted back. RDS impact for single-AZ databases, which have also been recovered. Other services (tagged below) saw impact during the event, but most have fully recovered. Those that are still seeing impact, will provide updates via the Personal Health Dashboard as they work towards full recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS CloudTrail (N. Virginia)",
      "summary": "[RESOLVED] CloudTrail event delivery delays",
      "date": "1660680453",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:07 PM PDT</span>&nbsp;We are investigating increased latency of CloudTrail events causing CloudTrail events processing delays in the US-EAST-1 Region starting at 9:33 AM PDT. CloudTrail customers in US-EAST-1 Region will receive events with delay as high as 4 hours.  All new events after 12:57 PM PDT will be processed immediately. ETA for backlog consumption is 3:30 PM PDT.</div><div><span class=\"yellowfg\"> 2:10 PM PDT</span>&nbsp;Between 09:33 AM and 12:57 PM PDT Increased latency of CloudTrail events in the US-EAST-1 Region causing CloudTrail events processing delays. We have resolved the issue all backlog events are in the process of backfilling will be completed by 3:30 PM PDT. The service is operating normally. </div>",
      "service": "cloudtrail-us-east-1"
    },
    {
      "service_name": "Multiple services (Oregon)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1661361799",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies affecting some services within the US-WEST-2 Region. We are working to identify the root cause.</div><div><span class=\"yellowfg\">10:34 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies affecting some services within the US-WEST-2 Region. The root cause appears to be increased error rates and latencies for API Gateway endpoints within the US-WEST-2 Region. Customers with API Gateway endpoints would be experiencing increased error rates and latencies for their requests. Customers calling Lambda via an API Gateway, would also be experiencing increased error rates and latencies for their function invocations. Other AWS services that use API Gateway - for example Batch, EKS, and EventBridge - are also experiencing increased error rates and latencies. Amazon Connect is experiencing increased call failure as well as issues with user login. We have identified the root cause and are working on a mitigation to resolve the issue.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;We have addressed the root cause of the issue causing increased API error rates and latencies in the US-WEST-2 Region and are seeing recovery across affected API Gateway endpoints and AWS services.</div><div><span class=\"yellowfg\">10:53 AM PDT</span>&nbsp;Between 10:00 AM and 10:34 AM PDT we experienced increased API error rates and latencies affecting some services within the US-WEST-2 Region. The root cause of the event was increased error rates and latencies for API Gateway endpoints within the US-WEST-2 Region. Customers calling Lambda functions via an API Gateway, also experienced increased error rates and latencies for their function invocations. Other AWS services that use API Gateway - for example Batch, EKS, and EventBridge - also experienced increased error rates and latencies. Amazon Connect saw increased call failures as well as issues with user logins. We have resolved the root cause of the event and all affected API Gateway endpoints, Lambda functions, and AWS services have fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Amazon Elastic Container Service - Increased Rates of Insufficient Capacity Errors",
      "date": "1661380251",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:30 PM PDT</span>&nbsp;We can confirm increased insufficient capacity error rates for launching new ECS tasks using the Fargate launch type, starting at 1:15 PM PDT. Running tasks and tasks that utilize the EC2 launch type are not impacted. We continue to work through full resolution and will provide another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 4:06 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work through full resolution of the issue. We expect you to begin observing signs of recovery beginning at 4:20 PM PDT. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue. This is taking longer than expected, and we now expect it to be 5:00 PM PDT before you will begin to observe signs of recovery. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:20 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue, however we are experiencing some delays with full recovery. You will still see some task launches succeeding during this event. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:49 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue, however we are experiencing some delays with full recovery. We are working multiple, parallel paths to make additional capacity available. You will still see some task launches succeeding during this event. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 6:49 PM PDT</span>&nbsp;We have identified the cause of the decreased capacity and understand why the Fargate task launch success rate is only 70% at this point. We are working on multiple parallel actions to address the underlying issues and have identified one area in particular that should help us make faster progress towards recovery. We have started work on this and will have an indication of progress by 7:00 PM PDT. Once we have that progress data we will be able to provide an ETA for recovery. We are also making a change to the rate at which ECS launches tasks to reduce load on Fargate and to speed up recovery.\nFor customers with prepared and rehearsed plans for moving to a different region, they should consider exercising those if they are in a place to do so. Customers can also consider using EC2 with ECS and EKS as a mitigation option since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 7:45 PM PDT</span>&nbsp;We have identified the cause of the decreased capacity and understand why the Fargate task launch success rate is only 70% at this point. Our remediation actions are making slower progress than expected, so we are working on additional actions to further reduce load on Fargate. The work started in the previous update is still progressing but we do not yet have a projected ETA for when it will complete or when we will see recovery.\n\nCustomers can switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 8:46 PM PDT</span>&nbsp;The current state is that more than 50% of Fargate task launches in the US-EAST-1 region are succeeding. For the tasks that are failing, we have identified that this is due to a large amount of compute capacity, managed by Fargate, that is in a stuck state. We call these leaked instances. This results in customer tasks and pods not being able to be started, impacting both ECS on Fargate and EKS on Fargate. \n\nWe are driving multiple parallel efforts to address this. First, we're taking action to make sure no additional instances are leaked by reducing the call rates to Fargate, and second we are working to free up these leaked instances so they can be used to run customer tasks. \n\nAs stated in previous updates, the recovery actions are making slower progress than we expected which is preventing us from providing an ETA for recovery at this point. Right now we expect multiple hours before we see recovery. \n\nCustomers who already have Fargate tasks or pods running are recommended to not scale down until we have recovery. Customers can switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 9:19 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances, and we are already seeing faster progress towards this.\n\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will steadily ramp up Fargate task and pod launches and enable normal operations. We don't yet have an ETA, but will communicate one as soon as we have an ETA we feel confident about.\n\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 9:55 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We estimate that we have released 50% of the leaked instances and at the current rate that all leaked instances will have been released by 12:00 AM PDT. \n\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will steadily ramp up Fargate task and pod launches and enable normal operations. We expect to be able to communicate an ETA for recovery soon after we complete releasing all leaked instances. Once this happens, our best estimate is an additional 1 to 2 hours for recovery. \n\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">10:26 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We estimate that we have released 93% of the leaked instances and at the current rate that all leaked instances will have been released by 10:45 PM PDT.\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will slowly ramp up Fargate task and pod launches and enable normal operations. We expect to be able to communicate an ETA for recovery soon after we complete releasing all leaked instances. Once this happens, our best estimate is an additional 1 to 2 hours for recovery.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">11:42 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We have now released effectively all leaked instances and are now starting the process to re-enable Fargate task launches.\nWe will start by enabling Fargate task launches for a small number of accounts and then increase that as we see success. As the pace of recovery is dependent on how fast we increase task launches, it is still too early to provide an ETA for full recovery, but we still expect hours before we are fully recovered.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25, 12:46 AM PDT</span>&nbsp;We have started to enable Fargate task launches in the US-EAST-1 Region again. We are seeing successful task launches and continue to slowly increase the number of tasks being launched. Most customers will still see their task launches failing until we see enough progress to broadly enable task launches. We expect to have enough data to further increase task launches by 1:10 AM PDT.\nThe progress at that point will help inform ETA for recovery, we still estimate this to be hours out.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  1:55 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. At this point, all accounts can launch tasks, but at a lower task launch rate than usual. The lower than normal task launch rates means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. The impact of this is that scaling of services and deployments will take longer than usual. We will incrementally raise task launch rates back to normal levels as we monitor service recovery.\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 4:00 AM PDT.\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  3:08 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. We have just completed a change that increases the task launch rate for tasks running as part of an ECS service. This will reduce the time required both for service deployments and scaling up services. As a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. We will continue to incrementally raise the task launch rate back to normal levels as we monitor service recovery.\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 5:00 AM PDT.\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:03 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. In addition to the earlier change to increase the task launch rate for tasks running as part of an ECS service, we have also increased the task launch rate for the RunTask API from 1 task per second to 2 tasks per second. As a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. We will continue to incrementally raise the task launch rate back to normal levels as we monitor service recovery.\n\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 5:00 AM PDT.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:34 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. The task launch rate for the ECS RunTask API has now been increased from 2 tasks per second to 5 tasks per second and we are seeing a corresponding reduction in task launch failures due to customers exceeding the maximum task launch rate. We are continuing to monitor recovery and will keep incrementally raising the task launch rate until we return to the default of 20 tasks per second.\n\nAs a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed.\n\nWe are no longer seeing elevated task launch failures. It is, however likely to be past 5:00 AM PDT before we have returned to the default task launch rate of 20 tasks per second.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:57 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. All customers now have a task launch rate for the ECS RunTask API of 10 tasks per second and we see further reduction in task launch failures due to customers exceeding the maximum task launch rate. We are continuing to monitor recovery and will keep incrementally raising the task launch rate until we return to the default of 20 tasks per second.\n\nAs a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed.\n\nWe are no longer seeing elevated task launch failures. It is, however likely to be past 5:00 AM PDT before we have returned to the default task launch rate of 20 tasks per second.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  6:05 AM PDT</span>&nbsp;Between August 24 1:07 PM and August 25 6:00 AM PDT, ECS and EKS customers using Fargate experienced increased task and pod launch failure rates. The issue has been resolved and customers are now able to launch tasks and pods normally again. A very small number of customers may still see failing task and pod launches and we will provide updates to those customers using account specific events. ECS and EKS customers using EC2 were not impacted during this event and there was also no impact to any already running tasks or pods on Fargate.</div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "AWS Systems Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latency",
      "date": "1662737760",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:36 AM PDT</span>&nbsp;We are investigating increased API error rates and latency in the US-EAST-1 Region. This will impact customers' ability to make calls to Parameter Store.</div><div><span class=\"yellowfg\"> 9:29 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of data may be affected by the event as they are unable to retrieve the data. We have identified the subsystem where the errors are occurring and are working to resolve the issue. At this stage, we expect recovery to take more than an hour but will keep you updated on our progress.</div><div><span class=\"yellowfg\"> 9:50 AM PDT</span>&nbsp;We continue to work on the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configurations may be affected by the event as they are unable to retrieve the data. Customers invoking Lambda functions that use Environment Variables will also experience increased error rates and latencies. The subsystem responsible for the Parameter Store is experiencing resource contention, which is leading to the increased error rates and latencies. We continue to work toward the resolving the issue, but do not have an ETA on recovery at this stage. We will keep you updated on our progress.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;We continue to work on the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration may be affected by the event as they are unable to retrieve the data. We have confirmed that customers using Lambda Environment Variables are not affected by this event, but rather customers that are attempting to access the Parameter Store directly from a Lambda function. Customers can work around the issue by removing the use of the Parameter Store from their code, or look to use some other store, such as DynamoDB. We are making progress towards resolution but continue to see elevated error rates for the affected subsystem. We do not have an ETA on recovery at this stage and will keep you updated on our progress.</div><div><span class=\"yellowfg\">11:02 AM PDT</span>&nbsp;We are seeing recovery for the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration should be seeing recovery at this stage. We continue to monitor the subsystem that was experiencing resource contention, but expect error rates and latencies to remain at normal levels.</div><div><span class=\"yellowfg\">11:21 AM PDT</span>&nbsp;Starting at 7:34 AM PDT, we experienced increased error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration were affected by the event as they were unable to retrieve the data. Customers using Parameter Store from Lambda functions also experienced increased failure rates. The issue was resolved at 10:45 AM PDT, when error rates and latencies returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2systemsmanager-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Cape Town)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1663153260",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:01 AM PDT</span>&nbsp;We are investigating increased API error rates in the AF-SOUTH-1 Region. Existing instances are unaffected.</div><div><span class=\"yellowfg\"> 4:21 AM PDT</span>&nbsp;We are experiencing elevated API errors and latency with the IAM authentication service in the AF-SOUTH-1 Region.  This is causing impact on services that require IAM authentication.  We are actively investigating root cause to identify a path to mitigation.</div><div><span class=\"yellowfg\"> 5:13 AM PDT</span>&nbsp;We continue to see elevated API and latency for IAM authentication and authorization requests in the AF-SOUTH-1 Region. The IAM errors affect any AWS API call that requires authentication, though some calls may still be succeeding due to API credential caching. We have all teams engaged in investigation of root cause and mitigation, but we do not have an ETA for recovery at this time.</div><div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;We are starting to see recovery of IAM authentication and authorization requests, and customers should be seeing improvements in API error rates in the AF-SOUTH-1 Region.  We are continuing to monitor and work towards full recovery.</div><div><span class=\"yellowfg\"> 5:39 AM PDT</span>&nbsp;We are continuing to see improvement in IAM authentication and authorization API error rates, and a subsequent improvement in AWS service API availability in the AF-SOUTH-1 Region. We continue to monitor this event closely, and will do so until all services have fully recovered.</div><div><span class=\"yellowfg\"> 6:22 AM PDT</span>&nbsp;All service APIs are now operating correctly, and we continue to work towards full recovery of services in the AWS Management Console. We’re continuing to restore normal operations to all AWS consoles in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 6:54 AM PDT</span>&nbsp;Between 3:30 AM and 6:35 AM PDT, customers experienced elevated errors for API and console requests in the AF-SOUTH-1 Region due to an issue with IAM authentication and authorization, which caused elevated error rates for many AWS services in the AF-SOUTH-1 Region.  This issue has been resolved and all services are now working normally.</div>",
      "service": "ec2-af-south-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased Route 53 Health Check API latency",
      "date": "1663585076",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:57 AM PDT</span>&nbsp;We are experiencing delays creating, deleting, and updating Route 53 health checks.  This is also affecting provisioning and scaling workflows for services like ELB, EKS, and RDS that use Route 53 health checks as well.  We have identified the issue and have a path towards recovery, however we do expect that to take at least a couple hours.\n\nExisting Route 53 health checks are operating normally, and are correctly reflecting the health status of their configured resources. \n</div><div><span class=\"yellowfg\"> 4:32 AM PDT</span>&nbsp;We continue to work towards recovery of the Route 53 Health Check API, and expect health check create, update, and delete delays to be resolved within about 90 minutes.  However, it will take a bit longer for other impacted services to fully recover their own provisioning and scaling workflows as they have their own backlogs to work through.</div><div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;The Route 53 health check API has recovered, and all operations are now being processed normally.  Other services that were impacted by this issue are processing their scaling and provisioning backlogs.</div><div><span class=\"yellowfg\"> 6:25 AM PDT</span>&nbsp;Between 12:00 AM and 5:05 AM PDT, customers experienced elevated errors and latency when calling the Route 53 Health Checks API.  This issue also affected provisioning and scaling workflows for some other AWS services.  The Route 53 Health Checks API is now working normally.  ELB, EKS, EMR, and Amazon MQ are still processing backlogs, and affected customers can get more information in Account Specific Service Events.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Mumbai)",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1663816639",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased API error rates for DetachVolume, AttachVolume, and AssociateIamInstanceProfile in the AP-SOUTH-1 Region. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 8:35 PM PDT</span>&nbsp;Between 6:26 PM and 8:23 PM PDT we experienced increased API error rates for DetachVolume, AttachVolume, and AssociateIamInstanceProfile in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-south-1"
    },
    {
      "service_name": "AWS Systems Manager (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1664323892",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;We are experiencing increased latency and error rates when calling the StartSession API for a subset of customers in the US-WEST-2 Region. We have identified the issue and working towards resolution.</div><div><span class=\"yellowfg\"> 5:50 PM PDT</span>&nbsp;Between 2:40 PM and 5:45 PM PDT, we experienced increased latency and error rates when calling the StartSession API for a subset of customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2systemsmanager-us-west-2"
    },
    {
      "service_name": "Amazon API Gateway (Oregon)",
      "summary": "[RESOLVED] Increased Invoke Error Rates",
      "date": "1664385180",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:13 AM PDT</span>&nbsp;We are investigating increased error rates for invokes in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;We are investigating increased error rates for invokes in the US-WEST-2 Region. We do not yet have a root cause, but are investigating multiple potential root causes in parallel. In addition, we are implementing filters on inbound traffic from a set of sources with recent significant traffic shifts, which may help mitigate the impact.  We do not yet have a solid ETA, but will continue to provide updates as we progress.</div><div><span class=\"yellowfg\">10:59 AM PDT</span>&nbsp;We continue to see elevated error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. While engineers continue to work towards root cause, we have deployed traffic filters from sources with significant increases in traffic prior to the event. As a result of these traffic filters, we are seeing a reduction in error rates and latencies, but continue to work towards full recovery. Although error rates are improving, we do not yet have an ETA for full recovery. The issue is also affecting API requests to some AWS services, including those listed below. Amazon Connect is experiencing increased failures in handling new calls, chats, and tasks as well as issues with user login in the US-WEST-2 Region. We will continue to provide updates as we progress.</div><div><span class=\"yellowfg\">11:33 AM PDT</span>&nbsp;We continue to work on resolving the elevated error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. We continue to see a significant improvement in error rates, starting at 10:40 AM PDT, but are not seeing full recovery yet. The issue is caused by contention within the subsystem that is responsible for request processing within the API Gateway service. Engineers are engaged and have applied traffic filters as a precautionary measure, while they work to identify the root cause and resolve the issue. Engineers continue to work to reduce contention within the affected subsystem, which we believe will resolve the elevated error rates and latencies. Customers with applications that use API Gateway, or customers invoking Lambda functions via API Gateway, will be experiencing elevated error rates and latencies as a result of this issue. The AWS services listed below are also experiencing elevated error rates as a result of this issue. While we have seen improvements in error rates since 10:40 AM PDT, recovery has stalled and we do not have a clear ETA on full recovery. For customers that have dependencies on API Gateway and are experiencing error rates, we do not have any mitigations to recommend to address the issue on the customer side. We do expect error rates to continue to improve as contention with the affected subsystem resides, and will provide further updates as recovery progresses. </div><div><span class=\"yellowfg\">12:26 PM PDT</span>&nbsp;We continue to see an improvement in error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region, but have not fully resolved the issue. While our mitigations have improved error rates and latencies, we have also identified the root cause of the event. The subsystem responsible for request processing experienced increased load, which ultimately led to contention of a component within the affected subsystem. Engineers have been working to resolve the contention of the affected component, which has led to a reduction of error rates and latencies. The path to full recovery involves addressing the contention across the subsystem, which we are currently doing. As that progresses over the next two hours, we expect recovery to continue to improve. Customers with applications that use API Gateway will be experiencing elevated error rates and latencies as a result of this issue. Lambda is not affected by this event, but customers using API Gateway as an HTTP endpoint for Lambda will experience increased error rates and latencies. Other AWS services listed below are also experiencing elevated error rates as a result of this issue. For customers that have dependencies on API Gateway and are experiencing error rates, we do not have any mitigations to recommend to address the issue on the customer side. We do expect error rates to continue to improve as contention with the affected subsystem resides, and will provide further updates as recovery progresses. </div><div><span class=\"yellowfg\"> 1:03 PM PDT</span>&nbsp;Error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region, continue to hold steady. Engineers continue to work on resolving the contention affecting the subsystem responsible for request processing. We recently completed a mitigation that should help to reduce error rates and latencies to normal levels and will have further updates on the result of that change in the next update. Although Lambda function invocations are not affected by this issue, the Lambda console is experiencing some error rates which we are investigating. Other AWS services affected by this issue remain in much the same state, waiting on the recovery of API Gateway.</div><div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;Starting at 1:12 PM PDT, we saw a further reduction in error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. This was a result of the latest mitigation which addressed contention within the component in the subsystem responsible for request processing within API Gateway. Error rates are now at levels where some customers may begin to see recovery, and retries will begin to work more consistently. We will be applying the mitigation to the remaining hosts affected by the contention issue and expect to see further recovery from them in the next 30 minutes.</div><div><span class=\"yellowfg\"> 1:42 PM PDT</span>&nbsp;Starting at 1:31 PM PDT, error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region are now close to pre-event levels, and we continue to work on the remaining hosts that are affected by the contention issue. Several AWS services, including AWS Connect and Lambda are seeing signs of strong recovery. We expect all services to recover as API Gateway error rates and latencies return to normal levels. Customers should be seeing recovery at these error levels as well. We will continue to provide updates until the error rates and latencies have returned to normal levels.</div><div><span class=\"yellowfg\"> 2:05 PM PDT</span>&nbsp;As of 1:43 PM PDT, error rates and latencies for invokes for API Gateway endpoints in the US-WEST-2 Region are now at normal levels. The issue began at 9:20 AM PDT when error rates and latencies for API Gateway began to increase. Error rates began to improve at 10:38 AM PDT, when engineers took action to reduce contention within the subsystem that handles request processing for API Gateway. Error rates continued to improve until 1:10 PM PDT, when engineers applied a mitigation to resolve the contention within the affected subsystem. These actions accelerated recovery, and by 1:43 PM PDT, error rates and latencies had returned to normal levels. Affected AWS services have now recovered as well. The issue has been resolved and the service is operating normally.</div>",
      "service": "apigateway-us-west-2"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased API errors",
      "date": "1664397551",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:39 PM PDT</span>&nbsp;We are experiencing elevated errors for calls to the Redshift API and console.  We have identified the root cause of these errors, and are in the process of applying a mitigation to address the issue.  While we work to apply the mitigation, customers are encouraged to re-attempt failed API queries, as they are likely to succeed after multiple attempts.  We do not have a firm ETA for recovery at this time, but will report on the results of the mitigation when it has been applied in 1 hour.</div><div><span class=\"yellowfg\"> 2:42 PM PDT</span>&nbsp;Between 6:15 AM and 2:02 PM PDT, we experienced increased latency and error rates in US-EAST-1 Region impacting customers using Query Editor v2 and the Redshift Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Multiple services (Sao Paulo)",
      "summary": "[RESOLVED] Increased Error rates and Latencies",
      "date": "1664457641",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:20 AM PDT</span>&nbsp;We are investigating increased error rates and latencies affecting multiple services and the AWS Console in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:39 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for some services within the SA-EAST-1 Region. The AWS Management Console is also experiencing elevated error rates as a result of this issue. The root cause of the issue appears to be elevated network packet loss which is affecting some AWS service APIs. The EC2 network, and existing EC2 instances, are not affected by this event. Some services have seen improvements in error rates as a result of mitigations taken by engineers, but we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 6:55 AM PDT</span>&nbsp;Between 5:56 AM and 6:46 AM PDT, we experienced increased error rates and latencies affecting multiple services and the AWS Management Console in the SA-EAST-1 Region. The EC2 network, and existing EC2 instances, were not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-sa-east-1"
    },
    {
      "service_name": "Amazon Managed Grafana (Oregon)",
      "summary": "[RESOLVED] Authentication Issue",
      "date": "1664954146",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:15 AM PDT</span>&nbsp;We are investigating authentication issues affecting workspace availability in the US-WEST-2 Region</div><div><span class=\"yellowfg\">12:50 AM PDT</span>&nbsp;We have identified the cause of the authentication issue affecting some workspaces in the US-WEST-2 Region and continue to work toward resolution. </div><div><span class=\"yellowfg\"> 1:30 AM PDT</span>&nbsp;Between October 4 11:00 PM and October 5 1:07 AM PDT, we experienced authentication issue affecting some workspaces in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "grafana-us-west-2"
    },
    {
      "service_name": "Multiple services (UAE)",
      "summary": "[RESOLVED] Elevated API Error rates",
      "date": "1665092370",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;We are investigating elevated error rates for some services in the UAE (ME-CENTRAL-1) Region including S3, Lambda, and CloudTrail.</div><div><span class=\"yellowfg\"> 3:24 PM PDT</span>&nbsp;Between 2:13 PM PDT and 2:57 PM PDT we experienced increased error rates for Amazon S3 requests in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-me-central-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (UAE)",
      "summary": "[RESOLVED] Elevated API Error rates",
      "date": "1665093648",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:00 PM PDT</span>&nbsp;Beginning at 2:13 PM PDT, we began observing elevated error rates for Amazon S3 PUT and GET APIs in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail.</div><div><span class=\"yellowfg\"> 3:03 PM PDT</span>&nbsp;We are seeing recovery in the elevated error rates for S3 and other AWS services. We are continuing to monitor progress towards full recovery.</div><div><span class=\"yellowfg\"> 3:24 PM PDT</span>&nbsp;Between 2:13 PM PDT and 2:57 PM PDT we experienced increased error rates for Amazon S3 requests in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-me-central-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased API error rates and latency ",
      "date": "1665511560",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:06 AM PDT</span>&nbsp;We are investigating increased API error rates and latency in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">11:16 AM PDT</span>&nbsp;Between 10:33 AM and 11:08 AM PDT, customers may have experienced elevated latency and errors when accessing AWS APIs and consoles in the US-GOV-WEST-1 Region.  We have identified the root cause, which has been addressed, and all services are now operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Route 53 (US-West)",
      "summary": "[RESOLVED] Increased API error rates and latency ",
      "date": "1665513240",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:34 AM PDT</span>&nbsp;Between 10:33 AM and 11:08 AM PDT, customers may have experienced elevated latency and errors when accessing AWS APIs and consoles in the US-GOV-WEST-1 Region. We have identified the root cause, which has been addressed, and all services are now operating normally.  This issue also affected the following services: EC2, Redshift, CloudWatch and OpenSearch.</div>",
      "service": "route53-us-gov-west-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "[RESOLVED] Increased latency for UpdateEnvironment API calls",
      "date": "1665603967",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;We are investigating increased latency for UpdateEnvironment API calls. This issue impacts both the API, and requests made through the Management Console. Additionally, attempts to abort the update (via the AbortEnvironmentUpdate API) may return 5XX errors. We are actively working on identifying the root cause and will provide additional information as soon as possible.</div><div><span class=\"yellowfg\"> 1:28 PM PDT</span>&nbsp;We have identified the root cause of increased latency for UpdateEnvironment API calls in the US-EAST-1 Region. This issue impacts both the API, and requests made through the Management Console. Additionally, attempts to abort the update (via the AbortEnvironmentUpdate API) may return a 5XX error. We are observing partial recovery and continue to work through full recovery of the issue. Another update will be sent by 2:00 PM PDT or as more information becomes available.</div><div><span class=\"yellowfg\"> 1:49 PM PDT</span>&nbsp;Between 10:00 AM and 1:45 PM PDT, we experienced increased latency for UpdateEnvironment API calls in the US-EAST-1 Region.  Existing environments were not impacted.  Impact was limited to updating existing or creating new environments. The issue was related to a partially failed deployment that we were able to mitigate. The issue has been resolved and the service is operating normally.  </div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Cognito (Ohio)",
      "summary": "[RESOLVED] Increased error rates for Cognito Hosted UI",
      "date": "1666632939",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:35 AM PDT</span>&nbsp;Between 8:48 AM and 9:52 AM PDT, we experienced increased error rates for Cognito Hosted UI in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cognito-us-east-2"
    },
    {
      "service_name": "Amazon Simple Notification Service (Milan)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1666912855",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:20 PM PDT</span>&nbsp;We are investigating increased error rates for the Amazon SNS Publish API in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 4:32 PM PDT</span>&nbsp;We are beginning to see recovery in SNS Publish API error rates in the EU-SOUTH-1 Region. </div><div><span class=\"yellowfg\"> 4:55 PM PDT</span>&nbsp;Between 2:57 PM and 4:15 PM PDT we experienced elevated Publish API error rates and delivery latency in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-eu-south-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Increased Launch Failures",
      "date": "1667246817",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:06 PM PDT</span>&nbsp;We are investigating increased failures for newly launched instances within the US-EAST-2 Region. Changes to instance networking state, such as mapping Elastic IP addresses, may also experience increased error rates. We have identified root cause and are working to resolve the issue. Existing instances are not affected by this issue.</div><div><span class=\"yellowfg\"> 1:27 PM PDT</span>&nbsp;Between 12:18 PM and 1:20 PM PDT, we experienced increased failures for newly launched instances within the US-EAST-2 Region. Some APIs, such as those used to attach Elastic IP addresses to instances, were also affected by this event. Some AWS services, such as Elastic Load Balancing and Elastic Map Reduce, experienced provisioning delays due to failures in launching new EC2 instances. Existing instances were not affected by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS Transfer Family (N. Virginia)",
      "summary": "[RESOLVED] Increased Authentication Errors",
      "date": "1667939280",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:28 PM PST</span>&nbsp;We can confirm increased authentication errors when logging into SFTP and FTPS. We have identified the root cause and are in the process of mitigating the issue. Our current mitigation efforts are expected to take 90 minutes. We will provide you with another update by 2:00 PM PST.</div><div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;Between 11:10 AM and 12:40 PM PST, we experienced increased authentication errors when logging into SFTP and FTPS. The issue has been resolved and the service is operating normally.</div>",
      "service": "transfer-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Degraded EBS Volume Performance",
      "date": "1669228100",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:28 AM PST</span>&nbsp;Between 7:58 AM and 8:05 AM PST, some EBS volumes experienced degraded IO performance within the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS QuickSight (N. Virginia)",
      "summary": "[RESOLVED] Missing dashboard and other assets",
      "date": "1669654485",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:54 AM PST</span>&nbsp;We are investigating an issue with missing dashboards and other assets in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:49 AM PST</span>&nbsp;We are observing increased latency and timeouts for loading QuickSight assets such as dashboards and analysis in the homepage. We are observing a scaling bottleneck with one of our dependencies for the search functionality and we are currently working on scaling up the search node. In parallel, we are looking into any recent deployments that could have caused this. If customers know the dashboard and analysis URL, customers can navigate directly from those links. Embedding dashboards will not be affected. We will update you with the current status in 30 minutes. </div><div><span class=\"yellowfg\">10:18 AM PST</span>&nbsp;We are starting to see recovery for listing of Quicksight Dashboards and Analysis from the homepage. Customers with direct links to Quicksight Dashboards and Analysis, or Embedded Dashboards were not affected throughout this event.</div><div><span class=\"yellowfg\">11:02 AM PST</span>&nbsp;Between 7:37 AM and 10:28 AM PST, customers may have seen elevated latency and errors listing QuickSight Dashboards and Analyses from the homepage. The issue has been resolved and the service is operating normally. Customers with direct links to QuickSight Dashboards and Analyses, or Embedded Dashboards were not affected throughout this event.</div>",
      "service": "quicksight-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Ohio)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1670271960",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:26 PM PST</span>&nbsp;We are investigating an issue which may be impacting Internet connectivity between some customer networks and the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:51 PM PST</span>&nbsp;We can confirm an issue which is impacting Internet connectivity for the US-EAST-2 Region, and are attempting multiple parallel mitigation paths. Connectivity between instances within the US-EAST-2 Region, in-between AWS Regions, and Direct Connect traffic is not impacted by the event.  Some customers may be experiencing VPN connectivity due to this issue.</div><div><span class=\"yellowfg\">12:59 PM PST</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;Between 11:34 AM and 12:51 PM PST, customers experienced Internet connectivity issues for some networks to and from the US-EAST-2 Region. Connectivity between instances within the Region, in between Regions, and Direct Connect connectivity were not impacted by this issue. The issue has been resolved and connectivity has been fully restored.</div>",
      "service": "internetconnectivity-us-east-2"
    },
    {
      "service_name": "AWS Internet Connectivity (US-East)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1670273106",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:45 PM PST</span>&nbsp;We are investigating an issue which may be impacting Internet connectivity between some customer networks and the US-GOV-EAST-1 Region.</div><div><span class=\"yellowfg\">12:52 PM PST</span>&nbsp;We can confirm an issue which is impacting Internet connectivity for the US-GOV-EAST-1 Region, and are attempting multiple parallel mitigation paths. Connectivity between instances within the US-GOV-EAST-1 Region, in-between AWS Regions, and Direct Connect traffic is not impacted by the event.  Some customers may be experiencing VPN connectivity due to this issue.</div><div><span class=\"yellowfg\"> 1:00 PM PST</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 1:07 PM PST</span>&nbsp;Between 11:34 AM and 12:51 PM PST, customers experienced Internet connectivity issues for some networks to and from the US-GOV-EAST-1 Region. Connectivity between instances within the Region, in between Regions, and Direct Connect connectivity were not impacted by this issue. The issue has been resolved and connectivity has been fully restored.</div>",
      "service": "internetconnectivity-us-gov-east-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased errors viewing or updating payment information",
      "date": "1670327310",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:48 AM PST</span>&nbsp;We are investigating errors affecting the payments tab on the Billing console. Impact is limited to viewing or updating payments information. Payments are unaffected. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:13 AM PST</span>&nbsp;Starting December 6, 2:42 AM PST customers began experiencing errors when trying to access the payments tab on the AWS Billing console. Impact is limited to viewing or updating payments information. Existing payments configured in the Payment Console are unaffected. We continue to work on resolving the issue.</div><div><span class=\"yellowfg\"> 4:43 AM PST</span>&nbsp;Between 2:42 AM and 4:30 AM PST customers experienced errors when trying to access the payments tab on the AWS Billing console. Impact was limited to viewing or updating payments information and existing payments configured in the Payments Console were unaffected. The issue has been resolved and the Payment Console is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "AWS Identity and Access Management",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1670335858",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:10 AM PST</span>&nbsp;We are investigating increased IAM API latency and errors. We are actively working towards resolution.</div><div><span class=\"yellowfg\"> 6:41 AM PST</span>&nbsp;Between 4:45 AM and 6:19 AM PST, we experienced increased API error rates and latency for IAM APIs. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latencies",
      "date": "1670921550",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:52 AM PST</span>&nbsp;We are investigating increased API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. We have identified the root cause of the issue and are working towards resolution.</div><div><span class=\"yellowfg\"> 1:37 AM PST</span>&nbsp;We continue to work on resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. This issue prevents the creation of new Amazon Machine Images (AMI) within the US-EAST-1 region but does not prevent the launching of instances using existing AMIs. We have resolved the root cause of the initial impact and are currently working on draining the increased backlog of CreateImage requests. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 2:07 AM PST</span>&nbsp;We continue to make progress in resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. Since the last update, we have seen a steady decrease in the backlog of CreateImage requests. Until the backlog has been resolved, new CreateImage requests will fail with \"request limit exceeded\". Once we have cleared the backlog, we will allow new CreateImage requests to be accepted again. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 2:50 AM PST</span>&nbsp;We continue to make progress in resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. Since the last update, we have cleared the backlog of CreateImage requests and are now working to ensure that new CreateImage requests will be processed successfully. Once that is done, we will be removing the throttling for the CreateImage API, which will address the \"request limit exceeded\" error that is currently being returned. Once completed, we expect CreateImage requests to begin operating normally. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 3:52 AM PST</span>&nbsp;We continue to make progress in resolving the issue that is causing API error rates and latencies for CreateImage API calls in the US-EAST-1 Region. Since the last update, we have been working to ensure that new CreateImage requests will be processed successfully, and are nearly complete with that process. We expect to start removing the throttles for CreateImage in the next 45 minutes, which is when we expect CreateImage requests to begin operating normally. We'll continue to provide status updates as we make progress in resolving the issue.</div><div><span class=\"yellowfg\"> 4:54 AM PST</span>&nbsp;Between December 12 8:09 PM and December 13 4:32 AM PST, we experienced increased error rates and latencies for the CreateImage API in the US-EAST-1 Region. This issue affected the creation of new Amazon Machine Images (AMIs) but did not affect the launching (RunInstances) of existing AMIs. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Multiple services (Singapore)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1670968855",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:00 PM PST</span>&nbsp;We are investigating networking connectivity issues and API error rates and latencies for some services within the AP-SOUTHEAST-1 Region. We are working to identify the root cause and will provide more updates as we have them.</div><div><span class=\"yellowfg\"> 2:21 PM PST</span>&nbsp;We are seeing early signs of recovery for the issue affecting networking connectivity and increased API error rates and latencies for some services within the AP-SOUTHEAST-1 Region. The issue has not affected connectivity to the EC2 network. We are continuing to work towards full recovery.</div><div><span class=\"yellowfg\"> 2:50 PM PST</span>&nbsp;We continue to see recovery for all affected services within the AP-SOUTHEAST-1 Region and continue to monitor. While EC2 APIs were affected during the issue, connectivity to existing EC2 instances was not affected. We do not expect any further impact and will provide an update once we have completed all final validations.</div><div><span class=\"yellowfg\"> 3:15 PM PST</span>&nbsp;Between 1:30 PM and 2:20 PM PST, we experienced network connectivity issues which resulted in increased API error rates and latencies in the AP-SOUTHEAST-1 Region. CloudWatch metrics for some services were delayed. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-ap-southeast-1"
    },
    {
      "service_name": "AWS Amplify (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1671287057",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:24 AM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:43 AM PST</span>&nbsp;Between 5:14 AM and 6:27 AM PST, we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "amplify-us-east-1"
    },
    {
      "service_name": "Amazon CodeCatalyst (Oregon)",
      "summary": "[RESOLVED] Errors Creating User Aliases ",
      "date": "1671635037",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:03 AM PST</span>&nbsp;We are investigating errors while attempting to create a new alias in the US-WEST-2 Region. Existing customers are unaffected. We will provide another update at 7:45 AM PST.</div><div><span class=\"yellowfg\"> 7:43 AM PST</span>&nbsp;We continue to investigate errors while attempting to create new aliases for new customers on boarding in the US-WEST-2 Region. We have identified a potential root cause of this alias creation error and continue to work towards resolution. Existing customers remain unaffected. We will provide another update before 8:45 AM PST.</div><div><span class=\"yellowfg\"> 8:18 AM PST</span>&nbsp;Between 3:45 AM and 8:15 AM PST, customers experienced errors when attempting to create new aliases in the US-WEST-2 Region. Existing users were unaffected. The issue has been resolved and the service is operating normally.</div>",
      "service": "codecatalyst-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Impaired EC2 instances",
      "date": "1671848700",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:25 PM PST</span>&nbsp;We are investigating impairments for some EC2 instances and degraded performance for some EBS volumes in a single Availability Zone(use2-az1) in the US-EAST-2 Region.</div><div><span class=\"yellowfg\"> 6:34 PM PST</span>&nbsp;Between 6:00 PM and 6:24 PM PST, we experienced impairments for some EC2 instances and degraded performance for some EBS volumes in a single Availability Zone (use2-az1) in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "Amazon Elastic MapReduce (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates and Latencies",
      "date": "1674769200",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:40 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:09 PM PST</span>&nbsp;Between 1:00 PM and 1:35 PM PST, we experienced an increase in errors for EMR APIs in the US-EAST-1 Region. This issue impacted the EMR Management Console and API requests made to EMR. Our Engineering teams were immediately engaged, and by 1:04 PM PST we had identified the root cause as an issue with the subsystem responsible for processing these API requests. By 1:19 PM PST, we implemented our mitigation and immediately began to see recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "emr-us-east-1"
    },
    {
      "service_name": "Amazon CloudFront",
      "summary": "[RESOLVED] Elevated CloudFront latencies and error rates for IPv6 traffic",
      "date": "1675359840",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:44 AM PST</span>&nbsp;Between 9:02 AM and 9:27 AM PST, we experienced elevated errors and latencies for IPv6 traffic served by a subset of CloudFront edge locations. There was no impact to traffic served on IPv4 addresses. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudfront"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased error rates when accessing Cost Explorer, Budgets, and Cost and Usage Reports",
      "date": "1675368060",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:01 PM PST</span>&nbsp;We are investigating increased error rates when accessing Cost Explorer, Budgets, Cost and Usage Reports and Cost Explorer APIs and SDK in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:33 PM PST</span>&nbsp;We are experiencing error rates for Cost Explorer and Budgets impacting console, APIs and SDK. We do not currently see any impact to Cost and Usage Reports. We have identified the root cause and are working on mitigating the issue.  We will provide you with another update by 1:00 PM PST.</div><div><span class=\"yellowfg\"> 1:05 PM PST</span>&nbsp;We continue to experience increased Console, API and SDK error rates for Cost Explorer and Budgets. This is also impacting some portions of the Cost Management Console, Cost Categories Console, the Console home Cost and Usage widget and the Console Mobile Application. This is also causing delays for Cost Anomaly Detection (CAD) deliveries. CAD deliveries will resume upon mitigation. We have identified the root cause and are working on mitigating the issue. We will provide you with another update by 1:45 PM PST.</div><div><span class=\"yellowfg\"> 1:46 PM PST</span>&nbsp;We continue experiencing increased Console, API and SDK error rates for Cost Explorer and Budgets. This is also impacting some portions of the Cost Management Console, Cost Categories Console, the Console home Cost and Usage widget and the Console Mobile Application. This is also causing delays for Cost Anomaly Detection (CAD) deliveries. CAD deliveries will resume upon mitigation. Our mitigation efforts are underway and we will provide you with another update by 2:30 PM PST.</div><div><span class=\"yellowfg\"> 2:19 PM PST</span>&nbsp;We are currently seeing early signs of recovery and we continue to work towards full mitigation.  We will provide you another update by 3:15 PM PST.</div><div><span class=\"yellowfg\"> 2:47 PM PST</span>&nbsp;Between 11:28 AM and 2:05 PM PST, we experienced increased Console, API and SDK error rates for Cost Explorer and Budgets. This also impacted some portions of the Cost Management Console, Cost Categories Console, the Console home Cost and Usage widget and the Console Mobile Application. Queued Cost Anomaly Detection (CAD) deliveries are now being delivered, and are expected to complete within 30 minutes. The issue has been resolved and the services are operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Simple Notification Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Publish API Latencies and Errors",
      "date": "1675785900",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:05 AM PST</span>&nbsp;We are investigating increased Publish API latencies in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 8:25 AM PST</span>&nbsp;We can confirm increased Publish API latencies in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 8:37 AM PST</span>&nbsp;We have identified the cause of the Publish API latencies and errors in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 8:56 AM PST</span>&nbsp;We are now seeing a decrease in Publish API error rates and latencies in the US-EAST-1 Region. We continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 9:06 AM PST</span>&nbsp;Between 6:48 AM and 8:56 AM PST, we experienced elevated API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-us-east-1"
    },
    {
      "service_name": "Amazon Relational Database Service (N. Virginia)",
      "summary": "[RESOLVED] Connectivity issues affecting some instances",
      "date": "1676062380",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:53 PM PST</span>&nbsp;We are investigating connectivity issues affecting some instances in the US-EAST-1 Region.\n</div><div><span class=\"yellowfg\"> 1:07 PM PST</span>&nbsp;We can confirm Aurora Postgres customers are experiencing intermittent issues accessing their databases in the US-EAST-1 Region.  While we have identified the source of the issues, we do not have the root cause identified at this time.</div><div><span class=\"yellowfg\"> 1:44 PM PST</span>&nbsp;We'd like to provide some additional information about this issue. The underlying issue that caused impact to some Amazon Aurora PostgreSQL databases has been mitigated, and all affected customer databases have either recovered, or are in the process of recovering. This issue was caused by an impairment to an Aurora sub-system, which is used by a subset of customer databases in the US-EAST-1 Region. The impact of this issue was limited to a subset of Amazon Aurora PostgreSQL databases in the US-EAST-1 Region. Non-Aurora databases and Amazon Aurora MySQL databases were not impacted.\n\nWe are actively monitoring this sub-system while we work in parallel to ensure all customer databases are fully recovered. We will provide another update within 30 minutes.</div><div><span class=\"yellowfg\"> 2:19 PM PST</span>&nbsp;We have an additional update about the issue impacting some Amazon Aurora PostgreSQL customers in the US-EAST-1 Region. The vast majority of impacted customer databases have recovered and we are in the process of mitigating the remaining impacted databases. We will provide another update by 2:45 PM PST. </div><div><span class=\"yellowfg\"> 2:46 PM PST</span>&nbsp;We have an update on the issue impacting some Amazon Aurora PostgreSQL customers in the US-EAST-1 Region. We are continuing to work towards mitigating the remaining customer databases. We will provide another update by 3:45 PM PST.</div><div><span class=\"yellowfg\"> 3:30 PM PST</span>&nbsp;We have recovered all remaining Amazon Aurora PostgreSQL customer databases impacted by the issue in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\"> 3:48 PM PST</span>&nbsp;Between 11:18 AM and 2:44 PM PST, some Amazon Aurora PostgreSQL customers experienced issues accessing their databases in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "rds-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Jakarta)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1677167872",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:57 AM PST</span>&nbsp;We are investigating connectivity issues affecting Internet connectivity between some external networks and the AP-SOUTHEAST-3 Region. Connectivity to instances and all other services within the Region is not impacted.</div><div><span class=\"yellowfg\"> 8:03 AM PST</span>&nbsp;Between 7:23 AM and 7:46 AM PST, we experienced connectivity issues affecting Internet connectivity between some external networks and the AP-SOUTHEAST-3 Region. Connectivity to instances and all other services within the Region was not impacted. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-ap-southeast-3"
    },
    {
      "service_name": "AWS Import/Export",
      "summary": "[RESOLVED] Increased Error Rates for AWS Snowball",
      "date": "1677380629",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:03 PM PST</span>&nbsp;We are investigating increased error rates that block the listing of exportable AMIs for the AWS Snowball Management Console blocking Edge Compute orders.</div><div><span class=\"yellowfg\"> 7:49 PM PST</span>&nbsp;We continue to investigate increased error rates that affect the ordering of AWS Snow devices with Amazon Linux 2 and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 9:01 PM PST</span>&nbsp;We have identified the cause of the increased elevated error rates that affect the ordering of AWS Snow devices with Amazon Linux 2 and continue working towards resolution.</div><div><span class=\"yellowfg\">10:13 PM PST</span>&nbsp;We are deploying fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2, and while ETA for recovery is not yet clear, our deployments are expected to be completed in approximately 45 minutes and once completed, we should then be able to provide an ETA for recovery.</div><div><span class=\"yellowfg\">11:14 PM PST</span>&nbsp;We are deploying fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2, and approximate ETA for recovery is 2:00 AM PST. We will continue to keep you updated if this ETA changes.</div><div><span class=\"yellowfg\">Feb 26, 12:40 AM PST</span>&nbsp;Between February 25 4:30 PM and February 26 12:15 AM PST, the ordering of AWS Snow devices with Amazon Linux 2 experienced increased elevated error rates. The errors have recovered and the process of ordering AWS Snow Devices is now operating normally.\n</div>",
      "service": "import-export"
    },
    {
      "service_name": "AWS Import/Export (US-East)",
      "summary": "[RESOLVED] Increased Error Rates for AWS Snowball",
      "date": "1677380760",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:06 PM PST</span>&nbsp;We are investigating increased error rates that block the listing of exportable AMIs for the AWS Snowball Management Console blocking Edge Compute orders in the US-GOV-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:50 PM PST</span>&nbsp;We continue to investigate increased error rates that affect the ordering of AWS Snow devices with Amazon Linux 2 in the US-GOV-EAST-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 9:03 PM PST</span>&nbsp;We have identified the cause of the increased elevated error rates that affect the ordering of AWS Snow devices with Amazon Linux 2 in the US-GOV-EAST-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\">10:15 PM PST</span>&nbsp;We are deploying fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2 in the US-GOV-EAST-1 Region, and while ETA for recovery is not yet clear, our deployments are expected to be completed in approximately 45 minutes and once completed, we should then be able to provide an ETA for recovery.</div><div><span class=\"yellowfg\">11:17 PM PST</span>&nbsp;We are deploying fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2 in the US-GOV-EAST-1 Region, and approximate ETA for recovery is 2:00 AM PST. We will continue to keep you updated if this ETA changes.</div><div><span class=\"yellowfg\">Feb 26,  2:04 AM PST</span>&nbsp;We continue to deploy fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2 in the US-GOV-EAST-1 Region, and approximate ETA for recovery has now extended to 4:00 AM PST. We will continue to keep you updated if this ETA changes.</div><div><span class=\"yellowfg\">Feb 26,  2:46 AM PST</span>&nbsp;Between February 25 4:30 PM and February 26 2:44 AM PST, the ordering of AWS Snow devices with Amazon Linux 2 experienced increased elevated error rates in the US-GOV-EAST-1 Region. The errors have recovered and the process of ordering AWS Snow Devices is now operating normally.</div>",
      "service": "import-export-us-gov-east-1"
    },
    {
      "service_name": "AWS Import/Export (US-West)",
      "summary": "[RESOLVED] Increased Error Rates for AWS Snowball",
      "date": "1677380880",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:08 PM PST</span>&nbsp;We are investigating increased error rates that block the listing of exportable AMIs for the AWS Snowball Management Console blocking Edge Compute orders in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 7:50 PM PST</span>&nbsp;We continue to investigate increased error rates that affect the ordering of AWS Snow devices with Amazon Linux 2 in the US-GOV-WEST-1 Region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 9:05 PM PST</span>&nbsp;We have identified the cause of the increased elevated error rates that affect the ordering of AWS Snow devices with Amazon Linux 2 in the US-GOV-WEST-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\">10:17 PM PST</span>&nbsp;We are deploying fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2 in the US-GOV-WEST-1 Region, and while ETA for recovery is not yet clear, our deployments are expected to be completed in approximately 45 minutes and once completed, we should then be able to provide an ETA for recovery.</div><div><span class=\"yellowfg\">11:19 PM PST</span>&nbsp;We are deploying fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2 in the US-GOV-WEST-1 Region, and approximate ETA for recovery is 2:00 AM PST. We will continue to keep you updated if this ETA changes.</div><div><span class=\"yellowfg\">Feb 26,  2:05 AM PST</span>&nbsp;We continue to deploy fixes to mitigate the current impact on ordering AWS Snow devices with Amazon Linux 2 in the US-GOV-WEST-1 Region, and approximate ETA for recovery has now extended to 4:00 AM PST. We will continue to keep you updated if this ETA changes.</div><div><span class=\"yellowfg\">Feb 26,  2:11 AM PST</span>&nbsp;Between February 25 4:30 PM and February 26 2:07 AM PST, the ordering of AWS Snow devices with Amazon Linux 2 experienced increased elevated error rates in the US-GOV-WEST-1 Region. The errors have recovered and the process of ordering AWS Snow Devices is now operating normally.</div>",
      "service": "import-export-us-gov-west-1"
    },
    {
      "service_name": "AWS Identity and Access Management (US-West)",
      "summary": "[RESOLVED] Increased STS API error rates",
      "date": "1677814497",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:34 PM PST</span>&nbsp;We are investigating increased STS API error rates in the US-GOV-WEST-1 Region. We are actively working toward resolution.</div><div><span class=\"yellowfg\"> 8:08 PM PST</span>&nbsp;We have identified the cause of the increased STS API error rates in the US-GOV-WEST-1 Region and continue working towards resolution. Other AWS services whose features require STS actions may also be impacted.</div><div><span class=\"yellowfg\"> 8:52 PM PST</span>&nbsp;Between 5:35 PM and 8:35 PM PST, we experienced increased STS API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "iam-us-gov-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Oregon)",
      "summary": "[RESOLVED] Internet Connectivity in the US-WEST-2 Region",
      "date": "1678726651",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:57 AM PDT</span>&nbsp;Between 8:25 AM and 9:16 AM PDT, we experienced elevated packet loss and latency to a small set of internet destinations in the US-WEST-2 Region. Connectivity within the US-WEST-2 Region was not impacted. The issue has been resolved and the services are operating normally.</div>",
      "service": "internetconnectivity-us-west-2"
    },
    {
      "service_name": "AWS CloudFormation (Oregon)",
      "summary": "[RESOLVED] Increased Latencies",
      "date": "1680040146",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:49 PM PDT</span>&nbsp;We are experiencing increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-WEST-2 Region. Customers may notice their Cloudformation stacks appearing in 'UPDATE_ROLLBACK_IN_PROGRESS' state. We have identified the root cause and are actively working on mitigating the issue.</div><div><span class=\"yellowfg\"> 3:15 PM PDT</span>&nbsp;We can confirm increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-WEST-2 Region. Customers may notice their Cloudformation stacks appearing in an 'UPDATE_ROLLBACK_IN_PROGRESS' state. We are actively working on multiple paths to mitigation. We have already begun to see some level of recovery, and will continue to observe additional recovery as our mitigations progress. We continue to work toward full recovery and will update you in the next 30 minutes.</div><div><span class=\"yellowfg\"> 4:04 PM PDT</span>&nbsp;We continue to work toward mitigating the issue resulting in latencies when creating, updating, and deleting AWS Cloudformation stacks in the US-WEST-2 Region. While we initially observed some recovery, additional progress thus far has been slower than we initially anticipated. The team continues to work on multiple parallel paths for recovery, and will continue to provide updates as recovery progresses.</div><div><span class=\"yellowfg\"> 4:52 PM PDT</span>&nbsp;Between 11:23 AM and 4:28 PM PDT, we experienced increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudformation-us-west-2"
    }
  ],
  "current": []
}
